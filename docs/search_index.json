[["index.html", "Bayesian Data Analysis Notes Introduction", " Bayesian Data Analysis Notes Jonas Ringive Korsholm Introduction "],["chapter-1-golems-of-prague.html", "1 Chapter 1 - Golems of Prague", " 1 Chapter 1 - Golems of Prague This is a metaphor of having golems. These are machines that are given commands and then they will act accordingly, and do nothing else. The problem with this is that it has no background or knowledge, hence it is really up to us to make sure that it is not malfunctioning. Hence this is a metaphor for statistical models. Regarding models and hypotheses We see that a model is not the same as a hypothesis. With this there is critique on attempting to falsify the null model. That is because then you assume that something is exactly what you state in the null model. That is not true. Hence one should instead looking into falsifying the alternative model. Then what are we going to do? We are going to count the ways that an outcome can occur, to find the most probable events. This will give us bayesian distributions. These we are to explore to learn about the data and UPDATE our own beliefs. The disadvantage of bayesian approaches is the it can be cumbersome to perform. The reason that this is a small field, is that previously it has not been computationally possible. Although MCMC allows this! This is done by drawing an approximation of the original posterior distribution. Bayesian Data Analysis is about counting all the ways that the data can happen, according to assumptions. Assumptions with more ways to cause data are more plausible. CH1, p. 11 We see that the assumptions are what we put into the Golems. Hence what the Golem belief before seeing the data. Assumptions = prior beliefs. What the model assumes before What is probability? The frequentist view: This is basically traditional statistics, where we test a hypothesis based on a significance level. This relies on the data you have and the process you take. This is an objective perspective, you never carry on knowledge, it is all about the long relative frequencies. The bayesian probability: here we assume that there is nothing random. But what we call randomness is our ignorance. Hence e.g., with a coin toss, if we knew all about physics, we would always be able to predict the outcome. This implies that the bayesian view is subjective, that is also a great critique of this perception. "],["chapter-2-small-worlds-and-large-worlds.html", "2 Chapter 2 - Small worlds and Large Worlds 2.1 Models and Estimation 2.2 Exercises", " 2 Chapter 2 - Small worlds and Large Worlds The small world is the world of the golems assumptions. Bayesian golems are optimal, in the small world. The large world is the real world. No guarantee of optimality for any kind of golem. Terminoligy: Under drawing of replacement we are able to write out the possible contents Garden of Forking Data is the possible outcomes that we will see. Conjecture = the assumption of what the different observations looks like (or is constructed). e.g, we have a bag of 4 marbles, we may assume that there is 1 blue and 3 whites. In the following we are able to see what possible outcomes we expect to have. This can then be extended by a second draw: Now we see that there are 16 different paths, given the assumptions that we made (1 blue and 3 white). Now this expands exponentially, hence by including a third draw we would have 64 different outcomes. Now what? One should draw the whole garden and find the paths that can lead to the way of producing the desired output. As the number of paths increase exponentially, we quickly start to work with large counts. This is the reason, that Bayes Theorem got into this approach, as we are able to compress the counts as relative counts. We can do this with R. Lets say that we have 3, 8 and 9 ways of producing three different compositions of a bag of marbles, then we say: ways = c(3,8,9) #notice that the counts are found given the conjecture ways/sum(ways) ## [1] 0.15 0.40 0.45 Hence we see that the relative plausibility for 3 blue and 1 white is 45%, given the conjecture (the assumption of the composition of the bag) Building a model We want to: Design the model (data story) Condition on the data (update) Evaluate the model (critique) See also example with tossing globes in the following sections. priors let us say that we have a prior with absolutely no information (like traditional statistics), we do not know if it is 100% or 0% water, hence the prior looks the following: Now we introduce more information. Notice that we consecutively update priors. Prior distribution = striped and posterior distribution = solid line. We see that after the first observation (top left), there is 0 probability of 0% water, as we now know that there is water. Now we can look at the next observation, we see that it is land, now the posterior distribution is essentially centered at 0. Now lets look at a third observation, we see that the posterior is skewed to the right of the prior. We see that the more information we see, the taller (the more certain) will the distributions be. Key takeaway: the current posterior distribution will be the prior distribution for the following observation. How can one manipulate the process? We see that you can change how much the prior is updated, hence if you are certain about a prior distribution, then you can make it more difficult for the model to update this. Also we see from the example above, that the more information that is revealed, the less does the prior matter. 2.1 Models and Estimation Introduction: We see that the Bayesian Data Analysis takes the approach of having a prior probability (the probability of an event happening while ignoring the data we have at hand). Then we compute posterior probabilities as we introduce data. The posterior probabilities can be seen as a relative count of how many ways a given outcome can be replicated out of the total. The Bayesian Data Analysis can essentially be fitted with three different models: 1. Grid approximation: Good with few parameters 2. Quadratic approximation: Good with a moderate amount of parameters. Also, this is an approximate and is rarely applied. 3. Markov Chain Monte Carlo: Outperforms other models in in a high parameter scenario 4. (Analytical approach): this is often impossible. Examples will be shown during lectures, but will not be used. Remember that the Bayesian Estimate that we end up with, will always be a distribution and not a point estimate!! This is a section about models and estimation, based on chapter 1 - 3 from the book. Some terminology: Prior Distribution: This is the distribution of a prior event. E.g., lets say that we toss a coin. There are two sides hence we expect to see a normal bell curve centered in 50%. Posterior Distribution: This is basically just the prior distribution after we introduce observations. Lets say that we end up getting many consecutive heads, it implies that the probability of an outcome is actually skewed, e.g., the coin may be more heavy on one side. Hence we will see that the posterior distribution will not be centered around 50% but move to one of the sides. Therefore, you can model with (test different) prior probabilities. But the posterior probability is found after introducing data (observations/samples). Likelihood: This is just the relative number of ways that a given scenario can be produced. E.g., if you have discrete data, drawing marbles then the likelihood of some sequence is just the relative count of how you can construct such sequence. Prior probability = prior plausibility Updated plausibility = posterior probability The posterior is calculated as the following This is to be interpreted as: Average likelihood = evidens. This is summing over p. hence it ensures that the posterior distribution will sum to 1. Likelihood = ways to get the data Prior = prior ways to get the data Hence one ends up with probability of p given the new data. Assumptions when making the model Data story: Motivate the model by narrating how the data might arise. Update: Educate your model by feeding it the data. Basically the distribution for a given outcome is explored observation by observation. The more data we have seen the better should the distributions be. Evaluate: All statistical models require supervision, leading possibly to model revision. #Code 2.2 - finding likelihood dbinom(x = 6 #No. of &#39;successes&#39; water in this case ,size=9 #No. of tosses ,prob=0.5 #Probability of a given outcome (succes) ) # = 0.1640625 ## [1] 0.1640625 d for density or distribution. bi for binomial. We see that the probability of getting 6 water (and 3 land) is 16%. Given that the probability of water is 50%. The 16% is equivilent to the relative number of ways that 6 water and 3 land can be found. Notes in prior priobabilitieis. We see that oftentimes you only have one prior, and it can for instance be based on already seen data. Although a prior does not have to be based on that, one can test of different priors and see what that leads to. How to select a prior: In general we can always do better than just everything is equally likely, but notice there is no true prior. This means that a good prior is subjective, therefore, one can test with different priors and see how sensitive the model is to different priors. 2.1.1 Engines / Motors to estimate the models We are going to apply three different engines to estimate the model. 2.1.1.1 Grid approximation Here we use a grid of values to compute the likelihood of Water. This is basically just defining a range, and calculating the probabilities for the given value, then we end up being able to plot this. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(6 ,size=9 ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ) mtext( &quot;20 points&quot; ) abline(v = 0.67,lty = 2,col = &quot;darkgreen&quot;) mtext(text = &quot;Quap mean approximation, see next section&quot;,side = 1,at = 0.6,col = &quot;darkgreen&quot;,cex = 0.7) So we see that the probability of picking 6 times water peaks around 60% to 70%. The Grid approximation scales very badly, hence when you have a model with many variables it starts to get cumbersome to estimate. That is the reason that we go to quadratic approximation. 2.1.1.2 Quadratic approximation The quadratic approximation is basically utilizing Guassian (normal) distribution library(rethinking) globe.qa &lt;- quap( alist( W ~ dbinom( W+L ,p) , # binomial likelihood p ~ dunif(0,1) # uniform prior ) , data=list(W=6,L=3) ) # display summary of quadratic approximation precis( globe.qa ) x 0.6666664 x 0.1571339 x 0.4155361 x 0.9177966 We see that the mean is 0.67, hence the highest prior, this level is also inserted in the illustration above, to show that we end up in approximately the same place. Then the standard deviation (sd) is the spread en then the confidence intervals are shown. 2.1.1.3 Markov Chain Monte Carlo This sections does not yet go in detail with MCMC. Although the key takeaway is that quadratic approximation is also cumbersome and to some extent impossible when you have a lot of parameters. Therefore MCMC can be used instead. The following is a toy example with the same data: #R Code 2.8 n_samples &lt;- 1000 p &lt;- rep( NA , n_samples ) #Samples from the posterior distribution p[1] &lt;- 0.5 #Posterior W &lt;- 6 #Successes (Water) L &lt;- 3 #Non successes (Land) for ( i in 2:n_samples) { p_new &lt;- rnorm(1, p[i-1], 0.1) if(p_new &lt; 0) p_new &lt;- abs(p_new) if(p_new &gt; 1) p_new &lt;- 2 - p_new q0 &lt;- dbinom(W , W+L , p[i-1] ) q1 &lt;- dbinom(W , W+L , p_new ) p[i] &lt;- ifelse( runif(1) &lt; q1/q0 , p_new , p[i-1] ) } dens(p , xlim=c(0,1)) curve(dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE) 2.2 Exercises 2.2.1 2M1 Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p. W, W, W W, W, W, L L, W, W, L, W, W, W They can be calculated using the same piece of code. Although we must change the number of successes and the number of tosses. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 3 ,size=3 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) We see that if we only draw water then we get more and more certain that there is only water. If we are to make a new toss. Then we should what we see above will be our new prior. Hence we start with a uniform prior (the stupid prior) and end up with a prior that contain information. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 3 #No. of successes ,size=4 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,sub = paste(length,&quot; points&quot;) ) length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 5 #No. of successes ,size = 7 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,sub = paste(length,&quot; points&quot;) ) 2.2.2 2M2 Now assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above. With this we are going to put more information in the prior. More than just having a prior without any information. We see with this binomial (i guess you can say) technique, there will be a jump whenever the probability of water exceeds 50%. This come from the prior we set, where we expect that at least 50% of the globe is water and the rest is land, hence we think that there is a chance of having more water than land. Naturally one could set the prior to anything and see how this affect the results. length = 50 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- c(rep(0,length/2),rep(1,length/2)) # compute likelihood at each value in grid x = 3 size = 3 likelihood &lt;- dbinom(x = x ,size=size #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,2)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;,sub = &quot;All use the same prior&quot;) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) #And for the other three models x = 3 size = 4 likelihood &lt;- dbinom(x = x,size=size,prob=p_grid) unstd.posterior &lt;- likelihood * prior posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) x = 5 size = 7 likelihood &lt;- dbinom(x = x,size=size,prob=p_grid) unstd.posterior &lt;- likelihood * prior posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) "],["chapter-3-sampling-the-imaginary.html", "3 Chapter 3 - Sampling the Imaginary 3.1 Sampling from a grid-approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Exercises", " 3 Chapter 3 - Sampling the Imaginary They take the following example: We see that this is the equation for a person being vampire given that the test i positive. This is calculated by using Bayes Theorem. This can be written in code in the following: #3.1 Pr_Positive_Vampire &lt;- 0.95 #Condiational prob for positive given vampire Pr_Positive_Mortal &lt;- 0.01 #Essentially the false positive rate Pr_Vampire &lt;- 0.001 #Prior for being vampire Pr_Positive &lt;- Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire) (Pr_Vampire_Positive &lt;- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive) ## [1] 0.08683729 We see that given the test being positive, there is an 8.6% chance of being vampire compared to the default 0.001. So we see that even though the test has 95% percent correctness there is in fact still only less than 10% chance that you are a vampire given the positive test. We see that the actual true rate is dependent on how many in the population that are actual vampires. A more intuitive way of writing out this can be shown as: Leading to: The Aim of the following section is to build an intuition around the approximation techniques. We see that in the example it is very simple hence one would not necessarily need the approximation techniques. Although it is suggested to start using the fitting techniques early as one will use them as soon as the problem gets just a bit more complex. 3.1 Sampling from a grid-approximate posterior Lets take an example. We are going to take 10.000 samples p_grid &lt;- seq(from=0,to=1,length.out=1000 ) prob_p &lt;- rep(1,1000) #This is the prior. It is flat = stupid prior prob_data &lt;- dbinom(6,size=9,prob=p_grid) posterior &lt;- prob_data * prob_p posterior &lt;- posterior / sum(posterior) #sum(posterior) = average likelihood print(&quot;First 10 posteriors&quot;) posterior[1:10] par(mfrow = c(2,2)) plot(p_grid,main = &quot;p_grid&quot;) #The grid plot(prob_p,main = &quot;prob_p (prior)&quot;) #Prior plot(prob_data,main = &quot;prob_data (likelihood)&quot;) #likelihood plot(posterior,main = &quot;posterior&quot;) #Posterior ## [1] &quot;First 10 posteriors&quot; ## [1] 0.000000e+00 8.433659e-19 5.381333e-17 6.111249e-16 3.423368e-15 ## [6] 1.301978e-14 3.875963e-14 9.744233e-14 2.164638e-13 4.375070e-13 Notice that the p_grid is flat. Hence the Now lets sample from the prior distribution. #Code 3.3 set.seed(1337) samples &lt;- sample(p_grid ,prob=posterior ,size=10000 #The higher the number the smoother the curve ,replace=TRUE ) #Code 3.4 par(mfrow = c(2,1)) plot(samples) library(rethinking) dens(samples) We see that the densitity plot shows the estimated probability of water on the globe. What we have seen so far: we are replicating the posterior probability of water based on the data we have at hand. This is not of much value. We are next going to use the samples to understand the posterior. 3.2 Sampling to summarize Now we see that the models work is done. Although now it is up to the analyst to interprete the posterior distribution. This includes: How much posterior probability lies below some parameter value? How much posterior probability lies between two parameter values? Which parameter value marks the lower 5% of the posterior probability? Which range of parameter values contains 90% of the posterior probability? Which parameter value has highest posterior probability? This is essentially about three things: 1) defined boundaries, 2) defined probability mass, 3) point estimates. The following sections describe these. 3.2.1 Defined Boundaries This options(scipen = 0) # add up posterior probability where p &lt; 0.5 p_grid sum(posterior[p_grid &lt; 0.5]) #Sum all posteriors where the p_grid is &lt; 50% ## [1] 0.000000000 0.001001001 0.002002002 0.003003003 0.004004004 0.005005005 ## [7] 0.006006006 0.007007007 0.008008008 0.009009009 0.010010010 0.011011011 ## [13] 0.012012012 0.013013013 0.014014014 0.015015015 0.016016016 0.017017017 ## [19] 0.018018018 0.019019019 0.020020020 0.021021021 0.022022022 0.023023023 ## [25] 0.024024024 0.025025025 0.026026026 0.027027027 0.028028028 0.029029029 ## [31] 0.030030030 0.031031031 0.032032032 0.033033033 0.034034034 0.035035035 ## [37] 0.036036036 0.037037037 0.038038038 0.039039039 0.040040040 0.041041041 ## [43] 0.042042042 0.043043043 0.044044044 0.045045045 0.046046046 0.047047047 ## [49] 0.048048048 0.049049049 0.050050050 0.051051051 0.052052052 0.053053053 ## [55] 0.054054054 0.055055055 0.056056056 0.057057057 0.058058058 0.059059059 ## [61] 0.060060060 0.061061061 0.062062062 0.063063063 0.064064064 0.065065065 ## [67] 0.066066066 0.067067067 0.068068068 0.069069069 0.070070070 0.071071071 ## [73] 0.072072072 0.073073073 0.074074074 0.075075075 0.076076076 0.077077077 ## [79] 0.078078078 0.079079079 0.080080080 0.081081081 0.082082082 0.083083083 ## [85] 0.084084084 0.085085085 0.086086086 0.087087087 0.088088088 0.089089089 ## [91] 0.090090090 0.091091091 0.092092092 0.093093093 0.094094094 0.095095095 ## [97] 0.096096096 0.097097097 0.098098098 0.099099099 0.100100100 0.101101101 ## [103] 0.102102102 0.103103103 0.104104104 0.105105105 0.106106106 0.107107107 ## [109] 0.108108108 0.109109109 0.110110110 0.111111111 0.112112112 0.113113113 ## [115] 0.114114114 0.115115115 0.116116116 0.117117117 0.118118118 0.119119119 ## [121] 0.120120120 0.121121121 0.122122122 0.123123123 0.124124124 0.125125125 ## [127] 0.126126126 0.127127127 0.128128128 0.129129129 0.130130130 0.131131131 ## [133] 0.132132132 0.133133133 0.134134134 0.135135135 0.136136136 0.137137137 ## [139] 0.138138138 0.139139139 0.140140140 0.141141141 0.142142142 0.143143143 ## [145] 0.144144144 0.145145145 0.146146146 0.147147147 0.148148148 0.149149149 ## [151] 0.150150150 0.151151151 0.152152152 0.153153153 0.154154154 0.155155155 ## [157] 0.156156156 0.157157157 0.158158158 0.159159159 0.160160160 0.161161161 ## [163] 0.162162162 0.163163163 0.164164164 0.165165165 0.166166166 0.167167167 ## [169] 0.168168168 0.169169169 0.170170170 0.171171171 0.172172172 0.173173173 ## [175] 0.174174174 0.175175175 0.176176176 0.177177177 0.178178178 0.179179179 ## [181] 0.180180180 0.181181181 0.182182182 0.183183183 0.184184184 0.185185185 ## [187] 0.186186186 0.187187187 0.188188188 0.189189189 0.190190190 0.191191191 ## [193] 0.192192192 0.193193193 0.194194194 0.195195195 0.196196196 0.197197197 ## [199] 0.198198198 0.199199199 0.200200200 0.201201201 0.202202202 0.203203203 ## [205] 0.204204204 0.205205205 0.206206206 0.207207207 0.208208208 0.209209209 ## [211] 0.210210210 0.211211211 0.212212212 0.213213213 0.214214214 0.215215215 ## [217] 0.216216216 0.217217217 0.218218218 0.219219219 0.220220220 0.221221221 ## [223] 0.222222222 0.223223223 0.224224224 0.225225225 0.226226226 0.227227227 ## [229] 0.228228228 0.229229229 0.230230230 0.231231231 0.232232232 0.233233233 ## [235] 0.234234234 0.235235235 0.236236236 0.237237237 0.238238238 0.239239239 ## [241] 0.240240240 0.241241241 0.242242242 0.243243243 0.244244244 0.245245245 ## [247] 0.246246246 0.247247247 0.248248248 0.249249249 0.250250250 0.251251251 ## [253] 0.252252252 0.253253253 0.254254254 0.255255255 0.256256256 0.257257257 ## [259] 0.258258258 0.259259259 0.260260260 0.261261261 0.262262262 0.263263263 ## [265] 0.264264264 0.265265265 0.266266266 0.267267267 0.268268268 0.269269269 ## [271] 0.270270270 0.271271271 0.272272272 0.273273273 0.274274274 0.275275275 ## [277] 0.276276276 0.277277277 0.278278278 0.279279279 0.280280280 0.281281281 ## [283] 0.282282282 0.283283283 0.284284284 0.285285285 0.286286286 0.287287287 ## [289] 0.288288288 0.289289289 0.290290290 0.291291291 0.292292292 0.293293293 ## [295] 0.294294294 0.295295295 0.296296296 0.297297297 0.298298298 0.299299299 ## [301] 0.300300300 0.301301301 0.302302302 0.303303303 0.304304304 0.305305305 ## [307] 0.306306306 0.307307307 0.308308308 0.309309309 0.310310310 0.311311311 ## [313] 0.312312312 0.313313313 0.314314314 0.315315315 0.316316316 0.317317317 ## [319] 0.318318318 0.319319319 0.320320320 0.321321321 0.322322322 0.323323323 ## [325] 0.324324324 0.325325325 0.326326326 0.327327327 0.328328328 0.329329329 ## [331] 0.330330330 0.331331331 0.332332332 0.333333333 0.334334334 0.335335335 ## [337] 0.336336336 0.337337337 0.338338338 0.339339339 0.340340340 0.341341341 ## [343] 0.342342342 0.343343343 0.344344344 0.345345345 0.346346346 0.347347347 ## [349] 0.348348348 0.349349349 0.350350350 0.351351351 0.352352352 0.353353353 ## [355] 0.354354354 0.355355355 0.356356356 0.357357357 0.358358358 0.359359359 ## [361] 0.360360360 0.361361361 0.362362362 0.363363363 0.364364364 0.365365365 ## [367] 0.366366366 0.367367367 0.368368368 0.369369369 0.370370370 0.371371371 ## [373] 0.372372372 0.373373373 0.374374374 0.375375375 0.376376376 0.377377377 ## [379] 0.378378378 0.379379379 0.380380380 0.381381381 0.382382382 0.383383383 ## [385] 0.384384384 0.385385385 0.386386386 0.387387387 0.388388388 0.389389389 ## [391] 0.390390390 0.391391391 0.392392392 0.393393393 0.394394394 0.395395395 ## [397] 0.396396396 0.397397397 0.398398398 0.399399399 0.400400400 0.401401401 ## [403] 0.402402402 0.403403403 0.404404404 0.405405405 0.406406406 0.407407407 ## [409] 0.408408408 0.409409409 0.410410410 0.411411411 0.412412412 0.413413413 ## [415] 0.414414414 0.415415415 0.416416416 0.417417417 0.418418418 0.419419419 ## [421] 0.420420420 0.421421421 0.422422422 0.423423423 0.424424424 0.425425425 ## [427] 0.426426426 0.427427427 0.428428428 0.429429429 0.430430430 0.431431431 ## [433] 0.432432432 0.433433433 0.434434434 0.435435435 0.436436436 0.437437437 ## [439] 0.438438438 0.439439439 0.440440440 0.441441441 0.442442442 0.443443443 ## [445] 0.444444444 0.445445445 0.446446446 0.447447447 0.448448448 0.449449449 ## [451] 0.450450450 0.451451451 0.452452452 0.453453453 0.454454454 0.455455455 ## [457] 0.456456456 0.457457457 0.458458458 0.459459459 0.460460460 0.461461461 ## [463] 0.462462462 0.463463463 0.464464464 0.465465465 0.466466466 0.467467467 ## [469] 0.468468468 0.469469469 0.470470470 0.471471471 0.472472472 0.473473473 ## [475] 0.474474474 0.475475475 0.476476476 0.477477477 0.478478478 0.479479479 ## [481] 0.480480480 0.481481481 0.482482482 0.483483483 0.484484484 0.485485485 ## [487] 0.486486486 0.487487487 0.488488488 0.489489489 0.490490490 0.491491491 ## [493] 0.492492492 0.493493493 0.494494494 0.495495495 0.496496496 0.497497497 ## [499] 0.498498498 0.499499499 0.500500501 0.501501502 0.502502503 0.503503504 ## [505] 0.504504505 0.505505506 0.506506507 0.507507508 0.508508509 0.509509510 ## [511] 0.510510511 0.511511512 0.512512513 0.513513514 0.514514515 0.515515516 ## [517] 0.516516517 0.517517518 0.518518519 0.519519520 0.520520521 0.521521522 ## [523] 0.522522523 0.523523524 0.524524525 0.525525526 0.526526527 0.527527528 ## [529] 0.528528529 0.529529530 0.530530531 0.531531532 0.532532533 0.533533534 ## [535] 0.534534535 0.535535536 0.536536537 0.537537538 0.538538539 0.539539540 ## [541] 0.540540541 0.541541542 0.542542543 0.543543544 0.544544545 0.545545546 ## [547] 0.546546547 0.547547548 0.548548549 0.549549550 0.550550551 0.551551552 ## [553] 0.552552553 0.553553554 0.554554555 0.555555556 0.556556557 0.557557558 ## [559] 0.558558559 0.559559560 0.560560561 0.561561562 0.562562563 0.563563564 ## [565] 0.564564565 0.565565566 0.566566567 0.567567568 0.568568569 0.569569570 ## [571] 0.570570571 0.571571572 0.572572573 0.573573574 0.574574575 0.575575576 ## [577] 0.576576577 0.577577578 0.578578579 0.579579580 0.580580581 0.581581582 ## [583] 0.582582583 0.583583584 0.584584585 0.585585586 0.586586587 0.587587588 ## [589] 0.588588589 0.589589590 0.590590591 0.591591592 0.592592593 0.593593594 ## [595] 0.594594595 0.595595596 0.596596597 0.597597598 0.598598599 0.599599600 ## [601] 0.600600601 0.601601602 0.602602603 0.603603604 0.604604605 0.605605606 ## [607] 0.606606607 0.607607608 0.608608609 0.609609610 0.610610611 0.611611612 ## [613] 0.612612613 0.613613614 0.614614615 0.615615616 0.616616617 0.617617618 ## [619] 0.618618619 0.619619620 0.620620621 0.621621622 0.622622623 0.623623624 ## [625] 0.624624625 0.625625626 0.626626627 0.627627628 0.628628629 0.629629630 ## [631] 0.630630631 0.631631632 0.632632633 0.633633634 0.634634635 0.635635636 ## [637] 0.636636637 0.637637638 0.638638639 0.639639640 0.640640641 0.641641642 ## [643] 0.642642643 0.643643644 0.644644645 0.645645646 0.646646647 0.647647648 ## [649] 0.648648649 0.649649650 0.650650651 0.651651652 0.652652653 0.653653654 ## [655] 0.654654655 0.655655656 0.656656657 0.657657658 0.658658659 0.659659660 ## [661] 0.660660661 0.661661662 0.662662663 0.663663664 0.664664665 0.665665666 ## [667] 0.666666667 0.667667668 0.668668669 0.669669670 0.670670671 0.671671672 ## [673] 0.672672673 0.673673674 0.674674675 0.675675676 0.676676677 0.677677678 ## [679] 0.678678679 0.679679680 0.680680681 0.681681682 0.682682683 0.683683684 ## [685] 0.684684685 0.685685686 0.686686687 0.687687688 0.688688689 0.689689690 ## [691] 0.690690691 0.691691692 0.692692693 0.693693694 0.694694695 0.695695696 ## [697] 0.696696697 0.697697698 0.698698699 0.699699700 0.700700701 0.701701702 ## [703] 0.702702703 0.703703704 0.704704705 0.705705706 0.706706707 0.707707708 ## [709] 0.708708709 0.709709710 0.710710711 0.711711712 0.712712713 0.713713714 ## [715] 0.714714715 0.715715716 0.716716717 0.717717718 0.718718719 0.719719720 ## [721] 0.720720721 0.721721722 0.722722723 0.723723724 0.724724725 0.725725726 ## [727] 0.726726727 0.727727728 0.728728729 0.729729730 0.730730731 0.731731732 ## [733] 0.732732733 0.733733734 0.734734735 0.735735736 0.736736737 0.737737738 ## [739] 0.738738739 0.739739740 0.740740741 0.741741742 0.742742743 0.743743744 ## [745] 0.744744745 0.745745746 0.746746747 0.747747748 0.748748749 0.749749750 ## [751] 0.750750751 0.751751752 0.752752753 0.753753754 0.754754755 0.755755756 ## [757] 0.756756757 0.757757758 0.758758759 0.759759760 0.760760761 0.761761762 ## [763] 0.762762763 0.763763764 0.764764765 0.765765766 0.766766767 0.767767768 ## [769] 0.768768769 0.769769770 0.770770771 0.771771772 0.772772773 0.773773774 ## [775] 0.774774775 0.775775776 0.776776777 0.777777778 0.778778779 0.779779780 ## [781] 0.780780781 0.781781782 0.782782783 0.783783784 0.784784785 0.785785786 ## [787] 0.786786787 0.787787788 0.788788789 0.789789790 0.790790791 0.791791792 ## [793] 0.792792793 0.793793794 0.794794795 0.795795796 0.796796797 0.797797798 ## [799] 0.798798799 0.799799800 0.800800801 0.801801802 0.802802803 0.803803804 ## [805] 0.804804805 0.805805806 0.806806807 0.807807808 0.808808809 0.809809810 ## [811] 0.810810811 0.811811812 0.812812813 0.813813814 0.814814815 0.815815816 ## [817] 0.816816817 0.817817818 0.818818819 0.819819820 0.820820821 0.821821822 ## [823] 0.822822823 0.823823824 0.824824825 0.825825826 0.826826827 0.827827828 ## [829] 0.828828829 0.829829830 0.830830831 0.831831832 0.832832833 0.833833834 ## [835] 0.834834835 0.835835836 0.836836837 0.837837838 0.838838839 0.839839840 ## [841] 0.840840841 0.841841842 0.842842843 0.843843844 0.844844845 0.845845846 ## [847] 0.846846847 0.847847848 0.848848849 0.849849850 0.850850851 0.851851852 ## [853] 0.852852853 0.853853854 0.854854855 0.855855856 0.856856857 0.857857858 ## [859] 0.858858859 0.859859860 0.860860861 0.861861862 0.862862863 0.863863864 ## [865] 0.864864865 0.865865866 0.866866867 0.867867868 0.868868869 0.869869870 ## [871] 0.870870871 0.871871872 0.872872873 0.873873874 0.874874875 0.875875876 ## [877] 0.876876877 0.877877878 0.878878879 0.879879880 0.880880881 0.881881882 ## [883] 0.882882883 0.883883884 0.884884885 0.885885886 0.886886887 0.887887888 ## [889] 0.888888889 0.889889890 0.890890891 0.891891892 0.892892893 0.893893894 ## [895] 0.894894895 0.895895896 0.896896897 0.897897898 0.898898899 0.899899900 ## [901] 0.900900901 0.901901902 0.902902903 0.903903904 0.904904905 0.905905906 ## [907] 0.906906907 0.907907908 0.908908909 0.909909910 0.910910911 0.911911912 ## [913] 0.912912913 0.913913914 0.914914915 0.915915916 0.916916917 0.917917918 ## [919] 0.918918919 0.919919920 0.920920921 0.921921922 0.922922923 0.923923924 ## [925] 0.924924925 0.925925926 0.926926927 0.927927928 0.928928929 0.929929930 ## [931] 0.930930931 0.931931932 0.932932933 0.933933934 0.934934935 0.935935936 ## [937] 0.936936937 0.937937938 0.938938939 0.939939940 0.940940941 0.941941942 ## [943] 0.942942943 0.943943944 0.944944945 0.945945946 0.946946947 0.947947948 ## [949] 0.948948949 0.949949950 0.950950951 0.951951952 0.952952953 0.953953954 ## [955] 0.954954955 0.955955956 0.956956957 0.957957958 0.958958959 0.959959960 ## [961] 0.960960961 0.961961962 0.962962963 0.963963964 0.964964965 0.965965966 ## [967] 0.966966967 0.967967968 0.968968969 0.969969970 0.970970971 0.971971972 ## [973] 0.972972973 0.973973974 0.974974975 0.975975976 0.976976977 0.977977978 ## [979] 0.978978979 0.979979980 0.980980981 0.981981982 0.982982983 0.983983984 ## [985] 0.984984985 0.985985986 0.986986987 0.987987988 0.988988989 0.989989990 ## [991] 0.990990991 0.991991992 0.992992993 0.993993994 0.994994995 0.995995996 ## [997] 0.996996997 0.997997998 0.998998999 1.000000000 ## [1] 0.1718746 We see that the sum of the first 10 probabilities, as these are below. 3.2.2 Defined Probability Mass This is about finding an interval and interpreting this. For example, we want to know the region between the 10% and 90% quantiles, or the first 80%. This can be solved by doing: #R Code 3.9 quantile(samples,0.8) #Boundaries of lower 80% posterior probability, thus it starts at 0 #R Code 3.10 quantile(samples,c(0.1,0.9)) #Or between 10% and 90% posterior probability. hence midle 80% posterior probability ## 80% ## 0.7607608 ## 10% 90% ## 0.4484484 0.8119119 These we call percentile intervals (PI). For this there is functionality in the rethinkinglibrary. PI(samples,prob = 0.5) ## 25% 75% ## 0.5415415 0.7377377 We see that this will autoamtically find the center probability of the posterior distribution. This may not be convenient if for instance the peak is outside of the region that PI return. Therefore we have the functio HPDI, which stands for highest posterior density interval. This will find the densest probability mass. This is justified, as you can end up with the same probability mass region with many combinations, hence HPDI is merely helping with this procedure. HPDI(samples,prob = 0.5) ## |0.5 0.5| ## 0.5475475 0.7417417 Now we see that it finds a more narrow region aggregating to the same probability. The following also exemplify this: Many will confuse this with a confidence interval, while they will be named compatibility or credible intervals so they are not mixed up. Criticism of traditional confidence intervals. One sees that a common interpretation of confidence intervals is that with a CI of 95%, means that there is a 95% probability of the true value lying within the interval. THIS IS WRONG, that is a Bayesian interpretation and can only be used in a Bayesian setting. This is actually about what if you repeat an experiment, then 95% of the computed intervals will include the ‘true’ value. See page 58. Criticims of ‘true’ values. Remember that you are working in a small world and thus true answers can never really be found, these belong in the large world. 3.2.3 Point Estimates Point estimates are the third and final common summary task for the posterior distribution. Often this is not wanted, as point estimates will remove valuable information. The following are examples of getting the point estimates: par(mfrow = c(1,1)) plot(x = p_grid,y = posterior,type = &#39;l&#39;,main = &quot;Posterior distribution&quot;,sub = &quot;Showing different point estimates&quot;) grid() abline(v = p_grid[ which.max(posterior) ],col = &quot;darkblue&quot;) abline(v = chainmode( samples , adj=0.01 ),col = &quot;darkgreen&quot;) abline(v = mean( samples ),col = &quot;darkred&quot;) abline(v = median( samples ),col = &quot;darkorange&quot;) legend(&quot;topleft&quot;,legend = c(&quot;max posteriod&quot;,&quot;mode&quot;,&quot;mean&quot;,&quot;median&quot;),lty = 1,col = c(&quot;darkblue&quot;,&quot;darkgreen&quot;,&quot;darkred&quot;,&quot;darkorange&quot;)) #R Code 3.14 to 3.16 p_grid[ which.max(posterior) ] chainmode( samples , adj=0.01 ) #The mode: i.e., the most often appearing value mean( samples ) median( samples ) ## [1] 0.6666667 ## [1] 0.6830388 ## [1] 0.6359439 ## [1] 0.6446446 The question is then: what point estimate to use? We can apply a loss function to support the decision. We can find a series of loss given the grid and the loss function. loss &lt;- sapply(X = p_grid ,FUN = function(d) sum(posterior * abs(d-p_grid))) And for one specific point estimate: sum(posterior * abs(0.5 - p_grid)) #0.1640626 ## [1] 0.1640626 To find the point estimate with the lowest loss one can say: p_grid[which.min(loss)] #0.6446446, equal to the median ## [1] 0.6446446 There are naturally also other loss functions, e.g., \\((d-p)^2\\), which would lead to the posterio mean. 3.3 Sampling to simulate prediction We can sample data to simulate the observations from the model. It has the following advantages: Model Design: One can sample from the prior and see what one expects. This we will look more into in a later section. Model Checking: To see if you end up with the same model. Software Validation: One can use it to simulate the data that the models was built on. To check if the model can replicate the underlying data. I guess this is to check if something is broken. Research Design: Forecasting: One can simulate what will happen in the future. The following is an overview of how to simulate observations and make model checks: 3.3.1 Dummy Data This is basically drawing data given a certain probability of the different outcomes. One must remember that the outputs of such are small world numbers. The folo dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=9 #Size of each set ,prob=0.7) #70% water simplehist( dummy_w , xlab=&quot;dummy water count&quot; ) We see that we get mostly combinations with 6 or 7 water. 3.3.2 Model Checking This has two purposes: Ensure that the model fit worked correctly Evaluate the adequacy of a model for some purpose Did the software work? This is basically just to check if you have set it up correctly. Is the model adequate? There are no true models, hence you need to assess where the model fails to describe the data. One also experience that models tend to be overconfident. One wants to sample from the distribution to see if the model can be replicated. If we end up seeing very different results, then one should start considering is somthing is not taken into account. 3.4 Exercises 3.4.1 3M1 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before. length = 100 # define grid p_grid &lt;- seq(from=0 , to=1 , length.out = length) # define prior prior &lt;- rep(1, length) #The flat (stupid) prior # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 8 #Successes, water in this example ,size = 15 #No. of tosses ,prob = p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = p_grid[which.max(posterior)],col = &quot;darkred&quot;,lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) We see that the posterior distribution is now centered almost around 50%, as we almost in every second case see water. 3.4.2 3M2 Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p. First we draw 10.000 samples and then we can calculate the HDPI (higest posterior density interval) library(rethinking) samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.9) par(mfrow = c(1,1)) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = hdpi[1],lty = 2) abline(v = hdpi[2],lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) 3.4.3 3M3 Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses? Posterior predictive check = to inspect the posterior and see if it actually makes sense. par(mfrow = c(1,2)) plot(samples) dens(samples) mean(samples) ## [1] 0.5292899 dbinom(x = 8,size = 15,prob = mean(samples)) ## [1] 0.202942 We see that there is a 0.2 probability that you select 8 water in a total of 15 tosses. 3.4.4 3M4 Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses. dbinom(x = 6,size = 9,prob = mean(samples)) ## [1] 0.19262 We see a 0.19 probability of getting 6 water in a total of 9 tosses. 3.4.5 3M5 Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7. length = 100 # define grid p_grid &lt;- seq(from=0 , to=1 , length.out = length) # define prior prior &lt;- c(rep(0,length/2),rep(1,length/2)) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 8 #Successes, water in this example ,size = 15 #No. of tosses ,prob = p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = p_grid[which.max(posterior)],col = &quot;darkred&quot;,lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) #Calculating HDPI samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.9) abline(v = hdpi[1],lty = 2,col = &quot;darkblue&quot;) abline(v = hdpi[2],lty = 2,col = &quot;darkblue&quot;) Now we would see that the probability of drawing water is relatively higher than what we have seen before. Although that does not mean that one cannot draw land and there is also a chance of drawing only land. Calculating probability of different outcomes dbinom(x = 8,size = 15,prob = mean(samples)) ## [1] 0.1711683 We see that this went from a bit above 20% to almost 17% The following scenario increased. dbinom(x = 6,size = 9,prob = mean(samples)) ## [1] 0.2554195 We see that it is higher now, we went from 0.19 to 0.25. We see that the scenario of getting relatively more water is higher. 3.4.6 3H1 Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability? 1 = Male, 0 = Female. Ande birth 1 and 2 are two different datasets. library(rethinking) data(homeworkch3) sum(birth1) + sum(birth2) ## [1] 111 This means that there are 111 boys in the two datasets. length = 100 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(111 ,size=200 ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting plot(prior,type = &#39;l&#39;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of a boy&quot; ,ylab=&quot;posterior probability&quot; ) abline(v = p_grid[which.max(posterior)],col = &#39;darkgreen&#39;) 3.4.7 3H2 Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals. samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.5) hdpi ## |0.5 0.5| ## 0.5454545 0.5858586 # |0.5 0.5| #0.5454545 0.5858586 rethinking::HPDI(samples,prob = 0.89) ## |0.89 0.89| ## 0.5050505 0.6060606 rethinking::HPDI(samples,prob = 0.97) ## |0.97 0.97| ## 0.4848485 0.6262626 3.4.8 3H3 Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome? par(mfrow = c(2,1)) dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=200 #Size of each set ,prob=(111/200)) #55% boys dens(dummy_w ,adj = 0.7 #The higher the smoother, def = 0.5 ) hist(dummy_w) We see from the plot that it is centered around 111. We can look at the mean and median in the following. mean(dummy_w) median(dummy_w) ## [1] 110.984 ## [1] 111 We see that it is very close to 111 and the median is 111. 3.4.9 3H4 Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light? sum(birth1) ## [1] 51 dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=100 #Size of each set ,prob=(111/200)) #55% boys dens(dummy_w,adj = 0.7) abline(v = sum(birth1),col = &quot;darkred&quot;) hist(dummy_w) Now we see that the the densest part of the density plot is left skewed compared to the first born boys. 3.4.10 3H5 The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data? We know that it is a girl if the first born = 0. Thus we want to subset on these df &lt;- birth2[birth1 == 0] #Boys following girls dummy_w &lt;- rbinom(n = 100000 #No of observations ,size = length(birth1)-sum(birth1) #Size of each set ,prob = (111/200)) #55% boys dens(dummy_w,adj = 0.7) abline(v = sum(df) #no. of boys following girls ,col = &quot;darkred&quot;) hist(dummy_w) "],["geocentric-models.html", "4 Geocentric Models 4.1 Why Normal distributions are normal 4.2 A language for describing models 4.3 Gaussian model of height 4.4 Linear prediction 4.5 Curves form lines 4.6 Exercises", " 4 Geocentric Models This chapter introduce lienar regression as a Bayesian procedure. Thus we will apply probability measures for intepretation, as that is what Bayesian is. 4.1 Why Normal distributions are normal This section examplifies why normal distributions are normal and how it is often seen in nature. Jesper shows an example where binomial random draws will tend to be centered around 0 as there are the most paths leading towards the middle. That is why we often find the normal distribution in real life. It express that normal distributions come from addition of random events, e.g., the result of consecutive coin tosses. Also the product of small numbers approximate addition, hence the result of such outcomes is similar to the added scenario. Although with large numbers we will tend not to get a normal distribution by finding the product hereof. The book aim to teaach a strategy to model data and not a single model just for the toolbox. 4.2 A language for describing models This chapter summarize how models are defined and defines mathematical terminalogy. Recipe of defining models Recognize the variables that you want to understand. Observable variables = data. While unobservable things = parameters, e.g., averages and rates, e.g., you dont observe GDP growth rates, but it can be deducted from assessing varaibles (observed data). Each variable can be defined in terms of other variables or in terms of a probability distribution. This enables one to learn about associations between variables. The combination of variables and their probability distributions defines a joint generative model. This can be used to simulate hypothetical observations as well as analyze real ones. Mathematical terminology: We see that \\(W \\sim Binomial(N,p)\\) means that W (water) variable is binomial, it has two outcomes, yes or no. and then we have \\(p ~ Uniform(0,1)\\), means that p (proportion of water on the globe) is between 0 and 1. Notice that the wavy symbol ~ means that the models are stochastic, i.e., there are no instances on the left that are known with certainty. I.e., W is distributed as binomial Regarding priors: The priors must be set before you see the data, as if you did not do that, then it is no more a prior. If you have no idea what to set as a prior, then you set it to some random value, e.g., making it uniform to be within a specific range. 4.3 Gaussian model of height The following is an example to understand normal distributions of normal distributions. data(Howell1) d &lt;- Howell1 #d for data frame str(d) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... We see that we have height, wiehgt, age and male. This can also be summarized with mean, sd, percentiles and histograms. precis(d) x 138.2635963 35.6106176 29.3443934 0.4724265 x 27.6024476 14.7191782 20.7468882 0.4996986 x 81.108550 9.360721 1.000000 0.000000 x 165.73500 54.50289 66.13500 1.00000 x ▁▁▁▁▁▁▁▂▁▇▇▅▁ ▁▂▃▂▂▂▂▅▇▇▃▂▁ ▇▅▅▃▅▂▂▁▁ ▇▁▁▁▁▁▁▁▁▇ And thus we see the distributions. We are only going to be working with people above or equal to 18 years of age. Notice that we cannot use the histograms to suggest a distribution, that is because we need to select the priors on berforehand. The precis function also shows the compatibility intervals (recall the probability intervals) d2 &lt;- d[d$age &gt;= 18,] 4.3.1 The model Now we can define the model with with the following terms: \\[h_i \\sim Normal(\\mu,\\sigma)\\] \\(h_i \\sim \\mathcal{N}\\), this is exactly the same, just shortened. Meaning that height is stochastic, given the wavy character ~, it is normal with mu and sigma mean and standard deviation. The i means each element in vector h. Now we can specify the priors. this is done independently for each parameter (unobserved). It looks the following: We see that we have the likelihood that consist of the priors, which are specified afterwards. This can be plotted with. It means that the mean is centered in 178 and 20 standard deviation, thus two standard deviations (95% being 40), we will have 95% of the people within 178 +- 40. curve(dnorm(x,178,20),from = 100,to = 250,main = &#39;Mean prior&#39;) curve(dunif(x,0,50),from = -10,to = 60,main = &quot;Sigma prior&quot;) Now one can sample heights based on the two priors. samples &lt;- 10000 sample_mu &lt;- rnorm(samples,178,20) #notice we cannot take the mean of the data, as it is a prior!!! sample_sigma &lt;- runif(samples,0,50) prior_h &lt;- rnorm(samples,sample_mu,sample_sigma) dens(prior_h) One see that the mean is around the 178. The more samples we draw, the more normal will it look. Now this makes sense since the mean standard deviation is rather low, lets look at an example with a large standard deviation to the mean. samples &lt;- 10000 sample_mu &lt;- rnorm(samples,178,100) #notice we cannot take the mean of the data, as it is a prior!!! prior_h &lt;- rnorm(samples,sample_mu,sample_sigma) dens(prior_h) Now we see that the height can go all the way up to 600 and down to a negative number. When you have a lot of data instances, then such a prior is not harmfull even though it clearly expects a lot of people being very tall and some even below 0. So having an unreasonable prior is not necessarily bad. 4.3.2 Grid approximation of the posterior distribution Notice that in practice we dont really do this, as it is computationally heavy. Although it is shown for explanatory reasons. mu.list &lt;- seq(from=150, to=160, length.out=100) sigma.list &lt;- seq(from=7, to=9, length.out=100) #Expand list, mu and sigma, post &lt;- expand.grid(mu = mu.list, sigma = sigma.list) post$LL &lt;- sapply(1:nrow(post), function(i) sum( dnorm(x = d2$height ,mean = post$mu[i] ,sd = post$sigma[i] ,log = TRUE) ) ) post$prod &lt;- post$LL + dnorm( post$mu , 178 , 20 , TRUE) + dunif(post$sigma,0, 50, TRUE) post$prob &lt;- exp(post$prod - max(post$prod)) Now, lets inspect the posterior distribution par(mfrow = c(2,1)) #A contour plot contour_xyz(post$mu, post$sigma, post$prob) #A heatmap image_xyz(x = post$mu,y = post$sigma,z = post$prob) What we get from this, is the most probable combinations of the mu and sigma. Hence we see that very often the mean will be between 154 and 155 while the standard deviation is between 7.5 and 8. 4.3.3 Sampling from the posterior Now we are going to sample from the posterior, just as in the globe tossing problem. Here we are just also to sample from the mean and standard deviation. This is done by first making a set of samples, where each row number is listed, basically just an index. Then we use the index to return the given mean and standard deviation. sample.rows &lt;- sample(1:nrow(post),size = 10000, replace = TRUE) #We can draw the same instance twice sample.mu &lt;- post$mu[sample.rows] sample.sigma &lt;- post$sigma[sample.rows] We see that post$mu and sigma is a grid ranging from respectively 150 to 160 and 7 to 9. e.g., m &lt;- matrix(post$mu,nrow = 100,ncol = 100) head(m[1:10,1:10],n = 10) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 ## [2,] 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 ## [3,] 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 ## [4,] 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 ## [5,] 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 ## [6,] 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 ## [7,] 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 ## [8,] 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 ## [9,] 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 ## [10,] 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 ## [,9] [,10] ## [1,] 150.0000 150.0000 ## [2,] 150.1010 150.1010 ## [3,] 150.2020 150.2020 ## [4,] 150.3030 150.3030 ## [5,] 150.4040 150.4040 ## [6,] 150.5051 150.5051 ## [7,] 150.6061 150.6061 ## [8,] 150.7071 150.7071 ## [9,] 150.8081 150.8081 ## [10,] 150.9091 150.9091 Now lets plot the samples. par(mfrow = c(1,1)) plot(sample.mu,sample.sigma ,cex = 0.5,pch = 16 #Dot sizes and shape ,col = col.alpha(rangi2,0.1) #make transparant colors ) Now we can inspect the priors. par(mfrow = c(2,1)) dens(sample.mu) dens(sample.sigma) These should have been more or less normally distributed according to the book, but for some reason they are not. 4.3.4 Finding the posterior distribution with quap This is using quadratic approximation. data(&quot;Howell1&quot;) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] We can spcify the model with the following: \\[h_i \\sim Normal(\\mu,\\sigma)\\] \\[\\mu \\sim Normal(178,20)\\] \\[\\sigma \\sim Uniform(0,150)\\] While the equivilent in code is: height ~ dnorm(mu,sigma) mu ~ dnorm(178,10) sigma ~ dunif(0,50) This can be placed in a list: flist &lt;- alist( height ~ dnorm(mu,sigma) ,mu ~ dnorm(178,10) ,sigma ~ dunif(0,50) ) Now we can fit the model to the data: m4.1 &lt;- quap(flist,data = d2) precis(m4.1) x 154.636775 7.731408 x 0.4117468 0.2913949 x 153.978724 7.265703 x 155.294826 8.197114 4.3.5 Sampling from a quap Now we are going to sample from the quadratic approximation. Covariances is key to quadratic approximation. This can be found with: vcov(m4.1) ## mu sigma ## mu 0.1695354471 0.0008701619 ## sigma 0.0008701619 0.0849109920 This tells us how each parameter (unobserved) relates to every other parameter in the posterior distribution. In this scenario covariance does not matter a lot, but when we have a predictor variable, it will be key. This can be extended to 2 elements: A vector of variances for the parameters A correlation matrix that tells us how changes in any parameter lead to correlated changes in others. diag(vcov(m4.1)) cov2cor(vcov(m4.1)) ## mu sigma ## 0.16953545 0.08491099 ## mu sigma ## mu 1.000000000 0.007252502 ## sigma 0.007252502 1.000000000 We see that a change of 1 in mu will lead to a change of 1 in mu, and it will lead to a change of 0.0018 in sigma. This is very small, hence we can see that there is no correlation between the two of these. Thus, learning the mean will not tell anything about the standard deviation. Getting samples post &lt;- extract.samples(m4.1,n = 10000) head(post) mu sigma 154.2506 7.260139 155.2653 7.831658 155.2295 8.015096 155.4242 7.819525 154.6868 8.088762 154.5212 8.095557 precis(post) x 154.633205 7.736503 x 0.4122893 0.2928388 x 153.971768 7.265452 x 155.291918 8.206378 x ▁▁▅▇▂▁▁▁ ▁▁▁▂▅▇▇▃▁▁▁ We can see that distributions look more or less normal. plot(post,pch = 20 ,col = col.alpha(rangi2,0.1)) 4.4 Linear prediction The procedure that we end up going through: 1. Use link to generate distributions of posterior value for mu. 2. Use summary functions like mean or PI to find averages and lower and upper bounds of mu, for each value of the predictor variable. 3. Finally, use plotting functions like lines and shade to draw the lines and intervels. Or you might plot the distributions of the predictions, or do even something else. Now we are going to regress one variable on another, hence we have a predictor and an outcome variable. The following example is predicting height given the weight. plot(d2$height ~ d2$weight) There is clearly a relationship. 4.4.1 The lienar mdoel strategy To specify this model we will say: let x = weight, thus \\(\\bar{x}\\) is the average weight parameter of the observed variable. Now we see that heights are stochastic and normally distributed and is described by the mean and standard deviation. To predict height, we need the mean of the weight variable. This one is deterministic hence the = and not the ~ sign. It consists of the priors (assumptions), where we express \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). These are all stochastic, where \\(\\alpha\\) and \\(\\beta\\) are normally distributed while \\(\\sigma\\) is uniform. Now one can simulate the priors. set.seed(2971) N &lt;- 100 a &lt;- rnorm(N,178,20) #notice we cannot take the mean of the data, as it is a prior!!! b &lt;- rnorm(N,0,10) This returns 100 pairs of \\(\\alpha\\) and \\(\\beta\\). This can be plotted with: plot(NULL ,xlim=range(d2$weight), ylim=c(-100,400) ,xlab=&quot;weight&quot;, ylab=&quot;height&quot; ) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar) ,from=min(d2$weight), to=max(d2$weight) ,add=TRUE ,col=col.alpha(&quot;black&quot;,0.2)) We see that that we get many predictions where we expect a person to be higher than the worlds largest and smaller than the worlds smallest. We can address with by taking the logarithm, which is done in the following: Hence we see that log is added to the equation. b &lt;- rlnorm(10000,0,1) dens(b,xlim = c(0,5),adj=0.1) Doing the prior predictive simulation again, to compare with the logarithm of beta. par(mfrow = c(1,2)) set.seed(2971) N &lt;- 100 a &lt;- rnorm(N,178,20) b &lt;- rnorm(N,0,10) plot(NULL,xlim=range(d2$weight), ylim=c(-100,400),xlab=&quot;weight&quot;, ylab=&quot;height&quot; ) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar),from=min(d2$weight), to=max(d2$weight),add=TRUE,col=col.alpha(&quot;black&quot;,0.2)) #With beta logarithm set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) b &lt;- rlnorm(N, 0, 1) #RLNORM for logarithm plot(NULL ,xlim=range(d2$weight), ylim=c(-100,400),xlab=&quot;weight&quot;, ylab=&quot;height&quot;) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar),from=min(d2$weight), to=max(d2$weight),add=TRUE,col=col.alpha(&quot;black&quot;,0.2)) (#fig:4.41)Left = Initial model, Right = logarithm of beta Now we see that the joint prior for \\(\\alpha\\) and \\(\\beta\\) are realistict. Joint prior = The predicted height given the wiehgt, which rely on alpha and beta priors that we have made. Then, what is the correct prior? It is a fallacy that there is one unique value that is optimal for the prior. Thus one must reason for the selection of the prior and perhaps try different priors to see what it suggest. Essentially the prior is just information that you give the model and can e.g., work as a constrain, as we see in the example above where the priors initially returned extreme values, which we need to tamper down. Then one could say that we should compare the predictions with the actual sample and then optimize against this. Although one must be weary, as this will just yield to fitting against the sample, and is likely not to be the correct model. Although it boiles down to the purpose of the model. 4.4.2 Finding the posterior distribution We see that we have the following model specification: Notice that = is exchanged with &lt;-, that is by convention and must be used when specifying the model in R. data(&quot;Howell1&quot;) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] xbar &lt;- mean(d2$weight) #Fit the model m4.3 &lt;- quap( alist( height ~ dnorm(mu,sigma) ,mu &lt;- a + b * (weight - xbar) #Notice each weight is subtracted by the mean, the closer to the mean the smaller effect of b ,a ~ dnorm(178,20) ,b ~ dlnorm(0,1) ,sigma ~ dunif(0,50) ) ,data = d2 ) 4.4.3 Interpreting the posterior distribution One can interpret the posterior distribution in two ways: By assessing tables of information Plotting the posterior distributions It is often easiest to deduct conclusions based on the plots. The following make examples of both the tables and the plots. 4.4.3.1 Tables fo marginal distributions Marginal posterior distribution of the parameters: precis(m4.3 ,prob = 0.89 #PI for 89% percent, also default ) x 154.6013671 0.9032807 5.0718809 x 0.2703077 0.0419236 0.1911548 x 154.1693633 0.8362787 4.7663786 x 155.0333710 0.9702828 5.3773831 For example we see that the mean height increase by a factor of 0.9 if one person is 1 kg heavier, hence the heavier, the taller. The 5.5% and 94.5% indicate that 89% percent of the time one gets between 84cm and 97 cm taller if one is 1kg heavier. One can also assess the covariances: round(vcov(m4.3),3) ## a b sigma ## a 0.073 0.000 0.000 ## b 0.000 0.002 0.000 ## sigma 0.000 0.000 0.037 And we see that there is very little covariance among the parameters. This is a visual representation of the same. pairs(m4.3) 4.4.3.2 Plotting posterior inference against the data First the raw data is plotted: par(mfrow = c(1,1)) plot(height ~ weight,data = d2,col = rangi2) post &lt;- extract.samples(m4.3) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) curve(expr = a_map + b_map * (x - xbar),add = TRUE) #Ability to plot a function We see that the function we defined based on the posterior seem reasonable, although there are many plausible lines. Hence we are going to look into dealing with uncertainty. 4.4.3.3 Adding uncertainty around the mean We are going to make many of the lines to interprete where they end up, hence also reflecting uncertainty. post &lt;- extract.samples(m4.3) #Default = 10.000 post[1:5,] a b sigma 154.5789 0.9376825 5.220756 154.4067 0.8937310 4.752735 154.4622 0.9150822 5.341227 154.2649 0.9236067 5.160423 155.1258 0.9495934 5.108891 We see that these are basically all different functions that can be plotted to show the uncertainty. N &lt;- 150 #No. of samples dN &lt;- d2[1:N,] #subsetting #Approximating the the mN &lt;- quap( alist( height ~ dnorm( mu , sigma ) ,mu &lt;- a + b*( weight - mean(weight) ) ,a ~ dnorm( 178 , 20 ) ,b ~ dlnorm( 0 , 1 ) ,sigma ~ dunif( 0 , 50 ) ) ,data=dN ) The following will show one example where we loop of the data data fit the line. n &lt;- 20 #No of loops # extract n samples from the posterior distribution post &lt;- extract.samples(mN ,n=n) # display raw data and sample size plot(x = dN$weight ,y = dN$height ,xlim=range(d2$weight),ylim=range(d2$height) ,col=rangi2 ,xlab=&quot;weight&quot;,ylab=&quot;height&quot;) mtext(concat(&quot;N = &quot;,N,&quot;, Iterations = &quot;,n)) # plot the lines, with transparency for ( i in 1:n) #Draw a and b values from the subset of the data curve(post$a[i] + post$b[i]*(x-mean(dN$weight)) ,col=col.alpha(&quot;black&quot;,0.3) ,add=TRUE) One will see that the more observations we include, the more certain will the model become. One will often experience that we are more confident around the mean and less in the ends of the x-range. In other words, the less data we introduce, the less we rely on the priors that we specified before seeing the data. that is because the prior is updated according to the data. Notice that confidence is not equal to correctness that is due to the data that we have may not reflect a truer perspective than the priors. 4.4.3.4 Plotting regression intervlas and contours Now lets start an example where the weight is fixed to 50 kg. We will see that a person with 50kg is not fixed to one height, but some will have greater certainty. We see that we get 10.000 samples, thus 10.000 priors for both a and b and thus we can simulate the expected height for such a given person. post &lt;- extract.samples(m4.3,n = 10000) mu_at_50 &lt;- post$a + post$b * (50-xbar) dens(x = mu_at_50,col = rangi2,lwd = 2,xlab = &quot;mu|weight=50&quot;) Now what we want to do is the same, but for all weights. For this one can use the link function. mu &lt;- link(m4.3,n = 1000) #1000 is also default str(mu) ## num [1:1000, 1:352] 158 157 157 157 157 ... We see that 352 roes in the data hence we get a matrix 352 columns (one for each individual) with 1.000 rows. # define sequence of weights to compute predictions for these values will be on the horizontal axis weight.seq &lt;- seq(from=25, to=70, by=1) # use link to compute mu for each sample from posterior and for each weight in weight.seq mu &lt;- link( m4.3 , data=data.frame(weight=weight.seq) ) str(mu) ## num [1:1000, 1:46] 136 136 136 137 137 ... Now we see that since we fed 46 values for the weight, we get 46 columns instead of one pr invidual. plot(height ~ weight,data = d2,type=&quot;n&quot;) #use type=&quot;n&quot; to hide raw data # loop over samples and plot each mu value for (i in 1:100) points(weight.seq, mu[i,] ,pch=16, col=col.alpha(rangi2,0.1)) Finally we will summarize the distribution for each weight value. # summarize the distribution of mu mu.mean &lt;- apply(mu, 2, mean) #46 values, one for each weight mu.PI &lt;- apply(mu, 2, PI, prob=0.89) #The 89% PI for each mean mu.mean = the average height (mu) for each weight value. And the PI is just accompanied with this as well. Now we plot the means and the PI ontop of the data. #Raw data plot plot(height ~ weight, data = d2, col = col.alpha(rangi2,0.5)) #Ploatting MAP line (mean mu for each weight) lines(x = weight.seq,y = mu.mean) #Plotting PI intervals for each weight shade(mu.PI,weight.seq) NOTICE THAT THIS IS PREDICTION OF AVERAGE HEIGHTS, IN THE FOLLOWING WE MAKE INTERVALS FOR ACTUAL HEIGHTS 4.4.3.5 Prediction intevals Now we are going to predict actual heights. Also notice that up until now, we have not had to simulate sigma, as it is only relevant for predicting actual heights, and as we saw that mean heights are dependent (deterministic) on alpha and beta prior, but not on sigma. This comes down to, what is sigma, it is an expression of the scatter of the points. The following is an example of actual predictions and generating a band of percentile interval. #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m4.3, data=list(weight=weight.seq)) #n = 1000 str(sim.height) ## num [1:1000, 1:46] 136 131 140 139 136 ... This contains simulated heights and not distributions as we saw previously. Now we can generate the PI that we are going to plot. height.PI &lt;- apply(sim.height,2,PI,prob = 0.89) height.HPDI67 &lt;- apply(sim.height,2,HPDI,prob = 0.67) height.HPDI89 &lt;- apply(sim.height,2,HPDI,prob = 0.89) height.HPDI97 &lt;- apply(sim.height,2,HPDI,prob = 0.97) Lastly we need to plot the predictions and the percentile interval. #Plotting the data points plot(height ~ weight,data = d2,col = col.alpha(rangi2,0.5)) #Draw MAP line lines(weight.seq,y = mu.mean) #Draw HPDI region for simulated heights, notice I added two additional regions shade(height.HPDI67,weight.seq) shade(height.HPDI89,weight.seq) shade(height.HPDI97,weight.seq) Now we see that the region is far wider. 4.5 Curves form lines This is technically the same, but we add complexity in the form of more predictors. We approach this with polynomial regression and splines. 4.5.1 Polynomial regression This is basically using the same variable, but transforming it into second or third order polynomials. When doing polynomials you are at risk of generating very large numbers, hence one should standardize the variable to avoid this. This also means that interpreting the effects from each parameter is more difficult. Now we can specify the model with the following: Notice that this is for a cubic polynomial regression. One can just disregard $/beta_3$ in a quadratic polynomial. The following will plot the example of: #Standardize weights d$weight_s &lt;- ( d$weight - mean(d$weight) )/sd(d$weight) #Add vector of squared values, for the polynomial variable d$weight_s2 &lt;- d$weight_s^2 d$weight_s3 &lt;- d$weight_s^3 #For the cubic model #Specfify models #Linear model m4.4 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) ,data = d ) #Quadratic model (4.65) m4.5 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s + b2*weight_s2 , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , b2 ~ dnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) ,data = d ) #Cubic model (4.69) d$weight_s3 &lt;- d$weight_s^3 m4.6 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , b2 ~ dnorm( 0 , 10 ) , b3 ~ dnorm( 0 , 10 ) , sigma ~ dunif( 0 , 50 ) ) ,data=d ) #Plotting par(mfrow = c(1,3)) #Linear weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq ) #adds degree of poly mu &lt;- link( m4.4 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.4 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) #Quadratic weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq , weight_s2=weight.seq^2 ) #adds degree of poly mu &lt;- link( m4.5 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.5 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) #cubic weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq , weight_s2=weight.seq^2, weight_s3=weight.seq^3 ) #adds degree of poly mu &lt;- link( m4.6 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.6 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) Figure 4.1: Comparison between a linear, quadratic and cubic model. Notice that the same variable is used, just with different degree of polynomials. Table output for the polynomial precis(m4.5) x 146.057160 21.733407 -7.803209 5.774538 x 0.3689809 0.2888934 0.2741871 0.1764700 x 145.467457 21.271700 -8.241412 5.492504 x 146.646862 22.195114 -7.365005 6.056571 Now we see that b2 for instance is more complicated, as it is squared values that the parameter is multiplied with. Notice that a is still the intercept with y. Also since the same variable is in the model more than once, one cannot interpret one without the other. Arguments for why polynomials are bad: If you have regions with no data, the model can do all kinds of weird things Makes absurd predictions outside range of data It is likely to get some obscure shapes The model is difficult to interpret as you cannot interpret one parameter without the other,as the polynomial variable is tied to the non polynomial variable. Often having a polynomial makes no sense in real life, even though you may find a relationship In a quadratic model, the fitted lines will always go up and then down, hence it needs to fit the data in that way. The same applies to cubic functions. Actually not very flexible and cannot have monotone shape The spline that we see in the following is the answer to many of these difficulties. 4.5.2 Splines This is an alternative way of adding curvature. We are going to use basis splines, i.e., B-splines. This is also called P-splines, this is for penalty-splines, as the priors adds a penalty. The code is not exemplified yet. Do this. For now, see ?? It is basically just splines as we have seen earlier in ML. It is often a much better model and polynomial regression. During the lecture he shows a really nice representation of which of the basis functions that are active. 4.6 Exercises 4.6.1 4M1 For the model definition below, simulate observed y values from the prior (not the posterior). \\[y_i \\sim Normal(\\mu,\\sigma)\\] \\[\\mu \\sim Normal(0,10)\\] \\[\\sigma \\sim Exponential(1)\\] set.seed(1337) N &lt;- 100 mu &lt;- rnorm(n = N,0,10) #notice we cannot take the mean of the data, as it is a prior!!! sigma &lt;- dexp(1) prior_h &lt;- rnorm(N,mu,sigma) dens(prior_h) Note, in this example dnorm and rnorm is exactly the same Plotting the priors 4.6.2 4M8 In the chapter, we used 15 knots with the cherry blossom spline. 1. Increase the number of knots and observe what happens to the resulting spline. 1. Then adjust also the width of the prior on the weights 1. change the standard deviation of the prior and watch what happens. 1. What do you think the combination of knot number and the prior on the weights controls? #Creating a function for the exercise, so you dont have to run it all again to make modifications load_execute &lt;- function( N_knots ,model_spec = alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(6,10) ,w ~ dnorm(0,1) ,sigma ~ dexp(1) ) ) { library(rethinking) #Load data data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp #precis(d) #For inspections #Defining spline specifications num_knots &lt;- N_knots knot_list &lt;- quantile(x = d$year #Data ,probs = seq(0,1,length.out=num_knots) #probabilities ) knot_list &lt;- knot_list[-c(1,num_knots)] #Remove knot at 0 and 1 #Defining the splines library(splines) B &lt;- bs( x = d$year ,knots = knot_list ,degree = 3 #cubic splines ,intercept = TRUE ) #Define model m &lt;&lt;- quap( model_spec ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) #precis(m,depth = 2) } From this we can see all the parameters at the knots, although this does not provide much information anymore before plotting. We see the effect of each spline. Now we can plot the actual fitted lines and the compatibility interval. par(mfrow = c(3,1)) data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp for (i in seq(10,30,10)){ load_execute(N_knots = i) mu &lt;- link(m) mu_PI &lt;- apply(X = mu,MARGIN = 2,FUN = PI,0.97) #0.97 is input to PI #Adding raw data plot(d$year,d$temp ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;Year&quot; ,ylab = &quot;Temp&quot; ,main = paste(&quot;No. of knots:&quot;,i,sep = &quot; &quot;) ) #Adding shade for the intervals shade(object = mu_PI ,lim = d$year ,col = col.alpha(&quot;black&quot;,0.5) ) } 4.6.3 4H1 The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions. First we define the model to get the parameters, such as mean for beta. Recall that a = the intercept with y. library(rethinking) data(Howell1) d &lt;- Howell1 d2 &lt;- d[d$age&gt;=18,] xbar &lt;- mean(d2$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar), a ~ dnorm( 178 , 20 ), b ~ dlnorm( 0 , 1 ), sigma ~ dunif( 0 , 50 ) ), data=d2 ) precis(m) #similar to summary() x 154.6013676 0.9032809 5.0718805 x 0.2703076 0.0419236 0.1911547 x 154.1693637 0.8362789 4.7663783 x 155.033371 0.970283 5.377383 From this we are able to extract samples: post &lt;- extract.samples(m) str(post) ## &#39;data.frame&#39;: 10000 obs. of 3 variables: ## $ a : num 155 155 155 154 155 ... ## $ b : num 0.95 0.916 0.93 0.892 0.841 ... ## $ sigma: num 5.25 4.87 4.93 5.2 5.11 ... ## - attr(*, &quot;source&quot;)= chr &quot;quap posterior: 10000 samples from m&quot; Function to get n samples based on the input weight, it returns the mean y and the compatability interval f &lt;- function( weight ) { y &lt;- rnorm(n = 10000 , post$a + post$b * (weight - xbar) , post$sigma ) return( c( mean(y) , PI(y,prob=0.89) ) ) } weight_list &lt;- c(46.95,43.72,64.78,32.59,54.63) result &lt;- sapply( weight_list , f ) result ## [,1] [,2] [,3] [,4] [,5] ## 156.3964 153.4632 172.4339 143.3563 163.3754 ## 5% 148.1975 145.3652 164.1295 135.1368 155.3528 ## 94% 164.6221 161.6903 180.7454 151.5014 171.4404 setNames(object = data.frame(cbind(weight_list,t(result))) ,nm = c(&quot;Weight&quot;,&quot;Height&quot;,&quot;5%&quot;,&quot;94%&quot;)) Weight Height 5% 94% 46.95 156.3964 148.1975 164.6221 43.72 153.4632 145.3652 161.6903 64.78 172.4339 164.1295 180.7454 32.59 143.3563 135.1368 151.5014 54.63 163.3754 155.3528 171.4404 We see that the heights has now been estimated. Why we use the function: We need to samples to estimate the percentage intervals, thus sampling is required. One could merely also say \\(\\alpha + \\beta * (weight - \\bar{weight})\\), thus find the point estimate given the different weights. Although that will not reflect the compatibility intervals. Finding point estimates. weight = weight_list[2] m@coef[&#39;a&#39;] + m@coef[&#39;b&#39;] * (weight - xbar) ## a ## 153.4538 4.6.4 4H2 Select out all the rows in the Howell1data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it. data(Howell1) d &lt;- Howell1[Howell1$age&lt;18,] dim(d) ## [1] 192 4 Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets? xbar &lt;- mean(d$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar), a ~ dnorm( 178 , 20 ), b ~ dnorm( 0 , 10 ), sigma ~ dunif( 0 , 50 ) ), data = d ) summary(m) x 108.383568 2.719939 8.437316 x 0.6086646 0.0682926 0.4305876 x 107.410804 2.610794 7.749154 x 109.356332 2.829083 9.125478 We see see that for 1 increase in weight the mean is expected to increase by 2.72. While the 89% combatibiity intervals being 2.61 and 2.83, hence there is a positive effect. Thus by 10 kg, we expect someone to grow by a magnitude of 10. Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights. plot(x = d$weight,y = d$height,pch = 20,main = &quot;Regression, age &lt; 18&quot; ,xlab = &quot;Weight&quot;,ylab = &quot;Height&quot;) grid() #Adding the model predictions post &lt;- extract.samples(m) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) #Ability to plot a function curve(expr = a_map + b_map * (x - xbar) ,add = TRUE #Add to existing plot ,col = &quot;darkblue&quot; ,lty = 2 ) #Weight sequence for getting intervals weight.seq = seq(floor(min(d$weight)),ceiling(max(d$weight)),by = 1) #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m, data=list(weight=weight.seq)) #n = 1000 str(sim.height) ## num [1:1000, 1:42] 63.7 60.3 60.5 45.9 71.7 ... #PI + plot height.PI89 &lt;- apply(sim.height,2,PI,prob = 0.89) shade(height.PI89,weight.seq) What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model. One could try: Polynomial - as it appears as the relationship is not linear. Splines - same reason as above 4.6.5 4H3 Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens. Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1data frame, all 544 rows, adults and non-adults. Can you interpret the resulting estimates? #Load data data(Howell1) d &lt;- Howell1 paste0(&quot;data dimensions: &quot;,dim(d)[1]) #Define and approximate the model xbar &lt;- mean(d$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar), a ~ dnorm( 178 , 20 ), b ~ dlnorm( 0 , 1 ), sigma ~ dunif( 0 , 50 ) ), data = d ) precis(m) ## [1] &quot;data dimensions: 544&quot; x 138.279540 1.763637 9.345902 x 0.4006230 0.0272501 0.2833404 x 137.639267 1.720086 8.893069 x 138.919813 1.807188 9.798734 Same principle as earlier, we see that one grows with 1.76 for each increase in kg of weight. While the 89% plausible outcomes are 1.72 and 1.81. Begin with this plot: plot( height ~ weight , data=Howell1 ). Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% interval for the mean, and (3) the 97% interval for predicted heights. plot(x = d$weight,y = d$height,pch = 20,main = &quot;Regression, all observations&quot; ,xlab = &quot;Weight&quot;,ylab = &quot;Height&quot;) grid() #Adding the model predictions post &lt;- extract.samples(m) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) #Ability to plot a function curve(expr = a_map + b_map * (x - xbar) ,add = TRUE #Add to existing plot ,col = &quot;darkblue&quot; ,lty = 2 ,lwd = 1.5 ) #Weight sequence for getting intervals weight.seq = seq(floor(min(d$weight)),ceiling(max(d$weight)),by = 1) # mean intervals mu &lt;- link(m,data = list(weight = weight.seq)) mu.PI &lt;- apply(X = mu,MARGIN = 2,FUN = PI ,prob=0.97 #input for the function ) shade(object = mu.PI,lim = weight.seq,col = col.alpha(&quot;red&quot;,0.15)) #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m, data=list(weight=weight.seq)) #n = 1000 str(sim.height) #PI + plot intervals for the predictions height.PI97 &lt;- apply(sim.height,2,PI,prob = 0.97) shade(height.PI97,weight.seq,col = col.alpha(&quot;orange&quot;,0.15)) #Adding legend legend(&quot;topleft&quot; ,legend = c(&quot;Linear pred&quot;,&quot;mean PI97&quot;,&quot;pred PI97&quot;) ,col = c(&quot;darkblue&quot;,col = col.alpha(&quot;red&quot;,0.15),col.alpha(&quot;orange&quot;,0.15)) ,lty = 2 ,lwd = 5 ,cex = 0.8 ) ## num [1:1000, 1:60] 94 73.8 93.9 92.8 89.1 ... We see that the expected mean is very close to the linear prediction, while the actual predictions are expected to lie in a wider space. 4.6.6 4H6 Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing? #Load data data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp #Defining spline specifications num_knots &lt;- 15 knot_list &lt;- quantile(x = d$year #Data ,probs = seq(0,1,length.out=num_knots) #probabilities ) knot_list &lt;- knot_list[-c(1,num_knots)] #Remove knot at 0 and 1 #Defining the splines - notice that m is altered in later code to represent other examples B &lt;- bs( x = d$year ,knots = knot_list ,degree = 3 #cubic splines ,intercept = TRUE ) m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,10) ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) ### PLOTTING! par(mar = c(4, 4, .1, .1),mfrow = c(3,1)) #adjust print window ### Top print p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,10)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) ### Middle print m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,5) #This is altered ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,5)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) ### Bottum print m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,1) #This is altered ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,1)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) We see that the smaller we define the width of w, the less wiggly will the lines be. Hence we add more penalty to the movements, i.e., so we generalize more and fit less to the data. "],["lecture-notes-wrap-up-chapters-geocentric-models.html", "5 Lecture notes - wrap up chapters geocentric models", " 5 Lecture notes - wrap up chapters geocentric models Difference between binomial distribution and gaussian distribution, is that you have respectively binomial and continous data. library(dagitty) "],["chapter-5-the-many-variables-the-spurious-waffles.html", "6 Chapter 5 - The Many Variables &amp; The Spurious Waffles 6.1 Spurious Associations 6.2 Masked Relationship 6.3 Categorical Variables 6.4 Lecture notes - not integrated in the notes", " 6 Chapter 5 - The Many Variables &amp; The Spurious Waffles What is the difference between causal and spurious correlation and how do you define it. As you may see that two variables may be correlated, although it does not mean that one affect the other. Therefore we are going to use multiple regression, to account for effects. Even though it can overcome multiple regression, it can also create a spurious relationship. We are going to use DAGs as a backdoor criterion, to find the causes. 6.1 Spurious Associations This section elaborates on the difference between causal and spurious relationships. This is an example # load data and copy library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce # standardize variables d$A &lt;- scale( d$MedianAgeMarriage ) d$D &lt;- scale( d$Divorce ) d$M &lt;- scale( d$Marriage ) m5.1 &lt;- quap( alist( D ~ dnorm( mu , sigma ) , mu &lt;- a + bA * A , a ~ dnorm( 0 , 0.2 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) ,data = d) The following shows plausibile regression lines given the priors. par(mfrow = c(1,1)) set.seed(10) prior &lt;- extract.prior( m5.1 ) mu &lt;- link( m5.1 , post=prior , data=list( A=c(-2,2) ) ) plot( NULL , xlim=c(-2,2) , ylim=c(-2,2),main = &quot;Simulateting priors&quot;) for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha(&quot;black&quot;,0.4) ) Now we can make the posterior prediction. # compute percentile interval of mean A_seq &lt;- seq( from=-3 , to=3.2 , length.out=30 ) mu &lt;- link( m5.1 , data=list(A=A_seq) ) mu.mean &lt;- apply( mu ,MARGIN = 2,FUN = mean ) mu.PI &lt;- apply( mu ,MARGIN = 2 ,FUN = PI ) # plot it all plot( D ~ A , data=d , col=rangi2,main = &quot;Posterior prediction&quot;) lines( A_seq , mu.mean , lwd=2 ) shade( mu.PI , A_seq ) This follows up by expressing that one must be cautious about what variables that are included and if two or more variables correlate, is it merely because of a shared causal effect that the model does not account for? For this DAGs (directed acyclic graphs) can be used. In the following DAG we see that in the left, A has direct influence on D and M, while also an indirect relationship flowing through M. On the right A influence D and M while M and D are independent but has the same parent. Thus a change in A will lead to a change in M and D, thus if you regress the two of these on each other, it is likely that you’ll find a relationship, although this is just spurious as they just appear to follow the same trends, but it cannot be said that one affects the other. The purpose of the DAG is to capture the causality. Defining DAGS DMA_dag2 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) impliedConditionalIndependencies( DMA_dag2 ) ## D _||_ M | A This (D _||_ M | A) means that D is independent on M conditional on A. DMA_dag1 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D }&#39;) impliedConditionalIndependencies( DMA_dag1 ) This has no conditional independence, hence there will be no output. 6.1.1 Defining a multiple regression Lets say we we want to measure divorce rate. Where we have marriage rate and median age at marriage. Then you firs specify the model for the target variable, then to define what the mean consists of, you specify the regression model. We see that alpha = the intercept with y, then \\(\\beta_M\\) is the coefficient for the marriage rate and \\(\\beta_A\\) is the coefficient for the median age at marriage. Lastly we have the standard deviation \\(\\sigma\\), which is the standard deviation from the mean, we need this to make predictions of actual observations and not merely of the mean. 6.1.2 Approximating the posterior m5.3 &lt;- quap( alist( D ~ dnorm( mu , sigma ) , #The likelihood mu &lt;- a + bM*M + bA*A , #Declarative a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) ,data = d ) precis( m5.3 ) x -0.0000110 -0.0653767 -0.6135168 0.7851600 x 0.0970800 0.1507801 0.1509906 0.0778538 x -0.1551636 -0.3063524 -0.8548289 0.6607347 x 0.1551416 0.1755990 -0.3722047 0.9095854 We see that for marriage rate, it can be bothh positive and negative. While age when entering the marriage is negative, hence the higher the age the less probability of divorcing. We saw earlier that age and marriage rate may be correlated, although we see two different effects of these, when the linear models are run separately as above. This can also be visualized in the following. Here we see that M5.2 implies that there is a relationship between marriage rate and divorce, although when we run the multiple regression, we see that this variable points both positive and negative direction (m5.3). Hence marriage rate may be spurious. Thus we see that once we know the meadian age of marriage, there is no or little value in marriage rate, although if we dont know median age of mariage, there is still information to gain from the marriage rate. To influence the rate of divorce, one could legislate the age at marriage, but the marriage rate will not have effect. Thus it is important to consider whether we are doing inference or prediction. 6.1.3 The flow Define the model, the equations Define the priors Visualize the priors Draw samples from the posterior Making posterior predictions Predictor residual plots. From the residuals plots we are able to see if one variable still contains information about the target value, we expect the residuals of on variable to be horizontal, if not, it means that there is still some information to gain from the variable. Counterfactual plots, that is using standardized values. Posterior prediction plots: Now we look at predictions for the actial observed cases. Never analyze the residuals. Reason: we know the that the regression is internally fitting to the model, hence the rest is just the leftovers, thus they should not correlate with the target variable. 6.1.3.1 Posterior Prediction Plots The benefit of checking the model against the predictions, is to see how well the model performs. Now we can produce a simple posterior predictive check # call link without specifying new data # so it uses original data mu &lt;- link( m5.3 ) # summarize samples across cases mu_mean &lt;- apply( mu , 2 , mean ) mu_PI &lt;- apply( mu , 2 , PI ) # simulate observations # again no new data, so uses original data D_sim &lt;- sim( m5.3 , n=1e4 ) D_PI &lt;- apply( D_sim , 2 , PI ) Now we plot the model against the actual observations. plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , xlab=&quot;Observed divorce&quot; , ylab=&quot;Predicted divorce&quot; ) abline( a=0 , b=1 , lty=2 ) #The perfect prediction for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 ) 6.2 Masked Relationship Sometimes association between outcome of one variable is masked by another variable, hence a mediate effect. Thus, we need both variables, to describe the relationship. I think that the intuition is that in a bivariate model, one predictor may not be sufficient dimensionality to predict the variance in Y. Although if you have multiple variables, then one variable can account for what the other could not. Notice that this comes at a cost of more noise and complexity in the model, also we are able to create what is called confounding colliders. 6.3 Categorical Variables See example in the slides. Basiacally we can encode categoricals as: Dummies Indexes He argued, that indexes is the best way of encoding the data. We see that dummies create a coefficient for the given dummy, while using the index merely creates an \\(\\alpha\\) value for each category in the index. Also it makes the model simpler to write out, as you just have to specify the parameter (alpha for instance) for each index, e.g., these are just snippets with relevant lines: \\[ \\mu_i = \\alpha + \\beta_mm_i \\] \\[\\alpha_j \\sim Normal(0,10)\\] \\[\\beta_m \\sim Normal(0,50)\\] Which merely translates to: \\[\\mu_i = \\alpha_{SEX[i]}\\] \\[\\alpha_j \\sim Normal(178,20), for j = 1...2\\] This we see that the mean is alpha given the gender, where this is just given by the normal distribution for the given gender. 6.4 Lecture notes - not integrated in the notes Never peak at the data before you set the priors.This "],["chapter-6-the-haunted-dag-the-causal-terror.html", "7 Chapter 6 - The Haunted DAG &amp; The Causal Terror 7.1 Multicollinearity 7.2 Post Treatment Bias 7.3 Collider Bias 7.4 Confronting Confounding 7.5 Exercises 7.6 Lecture notes - not integrated in the text", " 7 Chapter 6 - The Haunted DAG &amp; The Causal Terror It is called the haunting DAG, as we see that checking different variables, may create colliders, where doing inference, the actual state will be skewed, e.g., see lecture notes @ref(lecture-notes—not-integrated-in-the-text). The following sections elaborates on three hazards: Multicollinearity Post Treatment Bias Collider Bias 7.1 Multicollinearity Multicollinearity is in its core not a bad effect in your model, although you want to avoid it when doing inference. In the litterature, there is an example of predicting heights based on the length of each leg, we see that the sum of the coefficient will add to the mean of the leg length in general. Hence overall the model will predict the right results although inference wise it is counter intuitive. In another example we have two highly correlated predictor variables that goes the opposite way of each other. We know in this example that these are in fact explained by the same (unobserved) variable, hence when you know one, you will also know the other. L = lactose, F = fat. D = density. When the density is low, it has high lactose and low fat and the opposite. Thus if we know F, then we know F and L and if we know D, we will know both L and F, hence we want to only include one of the observed variables into the model. In this example we saw from precis that when each observed variable is regressed individually the mean and compatibility intervals are far on each side of 0, although when both are included it will for both variables be centered, more or less, at 0 and have CI on both sides of 0. Notice that just because two variable correlate, we do not by default want to remove them! Conclusion: Just because you have many causal explanatory variables at hand, it does not mean that you should use all of them. 7.2 Post Treatment Bias did not get this 7.3 Collider Bias For this I will refer to the examples in the book about the grandchildren and the parents influence on childrens education McElreath (2020) pp. 180 - 182. In its basics, when we include one variable, it might also indirectly include information on an unobserverd variable that is explaning both the predictor and the target variable. Hence the effect in the predictor P will implicitly also be reflected in the model. I.e., the unobserved effect will be hidden / under the radar, thus we will not be able to distinguish this effect from the actual effect of P. 7.4 Confronting Confounding No matter the size of the DAG, the model will consist of the following types: There are scenarios we want to avoid etc. That I could elaborate on in the notes The approach to analyzing the DAG: List all paths (paths disregard directions) connecting X and Y Classify each path by whether it is open or closed. A path is open unless it contains a collider Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering X. If there are any open backdoor paths, decide which variable(s) to condition on o close it (if possible). Pages 186 - 187 has examples on scenarios to avoid and how to close backdoors. We have the following example where we want to analyze relationship on W to D. library(dagitty) dag_6.2 &lt;- dagitty( &quot;dag { A -&gt; D A -&gt; M -&gt; D A &lt;- S -&gt; M S -&gt; W -&gt; D }&quot; ) # coordinates(dag_6.2) &lt;- list(x = c(S=3,W=3,M=2,A=0,D=0) # ,y = c(S=0,W=3,M=2,A=0,D=3)) drawdag(dag_6.2) There are 3 open backdoors: S -&gt; A S -&gt; M S -&gt; W These all flow from S and affects either W or D. Solution: is to control for S, which will We can control for this with adjustmentSets(), which will show variables that we should make indepent given we control for a specific variable. adjustmentSets( dag_6.2 , exposure=&quot;W&quot; , outcome=&quot;D&quot; ) ## { A, M } ## { S } We see that we can either control for A and M or S. We can also let the model find the conditional independences by saying: impliedConditionalIndependencies(dag_6.2) ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S Thus we see that A and W are independent given we control for S. 7.5 Exercises 7.5.1 6M1 Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y: C &lt;- V -&gt; Y. Reanalyze the DAG.How many paths connect X to Y? Which must be closed? Which variables should you condition on now? library(dagitty) library(rethinking) dag &lt;- dagitty( &quot;dag { U [unobserved] V [unobserved] X [exposure] Y [outcome] A -&gt; U A -&gt; C U -&gt; B U -&gt; X C -&gt; B C -&gt; Y X -&gt; Y C &lt;- V -&gt; Y }&quot; ) drawdag(dag) Paths (undirected) from X to Y. List all paths (paths disregard directions) connecting X and Y X,Y X,U,B,C,Y X,U,A,C,Y X,U,B,C,V,Y X,U,A,C,V,Y Classify each path by whether it is open or closed. A path is open unless it contains a collider We see that B and C are colliders, hence we will not want to touch those, as it will open up the flows to the leading effects. Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering X. We see that U affects X, hence there is a backdoor. hence we want to close this relationship If there are any open backdoor paths, decide which variable(s) to condition on o close it (if possible). We see that U is a backdoor path, although it is unobserved hence we cannot adjust the model for that. (But can we adjust for A?) 7.5.2 6M3 Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (conditionon) to estimate the total causal influenceof X on Y. We must condition on Z, as the pipe from A to Y is being closed by conditioning on Z. One could Will this also terminate the flow directly from A to Y? (top right): Should not condition on anything. We see that X to Y is causal and open. We see that Z is a collider, hence we dont want that included. (bottom left): We see that Z is a collider, we dont want to control for that, thus by default A and Y are independent. Hence we just include X in the model. We include X and A. We see that in the 4th example that Z is not a collider, that is because if we list all the paths, there will not be any arrows pointing towards each other. X -&gt; Y X -&gt; Z -&gt; Y X &lt;- A -&gt; Z -&gt; Y Where in the third example we had: X -&gt; Z &lt;- Y (We have a collider) X &lt;- A -&gt; Z &lt;- Y (We have a collider) X -&gt; Y library(dagitty) library(rethinking) dag1 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; X -&gt; Y Z -&gt; Y A -&gt; Y }&quot; ) dag2 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; Y A -&gt; Y X -&gt; Z X -&gt; Y }&quot; ) dag3 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; X -&gt; Y A -&gt; Z X -&gt; Z Y -&gt; Z }&quot; ) dag4 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; Y A -&gt; X X -&gt; Y X -&gt; Z }&quot; ) adjustmentSets(x = dag1,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## { Z } adjustmentSets(x = dag2,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## {} adjustmentSets(x = dag3,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## {} adjustmentSets(x = dag4,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## { A } 7.5.3 6H library(rethinking) library(dagitty) data(foxes) dfs &lt;- foxes dfs$group &lt;- scale(dfs$group) dfs$avgfood &lt;- scale(dfs$avgfood) dfs$groupsize &lt;- scale(dfs$groupsize) dfs$area &lt;- scale(dfs$area) dfs$group &lt;- scale(dfs$weight) dag &lt;- dagitty( &quot;dag { weigth [outcome] weigth &lt;- groupsize area -&gt; avgfood -&gt; groupsize avgfood -&gt; weigth }&quot; ) drawdag(dag) 7.5.4 6H3 Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range. library(rethinking) library(dagitty) dag3 &lt;- dagitty( &quot;dag { weigth [outcome] area [exposure] weigth &lt;- groupsize area -&gt; avgfood -&gt; groupsize avgfood -&gt; weigth }&quot; ) drawdag(dag) Possible paths: area &gt; avgfood &gt; weight area &gt; avgfood &gt; groupsize &gt; weight We just need to include Area. 7.5.5 6H4 7.5.6 6H5 7.6 Lecture notes - not integrated in the text There are the following types: The fork: We see that Z is a commen cause, thus when we know the outcome of Z, there is no relationship between X and Y. The pipe: We see that Z is a mediator. We see that if we know (Condition) on Z, we remove the relationship between X and Y. We see that this is very similar to the fork. The collider: We see that there is a relationship from X to Z and Y to Z. Although if we control for Z, there is no relationship between X and Z. Thus, if we make the model as a linear regression, we will form a spurious relationship between X and Y, that we want to avoid. The following is an example. They must both be on, for the light to be on, but if switch is on and electicity is off, then the light is also off, hence if we know one of the parents, then we know the state of the other. Collider confounding: That is to identify if there actually is a relationship between two variables that explain the target variable. We see in the following example, we control for marriage, and the two subsets of data will make the age appear to have a negative effect on happiness, while we know that that is not true (the data was simulated.) Summary: For the pipe we dont want to control for Z, as that will terminate the relationship from C to Y. For the Collider we need to take care and not create a spurious relationship. We want to leave the collider alone, hence we do not touch it. "],["chapter-7-model-comparison.html", "8 Chapter 7 - Model comparison 8.1 The problem with parameters 8.2 Entropy and Accuracy 8.3 Golem Taming: Regularization 8.4 Predicting predictive accuracy 8.5 Model Comparison 8.6 Exercies", " 8 Chapter 7 - Model comparison 8.1 The problem with parameters The section elaborates on overfitting and underfitting. It introduce \\(R^2\\) and some reasoning for AIC and WAIC. It starts elaborating on why \\(R^2\\) is useless. p-values will never be an indicator for predictive accuracy!!, what p-value does is only care about type 1 error. In general, do not pay too much attention to these. 8.2 Entropy and Accuracy We see that entropy is able to reflect how wrong we are, instead of just measuring hit rate as accuracy does. When we calculate the entropy of a model, we will try to minize the entropy, that is because the large the entropy, the more surprised we are of the actual outcomes. The larger the entropy the larger is q (our model) to p (the actual outcome), the distance from our model to the true model is also called the Divergence. The equation: \\[Divergence= \\sum_i p_i(log(p_i) - log(q_i))\\] Divergence example: Go to the eath / mars example in the book. In its essence, we see that earth is a HIGH entropy place, as there are a lot of land and a lot of water (hence there is lots of both). When you then go to mars, you will be surprised. But the surprise going from mars to earth will be greater, as there is low entropy on mars (as almost 100% of the planet is covered in land), hence you will be very surprised going to a place with higher entropy. The simpler model is better than complicated models, that is because simpler model has higher entropy, as they generelize more than complicated models. Where we see complicated models can be compared with mars, where it is very certain about different outcomes (typical characteristics of overfitting) To estimate divergence for a model, we will use the log-score, this will be a distribution, this can be foind with lppd() from the rethinking package (log-pointwise-predictive-density). \\[Deviance = logscore * -2.\\] We will look at following information criteria: AIC WAIC PSIS We see that deviance has the same properties as R2, thus it will not penalize for introducing more variables and also overfitting. Then we also see that CV can make out of sample performance estimations based on testing the model on unseen data. Notice that nonsens models can have good predictions, but their inference and causal relationships may be totally off, hence prediction and inference is two totally different ways of approaching a problem. 8.3 Golem Taming: Regularization As we have seen in ML classes, we can regularize the model to make it harder to capture variance in the training / calibration process. We regularize the priors, we see that we have a prior (striped) that can take on many values, while two versions of regularizations is more conservative, hence it needs more extreme data to overwhelm the priors. 8.4 Predicting predictive accuracy We see that: the loo function is a very accurate out of sample performance estimate AIC: Information criteria to approximate out of sample performance, we see that WAIC always (almost) outperforms the AIC is WAIC: we see that lppd is the loglik that we have in the AIC. One can use the compare() function to compare different modes, that will be done on the WAIC, and it will show the weight, hence if you have several models, it will say what weights different models should have in a prediction scenario. pWAIC is the effective number of parameters in the model. One must remember that the information criteria is an indication of the overfit, i.e., the model overconfidence. It should in fact never be used for model selection, as these criteria does not care about causality. 8.5 Model Comparison 8.6 Exercies 8.6.1 Medium exercises 7M3. When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure. Because the information is reflecting how surprised a model is when it sees some data. 7M4. What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure. We see that the effective number of parameters are decreasing, that is because you constrain the possibilities of the model, hence less flexible, i.e., less complex and therefore less effective number of parameters. If we use WAIC() we will see a penalty term, this is representitive of the effective numbers parameters. 7M5. Provide an informal explanation of why informative priors reduce overfitting. If we have flat priors (some without information) we will see that the model can fit to any scenario. If we impose information in the priors we can manipulate the model to not model for scenarios that are impossible or extremely unlikely. 7M6. Provide an informal explanation of why overly informative priors result in underfitting. This is basically because we constrain the model too much. Hence when the model is fitted it needs very extreme cases to adjust the priors. 8.6.2 Hard exercises 7H1. In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in data(Laffer).Do: Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue? library(rethinking) data(Laffer) d &lt;- Laffer #Add vector of squared values, for the polynomial variable d$tax_rate2 &lt;- d$tax_rate^2 ## Starndardizing ## #&#39; I could have standardized the tax rate #linear model ml &lt;- quap( alist( tax_revenue ~ dnorm( mu , sigma ) , mu &lt;- a + b1*tax_rate , a ~ dnorm( 10 , 2 ) , #mean b1 ~ dlnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dunif( 0 , 50 ) ) ,data = d ) #quadratic model mq &lt;- quap( alist( tax_revenue ~ dnorm( mu , sigma ) , mu &lt;- a + b1*tax_rate + b2*tax_rate2 , a ~ dnorm( 10 , 2 ) , #mean b1 ~ dlnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect b2 ~ dnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dunif( 0 , 50 ) ) ,data = d ) WAIC(ml) WAIC(mq) WAIC lppd penalty std_err 126.9495 -55.91741 7.55732 23.69986 WAIC lppd penalty std_err 126.71 -56.10766 7.247347 21.5699 We see that the WAIC is increasing in the more complex model, we also see the penalty is increasing, meaning that the effective number of parameters is increasing. PSIS(ml) PSIS lppd penalty std_err 134.145 -67.07248 11.14961 31.13811 PSIS(mq) PSIS lppd penalty std_err 132.0448 -66.02242 10.01239 27.03682 Another approach can be: compare(ml,mq,func = PSIS) x 126.1800 131.2456 x 23.87878 26.47440 x 0.000000 5.065613 x NA 3.347204 x 7.314077 9.531420 x 0.9264099 0.0735901 We can look at the plot to see if they can be said to be different. We see that dPSIS is the difference to the best model, and the dSE is the standard error, if the dSE is smaller than dPSIS, then we can certainly say that the given model is worse than the linear model. The weights are the AIC weights, so if we were to predict, this is the weight that each model should have in a prediction setting. plot(compare(ml,mq,func = PSIS)) We see the there is no statistical evidence that the more complicated model is any better than the linear model. (We look at the small gray range in the middle) PSISk(ml) ## [1] 0.37 -0.03 0.27 -0.17 -0.18 -0.12 0.23 0.15 -0.18 0.01 0.59 2.63 ## [13] 0.62 0.28 -0.22 0.02 -0.21 -0.17 0.55 0.07 0.03 -0.03 0.07 0.11 ## [25] 0.06 0.13 0.16 0.18 0.02 We see in that one value spikes (theres is one value for each observation). What can we conclude? precis(ml) precis(mq) x 2.1789645 0.0501862 1.7082761 x 0.6494646 0.0218943 0.2277029 x 1.1409947 0.0151948 1.3443629 x 3.2169343 0.0851775 2.0721893 x 2.3518367 0.1210169 -0.0026556 1.7132067 x 0.9418560 0.0696866 0.0018208 0.2414320 x 0.8465689 0.0096443 -0.0055655 1.3273517 x 3.8571045 0.2323895 0.0002544 2.0990616 We see that in the linear model the intercept says that the tax return will be 2.18 if tax rate is 0. We see that the there is a positive effect from tax rate to tax return. And there is typically always a positive effect. We see in the quadratic model that the squared values are really just 0, while a greater effect is put on the first order. "],["chapter-8-model-comparison.html", "9 Chapter 8 - Model comparison 9.1 Building an interaction 9.2 Symmetry of interactions 9.3 Continuous interactions 9.4 Exercises", " 9 Chapter 8 - Model comparison Introduction, using the case of the planes coming back with holes in the wings, does that mean that one should make the wings stronger? A model will suggest that, because it only sees scenarios of where the holes are in planes that came back, hence it is conditional on planes returning, we rarely see holes in the center part. Since we only see planes coming back, we dont get to see the other condition. The following sections elaborate on how we can model interactions, meaning how one outcome may result another. 9.1 Building an interaction This section elaborates on what an interaction can be used at what for. library(rethinking) data(rugged) d &lt;- rugged # make log version of outcome d$log_gdp &lt;- log( d$rgdppc_2000 ) # extract countries with GDP data dd &lt;- d[ complete.cases(d$rgdppc_2000) , ] # rescale variables dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) #8.2 m8.1 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a + b*( rugged_std - 0.215 ) , a ~ dnorm( 1 , 1 ) , b ~ dnorm( 0 , 1 ) , sigma ~ dexp( 1 ) ) , data=dd ) #8.3 set.seed(7) prior &lt;- extract.prior( m8.1 ) # set up the plot dimensions plot( NULL , xlim=c(0,1) , ylim=c(0.5,1.5) , xlab=&quot;ruggedness&quot; , ylab=&quot;log GDP&quot; ) abline( h=min(dd$log_gdp_std) , lty=2 ) abline( h=max(dd$log_gdp_std) , lty=2 ) # draw 50 lines from the prior rugged_seq &lt;- seq( from=-0.1 , to=1.1 , length.out=30 ) mu &lt;- link( m8.1 , post=prior , data=data.frame(rugged_std=rugged_seq) ) for ( i in 1:50 ) lines( rugged_seq , mu[i,] , col=col.alpha(&quot;black&quot;,0.3) ) sum( abs(prior$b) &gt; 0.6 ) / length(prior$b) ## [1] 0.545 m8.1 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a + b*( rugged_std - 0.215 ) , a ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp(1) ) , data=dd ) 9.1.1 Adding an indicator variable isn’t enough. One may argue that indicator variables may be sufficient to reflect certain situations, e.g., if something is on Africa or not, hence an indicator (i.e., dummy variable) may be included in the model. Although that will just reflect the mean value of that given outcome as it is either in or out. #8.7 # make variable to index Africa (1) or not (2) dd$cid &lt;- ifelse( dd$cont_africa==1 , 1 , 2 ) #8.8 m8.2 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=dd ) #8.9 compare( m8.1 , m8.2 ) x -252.2694 -188.7489 x 15.30363 13.29716 x 0.00000 63.52044 x NA 15.14767 x 4.258180 2.693351 x 1 0 #8.10 precis( m8.2 , depth=2 ) x 0.8804170 1.0491586 -0.0465124 0.1123923 x 0.0159377 0.0101860 0.0456887 0.0060917 x 0.8549455 1.0328794 -0.1195318 0.1026565 x 0.9058885 1.0654378 0.0265069 0.1221281 #8.11 post &lt;- extract.samples(m8.2) diff_a1_a2 &lt;- post$a[,1] - post$a[,2] PI( diff_a1_a2 ) ## 5% 94% ## -0.1990056 -0.1378378 #8.12 rugged.seq &lt;- seq( from=-0.1 , to=1.1 , length.out=30 ) # compute mu over samples, fixing cid=2 mu.NotAfrica &lt;- link( m8.2 , data=data.frame( cid=2 , rugged_std=rugged.seq ) ) # compute mu over samples, fixing cid=1 mu.Africa &lt;- link( m8.2 , data=data.frame( cid=1 , rugged_std=rugged.seq ) ) # summarize to means and intervals mu.NotAfrica_mu &lt;- apply( mu.NotAfrica , 2 , mean ) mu.NotAfrica_ci &lt;- apply( mu.NotAfrica , 2 , PI , prob=0.97 ) mu.Africa_mu &lt;- apply( mu.Africa , 2 , mean ) mu.Africa_ci &lt;- apply( mu.Africa , 2 , PI , prob=0.97 ) plot(dd$log_gdp,dd$rugged,ylim = c(0.7,1.4)) grid() lines(mu.NotAfrica_mu,col = &quot;black&quot;) lines(mu.Africa_mu,col = &quot;blue&quot;) Thus we see that it is far better including an index instead of a dummy variable!!! 9.2 Symmetry of interactions About the index approach. 9.3 Continuous interactions 9.3.1 A Winter Flower This is an example with a flower that depends on sun and water to make photosynthesis, although having only one of the two is not useful at all, hence we need to deal with this using an interaction. This is the data: library(rethinking) data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 9.3.2 The models We are going to center W (water) and S (shade). d$blooms_std &lt;- d$blooms / max(d$blooms) d$water_cent &lt;- d$water - mean(d$water) d$shade_cent &lt;- d$shade - mean(d$shade) We have the non centered model: We set the priors to (we correct the sd in the following): Alpha = 0.5 means that when shade and sun is at its mean, the model expects blooms to be halfway to the observed maximum. We see that the slopes (betas) for W and S is 0, meaning that we have no idea if there is going to be relatively more shade or water, hence it will just be centered at 0. But what does the standard deviation of 1 imply? We see that the blooming should not go below 0 and above 1. Lets us inspect the alpha parameter and what the prior expects to see. a &lt;- rnorm(n = 1e4 ,mean = 0.5 ,sd = 1) #Intercept alpha less than 0 and higher than 1 sum( a &lt; 0 | a &gt; 1 ) / length( a ) ## [1] 0.6189 It implies that the priors assign most of the probability outside of the range of 0 to 1. Hence we try to constrain the priors a bit: a &lt;- rnorm(n = 1e4,mean = 0.5,sd = 0.25) sum( a &lt; 0 | a &gt; 1 ) / length( a ) ## [1] 0.0443 Now we see that only 5% is less than 0 and higher than 1. That is much better. Notice that we want to allow the model to reach the edges and probably go a bit above / below so we can fit data within this region. Regarding the beta values we see that the bw &lt;- rnorm(n = 1e4,mean = 0,sd = 1) dens(bw,main = &quot;bw prior&quot;) We also see that bw &lt;- rnorm(n = 1e4,mean = 0,sd = 0.25) dens(bw,main = &quot;bw prior&quot;) Now we see that the PI for 95% is between -0.5 and 0.5. PI(samples = bw,0.95) ## 3% 98% ## -0.4858059 0.4828205 In general the goal of the priors in this example is to set priors with weak information but also constrain the model from overfitting. The is how the model looks when finalized: #Non interaction model! m8.4 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) ,data=d ) precis(m8.4) x 0.3587658 0.2050338 -0.1125315 0.1581543 x 0.0302189 0.0368895 0.0368757 0.0214437 x 0.3104702 0.1460771 -0.1714660 0.1238831 x 0.4070615 0.2639904 -0.0535971 0.1924255 Building the interaction model The model now look like this: #With interaction m8.5 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , bws ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) ,data = d) 9.3.3 Plotting posterior predictions To do this, we must: Draw samples from #Plotting the non interaction model par(mfrow=c(2,3)) # 3 plots in 1 row models &lt;- list(m8.4,m8.5) for(m in models){ for(s in -1:1) { #Create index of observations given the shade level idx &lt;- which(d$shade_cent==s) #We have three levels -1, 0 and 1. #Plot each water levels given the shade level plot(d$water_cent[idx] ,d$blooms_std[idx] ,xlim=c(-1,1),ylim=c(0,1) ,xlab=&quot;water&quot;,ylab=&quot;blooms&quot; ,pch=16,col=rangi2 ) #Draw samples mu &lt;- link(m #Non interaction model ,data = data.frame(shade_cent=s,water_cent=-1:1)) #Plotting 20 posterior lines for(i in 1:20) lines(x = -1:1,y = mu[i,] ,col=col.alpha(&quot;black&quot;,0.3)) } } Figure 9.1: Top row = no interaction model, bottum row = innteraction model. 9.3.4 Plotting prior predictions Now we are going to extract priors to plot these. set.seed(7) prior &lt;- extract.prior(m8.5) #Plotting the non interaction model par(mfrow=c(2,3)) # 3 plots in 1 row models &lt;- list(m8.4,m8.5) for(m in models){ for(s in -1:1) { #Create index of observations given the shade level idx &lt;- which(d$shade_cent==s) #We have three levels -1, 0 and 1. #Plot each water levels given the shade level plot(d$water_cent[idx] ,d$blooms_std[idx] ,xlim=c(-1,1),ylim=c(-0.5,1.5) ,xlab=&quot;water&quot;,ylab=&quot;blooms&quot; ,pch=16,col=rangi2 ) abline(h = c(0,1),lty = 2,col = &quot;grey&quot;) #Draw samples mu &lt;- link(m #Non interaction model ,data = data.frame(shade_cent=s,water_cent=-1:1) ,post = prior) #Plotting 20 prior lines for(i in 1:20) lines(x = -1:1,y = mu[i,] ,col=col.alpha(&quot;black&quot;,0.3)) } } Figure 9.2: Top row = no interaction model, bottum row = innteraction model. We see that the priors both for the interaction and no interaction model is typically within the limits, so that is good. We can say that the priors are: Harmless Weakly realistic The priors include non or very low bias to positive or negative effects as most things can happen, although the priors tend to be in a reasonable range, hence we are doing better then a flat prior, but we do not include much information either. 9.4 Exercises 9.4.1 8M4 Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior assumptions mean for the interaction prior, if anything? Notes: Need to be changed, so the prior distribution is higher than 0 and less than 0, thus the distribution of the two distributions must be on each side of 0. We can do this by manipulating the mean or the standard deviation of the dsitruibutions. There are two difficulties with this: You can end up having an unrealistic mean. To avoid that, you can manipulate the standard deviation, although that may make too much certainty Also a normal distribution will never be strictly within some region, as it never touches 0. Thus we can use an exponential distribution, although that only has one tail. Thus we can make a log normal distribution instead. The model is from code chunk 8.24 - m8.5. #Loading data library(rethinking) data(&quot;tulips&quot;) d &lt;- tulips #&#39; water and shade variables are three levels, ordered after amount of water/shade. #&#39; Cluster of plants in the same section of the greenhouse. #&#39; Blooms = the outcome variable which we have to predict. #Transforming the data d$blooms_std &lt;- d$blooms / max(d$blooms) d$water_cent &lt;- d$water - mean(d$water) d$shade_cent &lt;- d$shade - mean(d$shade) 9.4.1.1 Interaction model Model specification #The model m &lt;- quap( alist( blooms_std ~ dnorm(mu,sigma), mu &lt;- a + bw * water_cent - bs * shade_cent + bws * water_cent * shade_cent, #Notice there is minus bs a ~ dnorm(0.5,0.25), bw ~ dlnorm(0,0.25), bs ~ dlnorm(0,0.25), bws ~ dlnorm(0,0.25), sigma ~ dexp(1) ) ,data = d ) precis(m) x 0.3822041 0.5410913 0.5081694 0.4926979 0.6048014 x 0.1058485 0.0990471 0.0967641 0.1027979 0.1203730 x 0.2130377 0.3827949 0.3535217 0.3284070 0.4124220 x 0.5513704 0.6993877 0.6628172 0.6569888 0.7971807 We see that the more shade, the less blooms and the more water the more blooms. Although the combination of water and shade will lead to less blooms, I guess because the plant will be drowning in water. Prior predictive simulation par(mfrow = c(1,1)) simulations &lt;- 20 prior &lt;- extract.prior(m) for (s in -1:1) { idx &lt;- which(d$shade_cent == s) plot(x = d$water_cent[idx] ,y = d$blooms[idx] ,xlim = c(-1,1), ylim = c(-0.5,1.5) ,xlab = &quot;water&quot; ,ylab = &quot;blooms&quot; ,pch = 16 ,col = rangi2 ,main = paste(&quot;Priors predictive simulation, S = &quot;,s) ) abline(h = c(0,1),lty = 2) #Call link function to make predictions mu &lt;- link(fit = m ,data = data.frame(shade_cent = s, water_cent = -1:1) ,post = prior #posterior is just the prior, hence we introduce no data ) for (i in 1:simulations) { lines(-1:1 ,mu[i,] ,col = col.alpha(&quot;black&quot;,0.3) ) } } We see that we are going to regularize the priors even more, to make better priors. #The model m &lt;- quap( alist( blooms_std ~ dnorm(mu,sigma), mu &lt;- a + bw * water_cent - bs * shade_cent + bws * water_cent * shade_cent, #Notice there is minus bs a ~ dnorm(0.5,0.25), bw ~ dlnorm(-2,0.25), #Set mean to -2 bs ~ dlnorm(-2,0.25), #Set mean to -2 bws ~ dlnorm(-2,0.25), #Set mean to -2 sigma ~ dexp(1) ) ,data = d ) simulations &lt;- 20 prior &lt;- extract.prior(m) for (s in -1:1) { idx &lt;- which(d$shade_cent == s) plot(x = d$water_cent[idx] ,y = d$blooms[idx] ,xlim = c(-1,1), ylim = c(-0.5,1.5) ,xlab = &quot;water&quot; ,ylab = &quot;blooms&quot; ,pch = 16 ,col = rangi2 ,main = paste(&quot;Priors predictive simulation, S = &quot;,s) ) abline(h = c(0,1),lty = 2) #Call link function to make predictions mu &lt;- link(fit = m ,data = data.frame(shade_cent = s, water_cent = -1:1) ,post = prior #posterior is just the prior, hence we introduce no data ) for (i in 1:simulations) { lines(-1:1 ,mu[i,] ,col = col.alpha(&quot;black&quot;,0.3) ) } } 9.4.1.2 Conclusion We see that the regularized priors are more realistic. Never plot the priors with the actual data points, that is just done for practical reasons. 9.4.2 8H3 "],["chapter-9-markov-chain-monte-carlo.html", "10 Chapter 9 - Markov Chain Monte Carlo 10.1 Good King Markov and his island kingdom 10.2 Metropolis algorithms 10.3 Hamiltonian Monte Carlo 10.4 Easy HMC: ulam 10.5 Care and feeding of your Markov chain 10.6 Lecture notes", " 10 Chapter 9 - Markov Chain Monte Carlo 10.1 Good King Markov and his island kingdom 10.2 Metropolis algorithms 10.3 Hamiltonian Monte Carlo This is a more effect approach for sampling. 10.4 Easy HMC: ulam 10.4.1 Fitting using quap The following is an example of how the ulam function works. We are going to use the example from chapter 8 about predicting log GDP based on ruggedness of a nation. library(rethinking) data(rugged) d &lt;- rugged d$log_gdp &lt;- log(d$rgdppc_2000) dd &lt;- d[ complete.cases(d$rgdppc_2000) , ] dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) dd$cid &lt;- ifelse( dd$cont_africa==1 , 1 , 2 ) The following is an example of how to specify the model and fit it using the quap method. m8.3 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b[cid]*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b[cid] ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=dd ) precis( m8.3 , depth=2 ) x 0.8865639 1.0505696 0.1325055 -0.1425763 0.1094900 x 0.0156751 0.0099362 0.0742018 0.0547474 0.0059347 x 0.8615121 1.0346896 0.0139167 -0.2300732 0.1000051 x 0.9116158 1.0664496 0.2510943 -0.0550794 0.1189748 We see that we have an index model instead of indicator variables, thus one alpha and beta for each category. Where 1 = Africa and 2 = all other countries. Thus we see that alpha parameter (intercept) is positive for both countries, we also set the mean to be positive. And we see that the coefficient for the ruggedness is positive for Africa and negative for the rest of the world. We will also fit this model using Hamiltonian Monte Carlo 10.4.2 Fitting using HMC This section will be separated in two parts: 10.4.2.1 Preparing data To do this, one must transform the variables just as we did with the quap method. Although for this method, we will arrange the data in a list. The benfit is that the different does not have to be of the same length, when arranged in lists. dat_slim &lt;- list( log_gdp_std = dd$log_gdp_std, rugged_std = dd$rugged_std, cid = as.integer(dd$cid) ) str(dat_slim) ## List of 3 ## $ log_gdp_std: num [1:170] 0.88 0.965 1.166 1.104 0.915 ... ## $ rugged_std : num [1:170] 0.138 0.553 0.124 0.125 0.433 ... ## $ cid : int [1:170] 1 2 2 2 2 2 2 2 2 1 ... 10.4.2.2 Sampling from the posterior Now we are going to fit the model using ulam, which relies on the stan engine. m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu,sigma), mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1,0.1), b[cid] ~ dnorm(0,0.3), sigma ~ dexp(1) ) ,data = dat_slim ,chains = 1 #no of independent chains to sample from ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;f3314e777e4c586121dcc9de98266129&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.6e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.36 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.057177 seconds (Warm-up) ## Chain 1: 0.03605 seconds (Sampling) ## Chain 1: 0.093227 seconds (Total) ## Chain 1: We see that there is a warmup, that is something about calibrating the model to find the distribution that it should draw from, and then it starts drawing samples. precis(m9.1,depth = 2) x 0.8858647 1.0509446 0.1306847 -0.1426126 0.1115789 x 0.0146656 0.0098105 0.0742874 0.0595235 0.0063499 x 0.8628451 1.0355854 0.0112044 -0.2403829 0.1016925 x 0.9092368 1.0668772 0.2464492 -0.0512215 0.1218014 x 498.1959 662.9343 426.8011 523.4390 525.4982 x 0.9981420 0.9993469 0.9986134 1.0007405 0.9979981 Now we see that the outcome is the same, although we have two new outputs: n_eff: crude estimate of the number of independent samples you managed to get Rhat4: An indicator of the convergence of the Markov Chains to the target distribution. ‘4’ is just the version of the Rhat (\\(\\hat{R}\\)). 10.4.2.3 Sampling again, in parallel You can sample different chains in the same time, one for each core in your computer. m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu,sigma), mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1,0.1), b[cid] ~ dnorm(0,0.3), sigma ~ dexp(1) ) ,data = dat_slim ,chains = 4 #no of independent chains to sample from ,cores = 4 #We use four cores ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 10.4.2.4 Visualization you can plot the sampling, to see the distributions that you find. pairs(m9.1) Here we see the distribution of the parameters. 10.4.2.5 Checking the chain We have two visual tools: traceplots: here we look for: Stationarity: we want the line to show no trend and stay within the same boundaries over time good mixing: that the chain rapidly explores the full region and not being stuck in regions. convergence: that multiple independent chains stick around the same region. When you plot multiple chains it starts getting difficult to see how each chain compares to the others. That is what trank plots is dealing with. trankplots (trace rank plots): We want to see the histograms of the independent chains overlapping each other. 10.5 Care and feeding of your Markov chain This section has some illustrations of bad chains. Basically that is just when we see that the probability region is not explored efficiently. 10.5.1 How many samples do you need? Terminalogy (with defaults): iter = total number of samples warmup = iter/2 Then how many samples do you need to estimate the posterior? It is all about the effective number of samples. If the chain is autocorrelated, then the n_eff will be lower than the number of iterations, although if it is anti-correlated, then you can have more effective samples than the number of iterations, that is merely due to the fact that some methods are able to outperform randomness of drawing samples. Now you must ask yourself, do you want to estimate something around the mean or toward the tails of a distribution. If towards the mean, then you dont need many samples (rule of thumb a 100 or a couple 100), but if you want to explore the tails, then you need a lot of samples, as these are more difficult to explore. Thus there is no good rule of thumb in general terms. Also the shape of the posterior distribution that you are trying to map is affecting, if it is simple, then it is easier to map than if it is complex. Notice that stan will let you know if it is uncertain around the tails, thus you need more samples. 10.5.2 How many chains do you need? Always start with one chain when building the model, as the stan will only return error messages and not just errors, when running 1 chain. Then run multiple independent chains to cross validate the chains and see how they converge. Typically 3 or 4 chains are sufficient. 10.5.3 Taming a wild chain Basically flat priors or almost flat priors will lead to crazy sampling, as it can explore any region, also totally unrealistic regions. Hence we want to regularize this. If you have no idea of the priors then go for weakly informative priors, instead of stupid priors, as the likelihood will always outperform these priors. 10.6 Lecture notes Why Markov Chains? We see that markov chains is not only used in bayesian data analysis, but also for frequentist statistics. Recap: Why cant we use quadratic approximation: it is effective for simple models, we see that with flat priors it is the same as classical statistics. Although, when it gets more complicated, this approach will not be feasibile. Case with Markov Chains: Will always find the posterior distribution after time. Although it only works on the long run. But the MCMC method may need to run in a long long long time, hence that is a drawback of MCMC. This is also called the metropolis algorithm. That is from the researcher Nicholas Metropolis, this originates in the research of making fusion bombs. What is a chain? It is a simulation of chained events. WHere Markov Chain is a sequential simulation, where you can go to different outcomes, based on the current state and not the passed states. Hence it has no memory, this is often an advantage. Monte Carlo is coming from the city, where people gamble, hence there is not too much about this. MCMC is an engine of making integration. Hence it takes something very hard make simplify it. Different strategies of MCMC: Metropolis Metropolis Hastings Gibbs sampling: an efficient version of bullet 2, although in a Silow dimensional setting Hamiltonian Monte Carlo (HMC): This does not guess and check. This is more efficient even with very complicated models. Metropolis and Gibbs: is a guess and check method. This is now a thing of the past This is an example, where MH sampling is not working well: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&amp;target=banana The site also have different examples. Key tuning parameter is the step size, we see that the larger the step size, the more different places it will suggest. We see that the HMC runs a physical simulation, where it is trying to map the probability distribution. How: It is set randomly, and the follows the slope of the probability mass. Then it is being flicked, and then it follows the slope again. HMC has the following tuning parameters: Momentum, is random Step size, must be tuned. Adaptive speed, when it goes downwards, then the velocity is decreasing and when it goes upwards, then the velocity is decreasing. See a video on how this works here: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=HamiltonianMC&amp;target=banana we see that the direction may be set, although the ‘gravitation’ will bend the direction. It is calculating the gradient in each point, to estimate the slope, thus it is mathematically more complicated, although it means that all proposals are acceptet. It will run for some fixed time! This is also a tuning parameter. This is called a leapfrog step. We define how many jumps the algorithm can take before the sample proposal is drawn. Hence HMC, is flicking the particle, that will roll according to the slope, where the momentum is random each time. We see that each step takes more time to compute in HMC compared to the random samplers, such as Gibbs. Although it requires way less observations. HMC is basically a gradient descent method to end in a region with a probability mass, where it should always tend towards the probability mass. HMC problems: The U-turn phenomenon: We see that we end up in the same place as we started in the right scenario. Try to follow this link:https://chi-feng.github.io/mcmc-demo/app.html?algorithm=NaiveNUTS&amp;target=banana, select the standard distribution and increase the leopfrog steps(about 65) . There is a NUTS (no u turn) method. This method is going back and forwards and seeing when it starts turning. Then it stops and takes a sample. This means that we dont need to define the no. of leapfrog steps, hence it is adaptive to where it starts. The problem of no u turn sampler, is if we have a multimodal distribution (multiple hills), it has a tendency to get stuck in one of the hills. As it needs enough speed to break lose the gravity around probability mass. For diagnosis, see the slides, there are some examples and notes. "],["references.html", "References", " References McElreath, Richard. 2020. Statistical Rethinking. Chapman; Hall/CRC. https://doi.org/10.1201/9780429029608. "]]
