[["index.html", "Bayesian Data Analysis Notes Introduction", " Bayesian Data Analysis Notes Jonas Ringive Korsholm Introduction "],["chapter-1-golems-of-prague.html", "1 Chapter 1 - Golems of Prague", " 1 Chapter 1 - Golems of Prague This is a metaphor of having golems. These are machines that are given commands and then they will act accordingly, and do nothing else. The problem with this is that it has no background or knowledge, hence it is really up to us to make sure that it is not malfunctioning. Hence this is a metaphor for statistical models. Regarding models and hypotheses We see that a model is not the same as a hypothesis. With this there is critique on attempting to falsify the null model. That is because then you assume that something is exactly what you state in the null model. That is not true. Hence one should instead looking into falsifying the alternative model. Then what are we going to do? We are going to count the ways that an outcome can occur, to find the most probable events. This will give us bayesian distributions. These we are to explore to learn about the data and UPDATE our own beliefs. The disadvantage of bayesian approaches is the it can be cumbersome to perform. The reason that this is a small field, is that previously it has not been computationally possible. Although MCMC allows this! This is done by drawing an approximation of the original posterior distribution. Bayesian Data Analysis is about counting all the ways that the data can happen, according to assumptions. Assumptions with more ways to cause data are more plausible. CH1, p. 11 We see that the assumptions are what we put into the Golems. Hence what the Golem belief before seeing the data. Assumptions = prior beliefs. What the model assumes before What is probability? The frequentist view: This is basically traditional statistics, where we test a hypothesis based on a significance level. This relies on the data you have and the process you take. This is an objective perspective, you never carry on knowledge, it is all about the long relative frequencies. The bayesian probability: here we assume that there is nothing random. But what we call randomness is our ignorance. Hence e.g., with a coin toss, if we knew all about physics, we would always be able to predict the outcome. This implies that the bayesian view is subjective, that is also a great critique of this perception. "],["chapter-2-small-worlds-and-large-worlds.html", "2 Chapter 2 - Small worlds and Large Worlds 2.1 Models and Estimation 2.2 Exercises", " 2 Chapter 2 - Small worlds and Large Worlds The small world is the world of the golems assumptions. Bayesian golems are optimal, in the small world. The large world is the real world. No guarantee of optimality for any kind of golem. Terminoligy: Under drawing of replacement we are able to write out the possible contents Garden of Forking Data is the possible outcomes that we will see. Conjecture = the assumption of what the different observations looks like (or is constructed). e.g, we have a bag of 4 marbles, we may assume that there is 1 blue and 3 whites. In the following we are able to see what possible outcomes we expect to have. This can then be extended by a second draw: Now we see that there are 16 different paths, given the assumptions that we made (1 blue and 3 white). Now this expands exponentially, hence by including a third draw we would have 64 different outcomes. Now what? One should draw the whole garden and find the paths that can lead to the way of producing the desired output. As the number of paths increase exponentially, we quickly start to work with large counts. This is the reason, that Bayes Theorem got into this approach, as we are able to compress the counts as relative counts. We can do this with R. Lets say that we have 3, 8 and 9 ways of producing three different compositions of a bag of marbles, then we say: ways = c(3,8,9) #notice that the counts are found given the conjecture ways/sum(ways) ## [1] 0.15 0.40 0.45 Hence we see that the relative plausibility for 3 blue and 1 white is 45%, given the conjecture (the assumption of the composition of the bag) Building a model We want to: Design the model (data story) Condition on the data (update) Evaluate the model (critique) See also example with tossing globes in the following sections. priors let us say that we have a prior with absolutely no information (like traditional statistics), we do not know if it is 100% or 0% water, hence the prior looks the following: Now we introduce more information. Notice that we consecutively update priors. Prior distribution = striped and posterior distribution = solid line. We see that after the first observation (top left), there is 0 probability of 0% water, as we now know that there is water. Now we can look at the next observation, we see that it is land, now the posterior distribution is essentially centered at 0. Now lets look at a third observation, we see that the posterior is skewed to the right of the prior. We see that the more information we see, the taller (the more certain) will the distributions be. Key takeaway: the current posterior distribution will be the prior distribution for the following observation. How can one manipulate the process? We see that you can change how much the prior is updated, hence if you are certain about a prior distribution, then you can make it more difficult for the model to update this. Also we see from the example above, that the more information that is revealed, the less does the prior matter. 2.1 Models and Estimation Introduction: We see that the Bayesian Data Analysis takes the approach of having a prior probability (the probability of an event happening while ignoring the data we have at hand). Then we compute posterior probabilities as we introduce data. The posterior probabilities can be seen as a relative count of how many ways a given outcome can be replicated out of the total. The Bayesian Data Analysis can essentially be fitted with three different models: 1. Grid approximation: Good with few parameters 2. Quadratic approximation: Good with a moderate amount of parameters. Also, this is an approximate and is rarely applied. 3. Markov Chain Monte Carlo: Outperforms other models in in a high parameter scenario 4. (Analytical approach): this is often impossible. Examples will be shown during lectures, but will not be used. Remember that the Bayesian Estimate that we end up with, will always be a distribution and not a point estimate!! This is a section about models and estimation, based on chapter 1 - 3 from the book. Some terminology: Prior Distribution: This is the distribution of a prior event. E.g., lets say that we toss a coin. There are two sides hence we expect to see a normal bell curve centered in 50%. Posterior Distribution: This is basically just the prior distribution after we introduce observations. Lets say that we end up getting many consecutive heads, it implies that the probability of an outcome is actually skewed, e.g., the coin may be more heavy on one side. Hence we will see that the posterior distribution will not be centered around 50% but move to one of the sides. Therefore, you can model with (test different) prior probabilities. But the posterior probability is found after introducing data (observations/samples). Likelihood: This is just the relative number of ways that a given scenario can be produced. E.g., if you have discrete data, drawing marbles then the likelihood of some sequence is just the relative count of how you can construct such sequence. Prior probability = prior plausibility Updated plausibility = posterior probability The posterior is calculated as the following This is to be interpreted as: Average likelihood = evidens. This is summing over p. hence it ensures that the posterior distribution will sum to 1. Likelihood = ways to get the data Prior = prior ways to get the data Hence one ends up with probability of p given the new data. Assumptions when making the model Data story: Motivate the model by narrating how the data might arise. Update: Educate your model by feeding it the data. Basically the distribution for a given outcome is explored observation by observation. The more data we have seen the better should the distributions be. Evaluate: All statistical models require supervision, leading possibly to model revision. #Code 2.2 - finding likelihood dbinom(x = 6 #No. of &#39;successes&#39; water in this case ,size=9 #No. of tosses ,prob=0.5 #Probability of a given outcome (succes) ) # = 0.1640625 ## [1] 0.1640625 d for density or distribution. bi for binomial. We see that the probability of getting 6 water (and 3 land) is 16%. Given that the probability of water is 50%. The 16% is equivilent to the relative number of ways that 6 water and 3 land can be found. Notes in prior priobabilitieis. We see that oftentimes you only have one prior, and it can for instance be based on already seen data. Although a prior does not have to be based on that, one can test of different priors and see what that leads to. How to select a prior: In general we can always do better than just everything is equally likely, but notice there is no true prior. This means that a good prior is subjective, therefore, one can test with different priors and see how sensitive the model is to different priors. 2.1.1 Engines / Motors to estimate the models We are going to apply three different engines to estimate the model. 2.1.1.1 Grid approximation Here we use a grid of values to compute the likelihood of Water. This is basically just defining a range, and calculating the probabilities for the given value, then we end up being able to plot this. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(6 ,size=9 ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ) mtext( &quot;20 points&quot; ) abline(v = 0.67,lty = 2,col = &quot;darkgreen&quot;) mtext(text = &quot;Quap mean approximation, see next section&quot;,side = 1,at = 0.6,col = &quot;darkgreen&quot;,cex = 0.7) So we see that the probability of picking 6 times water peaks around 60% to 70%. The Grid approximation scales very badly, hence when you have a model with many variables it starts to get cumbersome to estimate. That is the reason that we go to quadratic approximation. 2.1.1.2 Quadratic approximation The quadratic approximation is basically utilizing Guassian (normal) distribution library(rethinking) globe.qa &lt;- quap( alist( W ~ dbinom( W+L ,p) , # binomial likelihood p ~ dunif(0,1) # uniform prior ) , data=list(W=6,L=3) ) # display summary of quadratic approximation precis( globe.qa ) x 0.6666665 x 0.1571338 x 0.4155362 x 0.9177967 We see that the mean is 0.67, hence the highest prior, this level is also inserted in the illustration above, to show that we end up in approximately the same place. Then the standard deviation (sd) is the spread en then the confidence intervals are shown. 2.1.1.3 Markov Chain Monte Carlo This sections does not yet go in detail with MCMC. Although the key takeaway is that quadratic approximation is also cumbersome and to some extent impossible when you have a lot of parameters. Therefore MCMC can be used instead. The following is a toy example with the same data: #R Code 2.8 n_samples &lt;- 1000 p &lt;- rep( NA , n_samples ) #Samples from the posterior distribution p[1] &lt;- 0.5 #Posterior W &lt;- 6 #Successes (Water) L &lt;- 3 #Non successes (Land) for ( i in 2:n_samples) { p_new &lt;- rnorm(1, p[i-1], 0.1) if(p_new &lt; 0) p_new &lt;- abs(p_new) if(p_new &gt; 1) p_new &lt;- 2 - p_new q0 &lt;- dbinom(W , W+L , p[i-1] ) q1 &lt;- dbinom(W , W+L , p_new ) p[i] &lt;- ifelse( runif(1) &lt; q1/q0 , p_new , p[i-1] ) } dens(p , xlim=c(0,1)) curve(dbeta( x , W+1 , L+1 ) , lty=2 , add=TRUE) 2.2 Exercises 2.2.1 2M1 Recall the globe tossing model from the chapter. Compute and plot the grid approximate posterior distribution for each of the following sets of observations. In each case, assume a uniform prior for p. W, W, W W, W, W, L L, W, W, L, W, W, W They can be calculated using the same piece of code. Although we must change the number of successes and the number of tosses. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 3 ,size=3 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) We see that if we only draw water then we get more and more certain that there is only water. If we are to make a new toss. Then we should what we see above will be our new prior. Hence we start with a uniform prior (the stupid prior) and end up with a prior that contain information. length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 3 #No. of successes ,size=4 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,sub = paste(length,&quot; points&quot;) ) length = 20 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 5 #No. of successes ,size = 7 #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;b&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,sub = paste(length,&quot; points&quot;) ) 2.2.2 2M2 Now assume a prior for p that is equal to zero when p &lt; 0.5 and is a positive constant when p ≥ 0.5. Again compute and plot the grid approximate posterior distribution for each of the sets of observations in the problem just above. With this we are going to put more information in the prior. More than just having a prior without any information. We see with this binomial (i guess you can say) technique, there will be a jump whenever the probability of water exceeds 50%. This come from the prior we set, where we expect that at least 50% of the globe is water and the rest is land, hence we think that there is a chance of having more water than land. Naturally one could set the prior to anything and see how this affect the results. length = 50 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- c(rep(0,length/2),rep(1,length/2)) # compute likelihood at each value in grid x = 3 size = 3 likelihood &lt;- dbinom(x = x ,size=size #No. of tosses ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,2)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;,sub = &quot;All use the same prior&quot;) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) #And for the other three models x = 3 size = 4 likelihood &lt;- dbinom(x = x,size=size,prob=p_grid) unstd.posterior &lt;- likelihood * prior posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) x = 5 size = 7 likelihood &lt;- dbinom(x = x,size=size,prob=p_grid) unstd.posterior &lt;- likelihood * prior posterior &lt;- unstd.posterior / sum(unstd.posterior) plot(p_grid ,posterior,type=&quot;l&quot;,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot;,sub = paste(length,&quot;points, success =&quot;,x,&quot;and size =&quot;,size)) "],["chapter-3-sampling-the-imaginary.html", "3 Chapter 3 - Sampling the Imaginary 3.1 Sampling from a grid-approximate posterior 3.2 Sampling to summarize 3.3 Sampling to simulate prediction 3.4 Exercises", " 3 Chapter 3 - Sampling the Imaginary They take the following example: We see that this is the equation for a person being vampire given that the test i positive. This is calculated by using Bayes Theorem. This can be written in code in the following: #3.1 Pr_Positive_Vampire &lt;- 0.95 #Condiational prob for positive given vampire Pr_Positive_Mortal &lt;- 0.01 #Essentially the false positive rate Pr_Vampire &lt;- 0.001 #Prior for being vampire Pr_Positive &lt;- Pr_Positive_Vampire * Pr_Vampire + Pr_Positive_Mortal * (1 - Pr_Vampire) (Pr_Vampire_Positive &lt;- Pr_Positive_Vampire*Pr_Vampire / Pr_Positive) ## [1] 0.08683729 We see that given the test being positive, there is an 8.6% chance of being vampire compared to the default 0.001. So we see that even though the test has 95% percent correctness there is in fact still only less than 10% chance that you are a vampire given the positive test. We see that the actual true rate is dependent on how many in the population that are actual vampires. A more intuitive way of writing out this can be shown as: Leading to: The Aim of the following section is to build an intuition around the approximation techniques. We see that in the example it is very simple hence one would not necessarily need the approximation techniques. Although it is suggested to start using the fitting techniques early as one will use them as soon as the problem gets just a bit more complex. 3.1 Sampling from a grid-approximate posterior Lets take an example. We are going to take 10.000 samples p_grid &lt;- seq(from=0,to=1,length.out=1000 ) prob_p &lt;- rep(1,1000) #This is the prior. It is flat = stupid prior prob_data &lt;- dbinom(6,size=9,prob=p_grid) posterior &lt;- prob_data * prob_p posterior &lt;- posterior / sum(posterior) #sum(posterior) = average likelihood print(&quot;First 10 posteriors&quot;) posterior[1:10] par(mfrow = c(2,2)) plot(p_grid,main = &quot;p_grid&quot;) #The grid plot(prob_p,main = &quot;prob_p (prior)&quot;) #Prior plot(prob_data,main = &quot;prob_data (likelihood)&quot;) #likelihood plot(posterior,main = &quot;posterior&quot;) #Posterior ## [1] &quot;First 10 posteriors&quot; ## [1] 0.000000e+00 8.433659e-19 5.381333e-17 6.111249e-16 3.423368e-15 ## [6] 1.301978e-14 3.875963e-14 9.744233e-14 2.164638e-13 4.375070e-13 Notice that the p_grid is flat. Hence the Now lets sample from the prior distribution. #Code 3.3 set.seed(1337) samples &lt;- sample(p_grid ,prob=posterior ,size=10000 #The higher the number the smoother the curve ,replace=TRUE ) #Code 3.4 par(mfrow = c(2,1)) plot(samples) library(rethinking) dens(samples) We see that the densitity plot shows the estimated probability of water on the globe. What we have seen so far: we are replicating the posterior probability of water based on the data we have at hand. This is not of much value. We are next going to use the samples to understand the posterior. 3.2 Sampling to summarize Now we see that the models work is done. Although now it is up to the analyst to interprete the posterior distribution. This includes: How much posterior probability lies below some parameter value? How much posterior probability lies between two parameter values? Which parameter value marks the lower 5% of the posterior probability? Which range of parameter values contains 90% of the posterior probability? Which parameter value has highest posterior probability? This is essentially about three things: 1) defined boundaries, 2) defined probability mass, 3) point estimates. The following sections describe these. 3.2.1 Defined Boundaries This options(scipen = 0) # add up posterior probability where p &lt; 0.5 p_grid sum(posterior[p_grid &lt; 0.5]) #Sum all posteriors where the p_grid is &lt; 50% ## [1] 0.000000000 0.001001001 0.002002002 0.003003003 0.004004004 0.005005005 ## [7] 0.006006006 0.007007007 0.008008008 0.009009009 0.010010010 0.011011011 ## [13] 0.012012012 0.013013013 0.014014014 0.015015015 0.016016016 0.017017017 ## [19] 0.018018018 0.019019019 0.020020020 0.021021021 0.022022022 0.023023023 ## [25] 0.024024024 0.025025025 0.026026026 0.027027027 0.028028028 0.029029029 ## [31] 0.030030030 0.031031031 0.032032032 0.033033033 0.034034034 0.035035035 ## [37] 0.036036036 0.037037037 0.038038038 0.039039039 0.040040040 0.041041041 ## [43] 0.042042042 0.043043043 0.044044044 0.045045045 0.046046046 0.047047047 ## [49] 0.048048048 0.049049049 0.050050050 0.051051051 0.052052052 0.053053053 ## [55] 0.054054054 0.055055055 0.056056056 0.057057057 0.058058058 0.059059059 ## [61] 0.060060060 0.061061061 0.062062062 0.063063063 0.064064064 0.065065065 ## [67] 0.066066066 0.067067067 0.068068068 0.069069069 0.070070070 0.071071071 ## [73] 0.072072072 0.073073073 0.074074074 0.075075075 0.076076076 0.077077077 ## [79] 0.078078078 0.079079079 0.080080080 0.081081081 0.082082082 0.083083083 ## [85] 0.084084084 0.085085085 0.086086086 0.087087087 0.088088088 0.089089089 ## [91] 0.090090090 0.091091091 0.092092092 0.093093093 0.094094094 0.095095095 ## [97] 0.096096096 0.097097097 0.098098098 0.099099099 0.100100100 0.101101101 ## [103] 0.102102102 0.103103103 0.104104104 0.105105105 0.106106106 0.107107107 ## [109] 0.108108108 0.109109109 0.110110110 0.111111111 0.112112112 0.113113113 ## [115] 0.114114114 0.115115115 0.116116116 0.117117117 0.118118118 0.119119119 ## [121] 0.120120120 0.121121121 0.122122122 0.123123123 0.124124124 0.125125125 ## [127] 0.126126126 0.127127127 0.128128128 0.129129129 0.130130130 0.131131131 ## [133] 0.132132132 0.133133133 0.134134134 0.135135135 0.136136136 0.137137137 ## [139] 0.138138138 0.139139139 0.140140140 0.141141141 0.142142142 0.143143143 ## [145] 0.144144144 0.145145145 0.146146146 0.147147147 0.148148148 0.149149149 ## [151] 0.150150150 0.151151151 0.152152152 0.153153153 0.154154154 0.155155155 ## [157] 0.156156156 0.157157157 0.158158158 0.159159159 0.160160160 0.161161161 ## [163] 0.162162162 0.163163163 0.164164164 0.165165165 0.166166166 0.167167167 ## [169] 0.168168168 0.169169169 0.170170170 0.171171171 0.172172172 0.173173173 ## [175] 0.174174174 0.175175175 0.176176176 0.177177177 0.178178178 0.179179179 ## [181] 0.180180180 0.181181181 0.182182182 0.183183183 0.184184184 0.185185185 ## [187] 0.186186186 0.187187187 0.188188188 0.189189189 0.190190190 0.191191191 ## [193] 0.192192192 0.193193193 0.194194194 0.195195195 0.196196196 0.197197197 ## [199] 0.198198198 0.199199199 0.200200200 0.201201201 0.202202202 0.203203203 ## [205] 0.204204204 0.205205205 0.206206206 0.207207207 0.208208208 0.209209209 ## [211] 0.210210210 0.211211211 0.212212212 0.213213213 0.214214214 0.215215215 ## [217] 0.216216216 0.217217217 0.218218218 0.219219219 0.220220220 0.221221221 ## [223] 0.222222222 0.223223223 0.224224224 0.225225225 0.226226226 0.227227227 ## [229] 0.228228228 0.229229229 0.230230230 0.231231231 0.232232232 0.233233233 ## [235] 0.234234234 0.235235235 0.236236236 0.237237237 0.238238238 0.239239239 ## [241] 0.240240240 0.241241241 0.242242242 0.243243243 0.244244244 0.245245245 ## [247] 0.246246246 0.247247247 0.248248248 0.249249249 0.250250250 0.251251251 ## [253] 0.252252252 0.253253253 0.254254254 0.255255255 0.256256256 0.257257257 ## [259] 0.258258258 0.259259259 0.260260260 0.261261261 0.262262262 0.263263263 ## [265] 0.264264264 0.265265265 0.266266266 0.267267267 0.268268268 0.269269269 ## [271] 0.270270270 0.271271271 0.272272272 0.273273273 0.274274274 0.275275275 ## [277] 0.276276276 0.277277277 0.278278278 0.279279279 0.280280280 0.281281281 ## [283] 0.282282282 0.283283283 0.284284284 0.285285285 0.286286286 0.287287287 ## [289] 0.288288288 0.289289289 0.290290290 0.291291291 0.292292292 0.293293293 ## [295] 0.294294294 0.295295295 0.296296296 0.297297297 0.298298298 0.299299299 ## [301] 0.300300300 0.301301301 0.302302302 0.303303303 0.304304304 0.305305305 ## [307] 0.306306306 0.307307307 0.308308308 0.309309309 0.310310310 0.311311311 ## [313] 0.312312312 0.313313313 0.314314314 0.315315315 0.316316316 0.317317317 ## [319] 0.318318318 0.319319319 0.320320320 0.321321321 0.322322322 0.323323323 ## [325] 0.324324324 0.325325325 0.326326326 0.327327327 0.328328328 0.329329329 ## [331] 0.330330330 0.331331331 0.332332332 0.333333333 0.334334334 0.335335335 ## [337] 0.336336336 0.337337337 0.338338338 0.339339339 0.340340340 0.341341341 ## [343] 0.342342342 0.343343343 0.344344344 0.345345345 0.346346346 0.347347347 ## [349] 0.348348348 0.349349349 0.350350350 0.351351351 0.352352352 0.353353353 ## [355] 0.354354354 0.355355355 0.356356356 0.357357357 0.358358358 0.359359359 ## [361] 0.360360360 0.361361361 0.362362362 0.363363363 0.364364364 0.365365365 ## [367] 0.366366366 0.367367367 0.368368368 0.369369369 0.370370370 0.371371371 ## [373] 0.372372372 0.373373373 0.374374374 0.375375375 0.376376376 0.377377377 ## [379] 0.378378378 0.379379379 0.380380380 0.381381381 0.382382382 0.383383383 ## [385] 0.384384384 0.385385385 0.386386386 0.387387387 0.388388388 0.389389389 ## [391] 0.390390390 0.391391391 0.392392392 0.393393393 0.394394394 0.395395395 ## [397] 0.396396396 0.397397397 0.398398398 0.399399399 0.400400400 0.401401401 ## [403] 0.402402402 0.403403403 0.404404404 0.405405405 0.406406406 0.407407407 ## [409] 0.408408408 0.409409409 0.410410410 0.411411411 0.412412412 0.413413413 ## [415] 0.414414414 0.415415415 0.416416416 0.417417417 0.418418418 0.419419419 ## [421] 0.420420420 0.421421421 0.422422422 0.423423423 0.424424424 0.425425425 ## [427] 0.426426426 0.427427427 0.428428428 0.429429429 0.430430430 0.431431431 ## [433] 0.432432432 0.433433433 0.434434434 0.435435435 0.436436436 0.437437437 ## [439] 0.438438438 0.439439439 0.440440440 0.441441441 0.442442442 0.443443443 ## [445] 0.444444444 0.445445445 0.446446446 0.447447447 0.448448448 0.449449449 ## [451] 0.450450450 0.451451451 0.452452452 0.453453453 0.454454454 0.455455455 ## [457] 0.456456456 0.457457457 0.458458458 0.459459459 0.460460460 0.461461461 ## [463] 0.462462462 0.463463463 0.464464464 0.465465465 0.466466466 0.467467467 ## [469] 0.468468468 0.469469469 0.470470470 0.471471471 0.472472472 0.473473473 ## [475] 0.474474474 0.475475475 0.476476476 0.477477477 0.478478478 0.479479479 ## [481] 0.480480480 0.481481481 0.482482482 0.483483483 0.484484484 0.485485485 ## [487] 0.486486486 0.487487487 0.488488488 0.489489489 0.490490490 0.491491491 ## [493] 0.492492492 0.493493493 0.494494494 0.495495495 0.496496496 0.497497497 ## [499] 0.498498498 0.499499499 0.500500501 0.501501502 0.502502503 0.503503504 ## [505] 0.504504505 0.505505506 0.506506507 0.507507508 0.508508509 0.509509510 ## [511] 0.510510511 0.511511512 0.512512513 0.513513514 0.514514515 0.515515516 ## [517] 0.516516517 0.517517518 0.518518519 0.519519520 0.520520521 0.521521522 ## [523] 0.522522523 0.523523524 0.524524525 0.525525526 0.526526527 0.527527528 ## [529] 0.528528529 0.529529530 0.530530531 0.531531532 0.532532533 0.533533534 ## [535] 0.534534535 0.535535536 0.536536537 0.537537538 0.538538539 0.539539540 ## [541] 0.540540541 0.541541542 0.542542543 0.543543544 0.544544545 0.545545546 ## [547] 0.546546547 0.547547548 0.548548549 0.549549550 0.550550551 0.551551552 ## [553] 0.552552553 0.553553554 0.554554555 0.555555556 0.556556557 0.557557558 ## [559] 0.558558559 0.559559560 0.560560561 0.561561562 0.562562563 0.563563564 ## [565] 0.564564565 0.565565566 0.566566567 0.567567568 0.568568569 0.569569570 ## [571] 0.570570571 0.571571572 0.572572573 0.573573574 0.574574575 0.575575576 ## [577] 0.576576577 0.577577578 0.578578579 0.579579580 0.580580581 0.581581582 ## [583] 0.582582583 0.583583584 0.584584585 0.585585586 0.586586587 0.587587588 ## [589] 0.588588589 0.589589590 0.590590591 0.591591592 0.592592593 0.593593594 ## [595] 0.594594595 0.595595596 0.596596597 0.597597598 0.598598599 0.599599600 ## [601] 0.600600601 0.601601602 0.602602603 0.603603604 0.604604605 0.605605606 ## [607] 0.606606607 0.607607608 0.608608609 0.609609610 0.610610611 0.611611612 ## [613] 0.612612613 0.613613614 0.614614615 0.615615616 0.616616617 0.617617618 ## [619] 0.618618619 0.619619620 0.620620621 0.621621622 0.622622623 0.623623624 ## [625] 0.624624625 0.625625626 0.626626627 0.627627628 0.628628629 0.629629630 ## [631] 0.630630631 0.631631632 0.632632633 0.633633634 0.634634635 0.635635636 ## [637] 0.636636637 0.637637638 0.638638639 0.639639640 0.640640641 0.641641642 ## [643] 0.642642643 0.643643644 0.644644645 0.645645646 0.646646647 0.647647648 ## [649] 0.648648649 0.649649650 0.650650651 0.651651652 0.652652653 0.653653654 ## [655] 0.654654655 0.655655656 0.656656657 0.657657658 0.658658659 0.659659660 ## [661] 0.660660661 0.661661662 0.662662663 0.663663664 0.664664665 0.665665666 ## [667] 0.666666667 0.667667668 0.668668669 0.669669670 0.670670671 0.671671672 ## [673] 0.672672673 0.673673674 0.674674675 0.675675676 0.676676677 0.677677678 ## [679] 0.678678679 0.679679680 0.680680681 0.681681682 0.682682683 0.683683684 ## [685] 0.684684685 0.685685686 0.686686687 0.687687688 0.688688689 0.689689690 ## [691] 0.690690691 0.691691692 0.692692693 0.693693694 0.694694695 0.695695696 ## [697] 0.696696697 0.697697698 0.698698699 0.699699700 0.700700701 0.701701702 ## [703] 0.702702703 0.703703704 0.704704705 0.705705706 0.706706707 0.707707708 ## [709] 0.708708709 0.709709710 0.710710711 0.711711712 0.712712713 0.713713714 ## [715] 0.714714715 0.715715716 0.716716717 0.717717718 0.718718719 0.719719720 ## [721] 0.720720721 0.721721722 0.722722723 0.723723724 0.724724725 0.725725726 ## [727] 0.726726727 0.727727728 0.728728729 0.729729730 0.730730731 0.731731732 ## [733] 0.732732733 0.733733734 0.734734735 0.735735736 0.736736737 0.737737738 ## [739] 0.738738739 0.739739740 0.740740741 0.741741742 0.742742743 0.743743744 ## [745] 0.744744745 0.745745746 0.746746747 0.747747748 0.748748749 0.749749750 ## [751] 0.750750751 0.751751752 0.752752753 0.753753754 0.754754755 0.755755756 ## [757] 0.756756757 0.757757758 0.758758759 0.759759760 0.760760761 0.761761762 ## [763] 0.762762763 0.763763764 0.764764765 0.765765766 0.766766767 0.767767768 ## [769] 0.768768769 0.769769770 0.770770771 0.771771772 0.772772773 0.773773774 ## [775] 0.774774775 0.775775776 0.776776777 0.777777778 0.778778779 0.779779780 ## [781] 0.780780781 0.781781782 0.782782783 0.783783784 0.784784785 0.785785786 ## [787] 0.786786787 0.787787788 0.788788789 0.789789790 0.790790791 0.791791792 ## [793] 0.792792793 0.793793794 0.794794795 0.795795796 0.796796797 0.797797798 ## [799] 0.798798799 0.799799800 0.800800801 0.801801802 0.802802803 0.803803804 ## [805] 0.804804805 0.805805806 0.806806807 0.807807808 0.808808809 0.809809810 ## [811] 0.810810811 0.811811812 0.812812813 0.813813814 0.814814815 0.815815816 ## [817] 0.816816817 0.817817818 0.818818819 0.819819820 0.820820821 0.821821822 ## [823] 0.822822823 0.823823824 0.824824825 0.825825826 0.826826827 0.827827828 ## [829] 0.828828829 0.829829830 0.830830831 0.831831832 0.832832833 0.833833834 ## [835] 0.834834835 0.835835836 0.836836837 0.837837838 0.838838839 0.839839840 ## [841] 0.840840841 0.841841842 0.842842843 0.843843844 0.844844845 0.845845846 ## [847] 0.846846847 0.847847848 0.848848849 0.849849850 0.850850851 0.851851852 ## [853] 0.852852853 0.853853854 0.854854855 0.855855856 0.856856857 0.857857858 ## [859] 0.858858859 0.859859860 0.860860861 0.861861862 0.862862863 0.863863864 ## [865] 0.864864865 0.865865866 0.866866867 0.867867868 0.868868869 0.869869870 ## [871] 0.870870871 0.871871872 0.872872873 0.873873874 0.874874875 0.875875876 ## [877] 0.876876877 0.877877878 0.878878879 0.879879880 0.880880881 0.881881882 ## [883] 0.882882883 0.883883884 0.884884885 0.885885886 0.886886887 0.887887888 ## [889] 0.888888889 0.889889890 0.890890891 0.891891892 0.892892893 0.893893894 ## [895] 0.894894895 0.895895896 0.896896897 0.897897898 0.898898899 0.899899900 ## [901] 0.900900901 0.901901902 0.902902903 0.903903904 0.904904905 0.905905906 ## [907] 0.906906907 0.907907908 0.908908909 0.909909910 0.910910911 0.911911912 ## [913] 0.912912913 0.913913914 0.914914915 0.915915916 0.916916917 0.917917918 ## [919] 0.918918919 0.919919920 0.920920921 0.921921922 0.922922923 0.923923924 ## [925] 0.924924925 0.925925926 0.926926927 0.927927928 0.928928929 0.929929930 ## [931] 0.930930931 0.931931932 0.932932933 0.933933934 0.934934935 0.935935936 ## [937] 0.936936937 0.937937938 0.938938939 0.939939940 0.940940941 0.941941942 ## [943] 0.942942943 0.943943944 0.944944945 0.945945946 0.946946947 0.947947948 ## [949] 0.948948949 0.949949950 0.950950951 0.951951952 0.952952953 0.953953954 ## [955] 0.954954955 0.955955956 0.956956957 0.957957958 0.958958959 0.959959960 ## [961] 0.960960961 0.961961962 0.962962963 0.963963964 0.964964965 0.965965966 ## [967] 0.966966967 0.967967968 0.968968969 0.969969970 0.970970971 0.971971972 ## [973] 0.972972973 0.973973974 0.974974975 0.975975976 0.976976977 0.977977978 ## [979] 0.978978979 0.979979980 0.980980981 0.981981982 0.982982983 0.983983984 ## [985] 0.984984985 0.985985986 0.986986987 0.987987988 0.988988989 0.989989990 ## [991] 0.990990991 0.991991992 0.992992993 0.993993994 0.994994995 0.995995996 ## [997] 0.996996997 0.997997998 0.998998999 1.000000000 ## [1] 0.1718746 We see that the sum of the first 10 probabilities, as these are below. 3.2.2 Defined Probability Mass This is about finding an interval and interpreting this. For example, we want to know the region between the 10% and 90% quantiles, or the first 80%. This can be solved by doing: #R Code 3.9 quantile(samples,0.8) #Boundaries of lower 80% posterior probability, thus it starts at 0 #R Code 3.10 quantile(samples,c(0.1,0.9)) #Or between 10% and 90% posterior probability. hence midle 80% posterior probability ## 80% ## 0.7607608 ## 10% 90% ## 0.4484484 0.8119119 These we call percentile intervals (PI). For this there is functionality in the rethinkinglibrary. PI(samples,prob = 0.5) ## 25% 75% ## 0.5415415 0.7377377 We see that this will autoamtically find the center probability of the posterior distribution. This may not be convenient if for instance the peak is outside of the region that PI return. Therefore we have the functio HPDI, which stands for highest posterior density interval. This will find the densest probability mass. This is justified, as you can end up with the same probability mass region with many combinations, hence HPDI is merely helping with this procedure. HPDI(samples,prob = 0.5) ## |0.5 0.5| ## 0.5475475 0.7417417 Now we see that it finds a more narrow region aggregating to the same probability. The following also exemplify this: Many will confuse this with a confidence interval, while they will be named compatibility or credible intervals so they are not mixed up. Criticism of traditional confidence intervals. One sees that a common interpretation of confidence intervals is that with a CI of 95%, means that there is a 95% probability of the true value lying within the interval. THIS IS WRONG, that is a Bayesian interpretation and can only be used in a Bayesian setting. This is actually about what if you repeat an experiment, then 95% of the computed intervals will include the ‘true’ value. See page 58. Criticims of ‘true’ values. Remember that you are working in a small world and thus true answers can never really be found, these belong in the large world. 3.2.3 Point Estimates Point estimates are the third and final common summary task for the posterior distribution. Often this is not wanted, as point estimates will remove valuable information. The following are examples of getting the point estimates: par(mfrow = c(1,1)) plot(x = p_grid,y = posterior,type = &#39;l&#39;,main = &quot;Posterior distribution&quot;,sub = &quot;Showing different point estimates&quot;) grid() abline(v = p_grid[ which.max(posterior) ],col = &quot;darkblue&quot;) abline(v = chainmode( samples , adj=0.01 ),col = &quot;darkgreen&quot;) abline(v = mean( samples ),col = &quot;darkred&quot;) abline(v = median( samples ),col = &quot;darkorange&quot;) legend(&quot;topleft&quot;,legend = c(&quot;max posteriod&quot;,&quot;mode&quot;,&quot;mean&quot;,&quot;median&quot;),lty = 1,col = c(&quot;darkblue&quot;,&quot;darkgreen&quot;,&quot;darkred&quot;,&quot;darkorange&quot;)) #R Code 3.14 to 3.16 p_grid[ which.max(posterior) ] chainmode( samples , adj=0.01 ) #The mode: i.e., the most often appearing value mean( samples ) median( samples ) ## [1] 0.6666667 ## [1] 0.6830388 ## [1] 0.6359439 ## [1] 0.6446446 The question is then: what point estimate to use? We can apply a loss function to support the decision. We can find a series of loss given the grid and the loss function. loss &lt;- sapply(X = p_grid ,FUN = function(d) sum(posterior * abs(d-p_grid))) And for one specific point estimate: sum(posterior * abs(0.5 - p_grid)) #0.1640626 ## [1] 0.1640626 To find the point estimate with the lowest loss one can say: p_grid[which.min(loss)] #0.6446446, equal to the median ## [1] 0.6446446 There are naturally also other loss functions, e.g., \\((d-p)^2\\), which would lead to the posterio mean. 3.3 Sampling to simulate prediction We can sample data to simulate the observations from the model. It has the following advantages: Model Design: One can sample from the prior and see what one expects. This we will look more into in a later section. Model Checking: To see if you end up with the same model. Software Validation: One can use it to simulate the data that the models was built on. To check if the model can replicate the underlying data. I guess this is to check if something is broken. Research Design: Forecasting: One can simulate what will happen in the future. The following is an overview of how to simulate observations and make model checks: 3.3.1 Dummy Data This is basically drawing data given a certain probability of the different outcomes. One must remember that the outputs of such are small world numbers. The folo dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=9 #Size of each set ,prob=0.7) #70% water simplehist( dummy_w , xlab=&quot;dummy water count&quot; ) We see that we get mostly combinations with 6 or 7 water. 3.3.2 Model Checking This has two purposes: Ensure that the model fit worked correctly Evaluate the adequacy of a model for some purpose Did the software work? This is basically just to check if you have set it up correctly. Is the model adequate? There are no true models, hence you need to assess where the model fails to describe the data. One also experience that models tend to be overconfident. One wants to sample from the distribution to see if the model can be replicated. If we end up seeing very different results, then one should start considering is somthing is not taken into account. 3.4 Exercises 3.4.1 3M1 Suppose the globe tossing data had turned out to be 8 water in 15 tosses. Construct the posterior distribution, using grid approximation. Use the same flat prior as before. length = 100 # define grid p_grid &lt;- seq(from=0 , to=1 , length.out = length) # define prior prior &lt;- rep(1, length) #The flat (stupid) prior # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 8 #Successes, water in this example ,size = 15 #No. of tosses ,prob = p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = p_grid[which.max(posterior)],col = &quot;darkred&quot;,lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) We see that the posterior distribution is now centered almost around 50%, as we almost in every second case see water. 3.4.2 3M2 Draw 10,000 samples from the grid approximation from above. Then use the samples to calculate the 90% HPDI for p. First we draw 10.000 samples and then we can calculate the HDPI (higest posterior density interval) library(rethinking) samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.9) par(mfrow = c(1,1)) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = hdpi[1],lty = 2) abline(v = hdpi[2],lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) 3.4.3 3M3 Construct a posterior predictive check for this model and data. This means simulate the distribution of samples, averaging over the posterior uncertainty in p. What is the probability of observing 8 water in 15 tosses? Posterior predictive check = to inspect the posterior and see if it actually makes sense. par(mfrow = c(1,2)) plot(samples) dens(samples) mean(samples) ## [1] 0.5292899 dbinom(x = 8,size = 15,prob = mean(samples)) ## [1] 0.202942 We see that there is a 0.2 probability that you select 8 water in a total of 15 tosses. 3.4.4 3M4 Using the posterior distribution constructed from the new (8/15) data, now calculate the probability of observing 6 water in 9 tosses. dbinom(x = 6,size = 9,prob = mean(samples)) ## [1] 0.19262 We see a 0.19 probability of getting 6 water in a total of 9 tosses. 3.4.5 3M5 Start over at 3M1, but now use a prior that is zero below p = 0.5 and a constant above p = 0.5. This corresponds to prior information that a majority of the Earth’s surface is water. Repeat each problem above and compare the inferences. What difference does the better prior make? If it helps, compare inferences (using both priors) to the true value p = 0.7. length = 100 # define grid p_grid &lt;- seq(from=0 , to=1 , length.out = length) # define prior prior &lt;- c(rep(0,length/2),rep(1,length/2)) # compute likelihood at each value in grid likelihood &lt;- dbinom(x = 8 #Successes, water in this example ,size = 15 #No. of tosses ,prob = p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting par(mfrow = c(2,1)) plot(prior,type = &#39;l&#39;,main = &quot;Prior&quot;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of water&quot; ,ylab=&quot;posterior probability&quot; ,main = &quot;Posterior distribution&quot; ,sub = paste(length,&quot; points&quot;) ) abline(v = p_grid[which.max(posterior)],col = &quot;darkred&quot;,lty = 2) mtext(paste(&quot;Max =&quot;,round(p_grid[which.max(posterior)],2))) #Calculating HDPI samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.9) abline(v = hdpi[1],lty = 2,col = &quot;darkblue&quot;) abline(v = hdpi[2],lty = 2,col = &quot;darkblue&quot;) Now we would see that the probability of drawing water is relatively higher than what we have seen before. Although that does not mean that one cannot draw land and there is also a chance of drawing only land. Calculating probability of different outcomes dbinom(x = 8,size = 15,prob = mean(samples)) ## [1] 0.1711683 We see that this went from a bit above 20% to almost 17% The following scenario increased. dbinom(x = 6,size = 9,prob = mean(samples)) ## [1] 0.2554195 We see that it is higher now, we went from 0.19 to 0.25. We see that the scenario of getting relatively more water is higher. Now we can also re-simulate the posteriors predictive distributions. library(rethinking) w &lt;- rbinom(10000,size = 15,prob = samples) par(mfrow = c(1,1)) plot(table(w)) Notice that the most frequent is 9 and not 8 as we have seen earlier. 3.4.6 3H1 Using grid approximation, compute the posterior distribution for the probability of a birth being a boy. Assume a uniform prior probability. Which parameter value maximizes the posterior probability? 1 = Male, 0 = Female. Ande birth 1 and 2 are two different datasets. library(rethinking) data(homeworkch3) sum(birth1) + sum(birth2) ## [1] 111 This means that there are 111 boys in the two datasets. length = 100 # define grid p_grid &lt;- seq( from=0 , to=1 , length.out = length ) # define prior prior &lt;- rep( 1 , length ) # compute likelihood at each value in grid likelihood &lt;- dbinom(111 ,size=200 ,prob=p_grid ) # compute product of likelihood and prior unstd.posterior &lt;- likelihood * prior # standardize the posterior, so it sums to 1 posterior &lt;- unstd.posterior / sum(unstd.posterior) #Plotting plot(prior,type = &#39;l&#39;) plot(p_grid ,posterior ,type=&quot;l&quot; ,xlab=&quot;probability of a boy&quot; ,ylab=&quot;posterior probability&quot; ) abline(v = 0.5,lty = 9) abline(v = p_grid[which.max(posterior)],col = &#39;darkgreen&#39;) 3.4.7 3H2 Using the sample function, draw 10,000 random parameter values from the posterior distribution you calculated above. Use these samples to estimate the 50%, 89%, and 97% highest posterior density intervals. samples &lt;- sample(p_grid,size=10000,replace=TRUE,prob=posterior) hdpi &lt;- rethinking::HPDI(samples,prob = 0.5) hdpi ## |0.5 0.5| ## 0.5454545 0.5858586 # |0.5 0.5| #0.5454545 0.5858586 rethinking::HPDI(samples,prob = 0.89) ## |0.89 0.89| ## 0.4949495 0.6060606 rethinking::HPDI(samples,prob = 0.97) ## |0.97 0.97| ## 0.4747475 0.6262626 3.4.8 3H3 Use rbinom to simulate 10,000 replicates of 200 births. You should end up with 10,000 numbers, each one a count of boys out of 200 births. Compare the distribution of predicted numbers of boys to the actual count in the data (111 boys out of 200 births). There are many good ways to visualize the simulations, but the dens command (part of the rethinking package) is probably the easiest way in this case. Does it look like the model fits the data well? That is, does the distribution of predictions include the actual observation as a central, likely outcome? par(mfrow = c(2,1)) dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=200 #Size of each set ,prob=(111/200)) #55% boys dens(dummy_w ,adj = 0.7 #The higher the smoother, def = 0.5 ) hist(dummy_w) We see from the plot that it is centered around 111. We can look at the mean and median in the following. mean(dummy_w) median(dummy_w) ## [1] 110.9869 ## [1] 111 We see that it is very close to 111 and the median is 111. 3.4.9 3H4 Now compare 10,000 counts of boys from 100 simulated first borns only to the number of boys in the first births, birth1. How does the model look in this light? sum(birth1) ## [1] 51 dummy_w &lt;- rbinom(n = 100000 #No of observations ,size=100 #Size of each set ,prob=(111/200)) #55% boys dens(dummy_w,adj = 0.7) abline(v = sum(birth1),col = &quot;darkred&quot;) hist(dummy_w) Now we see that the the densest part of the density plot is left skewed compared to the first born boys. 3.4.10 3H5 The model assumes that sex of first and second births are independent. To check this assumption, focus now on second births that followed female first borns. Compare 10,000 simulated counts of boys to only those second births that followed girls. To do this correctly, you need to count the number of first borns who were girls and simulate that many births, 10,000 times. Compare the counts of boys in your simulations to the actual observed count of boys following girls. How does the model look in this light? Any guesses what is going on in these data? We know that it is a girl if the first born = 0. Thus we want to subset on these df &lt;- birth2[birth1 == 0] #Boys following girls dummy_w &lt;- rbinom(n = 100000 #No of observations ,size = length(birth1)-sum(birth1) #Size of each set ,prob = (111/200)) #55% boys dens(dummy_w,adj = 0.7) abline(v = sum(df) #no. of boys following girls ,col = &quot;darkred&quot;) hist(dummy_w) "],["geocentric-models.html", "4 Geocentric Models 4.1 Why Normal distributions are normal 4.2 A language for describing models 4.3 Gaussian model of height 4.4 Linear prediction 4.5 Curves form lines 4.6 Exercises", " 4 Geocentric Models This chapter introduce lienar regression as a Bayesian procedure. Thus we will apply probability measures for intepretation, as that is what Bayesian is. 4.1 Why Normal distributions are normal This section examplifies why normal distributions are normal and how it is often seen in nature. Jesper shows an example where binomial random draws will tend to be centered around 0 as there are the most paths leading towards the middle. That is why we often find the normal distribution in real life. It express that normal distributions come from addition of random events, e.g., the result of consecutive coin tosses. Also the product of small numbers approximate addition, hence the result of such outcomes is similar to the added scenario. Although with large numbers we will tend not to get a normal distribution by finding the product hereof. The book aim to teaach a strategy to model data and not a single model just for the toolbox. 4.2 A language for describing models This chapter summarize how models are defined and defines mathematical terminalogy. Recipe of defining models Recognize the variables that you want to understand. Observable variables = data. While unobservable things = parameters, e.g., averages and rates, e.g., you dont observe GDP growth rates, but it can be deducted from assessing varaibles (observed data). Each variable can be defined in terms of other variables or in terms of a probability distribution. This enables one to learn about associations between variables. The combination of variables and their probability distributions defines a joint generative model. This can be used to simulate hypothetical observations as well as analyze real ones. Mathematical terminology: We see that \\(W \\sim Binomial(N,p)\\) means that W (water) variable is binomial, it has two outcomes, yes or no. and then we have \\(p ~ Uniform(0,1)\\), means that p (proportion of water on the globe) is between 0 and 1. Notice that the wavy symbol ~ means that the models are stochastic, i.e., there are no instances on the left that are known with certainty. I.e., W is distributed as binomial Regarding priors: The priors must be set before you see the data, as if you did not do that, then it is no more a prior. If you have no idea what to set as a prior, then you set it to some random value, e.g., making it uniform to be within a specific range. 4.3 Gaussian model of height The following is an example to understand normal distributions of normal distributions. data(Howell1) d &lt;- Howell1 #d for data frame str(d) ## &#39;data.frame&#39;: 544 obs. of 4 variables: ## $ height: num 152 140 137 157 145 ... ## $ weight: num 47.8 36.5 31.9 53 41.3 ... ## $ age : num 63 63 65 41 51 35 32 27 19 54 ... ## $ male : int 1 0 0 1 0 1 0 1 0 1 ... We see that we have height, wiehgt, age and male. This can also be summarized with mean, sd, percentiles and histograms. precis(d) x 138.2635963 35.6106176 29.3443934 0.4724265 x 27.6024476 14.7191782 20.7468882 0.4996986 x 81.108550 9.360721 1.000000 0.000000 x 165.73500 54.50289 66.13500 1.00000 x ▁▁▁▁▁▁▁▂▁▇▇▅▁ ▁▂▃▂▂▂▂▅▇▇▃▂▁ ▇▅▅▃▅▂▂▁▁ ▇▁▁▁▁▁▁▁▁▇ And thus we see the distributions. We are only going to be working with people above or equal to 18 years of age. Notice that we cannot use the histograms to suggest a distribution, that is because we need to select the priors on berforehand. The precis function also shows the compatibility intervals (recall the probability intervals) d2 &lt;- d[d$age &gt;= 18,] 4.3.1 The model Now we can define the model with with the following terms: \\[h_i \\sim Normal(\\mu,\\sigma)\\] \\(h_i \\sim \\mathcal{N}\\), this is exactly the same, just shortened. Meaning that height is stochastic, given the wavy character ~, it is normal with mu and sigma mean and standard deviation. The i means each element in vector h. Now we can specify the priors. this is done independently for each parameter (unobserved). It looks the following: We see that we have the likelihood that consist of the priors, which are specified afterwards. This can be plotted with. It means that the mean is centered in 178 and 20 standard deviation, thus two standard deviations (95% being 40), we will have 95% of the people within 178 +- 40. curve(dnorm(x,178,20),from = 100,to = 250,main = &#39;Mean prior&#39;) curve(dunif(x,0,50),from = -10,to = 60,main = &quot;Sigma prior&quot;) Now one can sample heights based on the two priors. samples &lt;- 10000 sample_mu &lt;- rnorm(samples,178,20) #notice we cannot take the mean of the data, as it is a prior!!! sample_sigma &lt;- runif(samples,0,50) prior_h &lt;- rnorm(samples,sample_mu,sample_sigma) dens(prior_h) One see that the mean is around the 178. The more samples we draw, the more normal will it look. Now this makes sense since the mean standard deviation is rather low, lets look at an example with a large standard deviation to the mean. samples &lt;- 10000 sample_mu &lt;- rnorm(samples,178,100) #notice we cannot take the mean of the data, as it is a prior!!! prior_h &lt;- rnorm(samples,sample_mu,sample_sigma) dens(prior_h) Now we see that the height can go all the way up to 600 and down to a negative number. When you have a lot of data instances, then such a prior is not harmfull even though it clearly expects a lot of people being very tall and some even below 0. So having an unreasonable prior is not necessarily bad. 4.3.2 Grid approximation of the posterior distribution Notice that in practice we dont really do this, as it is computationally heavy. Although it is shown for explanatory reasons. mu.list &lt;- seq(from=150, to=160, length.out=100) sigma.list &lt;- seq(from=7, to=9, length.out=100) #Expand list, mu and sigma, post &lt;- expand.grid(mu = mu.list, sigma = sigma.list) post$LL &lt;- sapply(1:nrow(post), function(i) sum( dnorm(x = d2$height ,mean = post$mu[i] ,sd = post$sigma[i] ,log = TRUE) ) ) post$prod &lt;- post$LL + dnorm( post$mu , 178 , 20 , TRUE) + dunif(post$sigma,0, 50, TRUE) post$prob &lt;- exp(post$prod - max(post$prod)) Now, lets inspect the posterior distribution par(mfrow = c(2,1)) #A contour plot contour_xyz(post$mu, post$sigma, post$prob) #A heatmap image_xyz(x = post$mu,y = post$sigma,z = post$prob) What we get from this, is the most probable combinations of the mu and sigma. Hence we see that very often the mean will be between 154 and 155 while the standard deviation is between 7.5 and 8. 4.3.3 Sampling from the posterior Now we are going to sample from the posterior, just as in the globe tossing problem. Here we are just also to sample from the mean and standard deviation. This is done by first making a set of samples, where each row number is listed, basically just an index. Then we use the index to return the given mean and standard deviation. sample.rows &lt;- sample(1:nrow(post),size = 10000, replace = TRUE) #We can draw the same instance twice sample.mu &lt;- post$mu[sample.rows] sample.sigma &lt;- post$sigma[sample.rows] We see that post$mu and sigma is a grid ranging from respectively 150 to 160 and 7 to 9. e.g., m &lt;- matrix(post$mu,nrow = 100,ncol = 100) head(m[1:10,1:10],n = 10) ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 150.0000 ## [2,] 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 150.1010 ## [3,] 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 150.2020 ## [4,] 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 150.3030 ## [5,] 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 150.4040 ## [6,] 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 150.5051 ## [7,] 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 150.6061 ## [8,] 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 150.7071 ## [9,] 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 150.8081 ## [10,] 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 150.9091 ## [,9] [,10] ## [1,] 150.0000 150.0000 ## [2,] 150.1010 150.1010 ## [3,] 150.2020 150.2020 ## [4,] 150.3030 150.3030 ## [5,] 150.4040 150.4040 ## [6,] 150.5051 150.5051 ## [7,] 150.6061 150.6061 ## [8,] 150.7071 150.7071 ## [9,] 150.8081 150.8081 ## [10,] 150.9091 150.9091 Now lets plot the samples. par(mfrow = c(1,1)) plot(sample.mu,sample.sigma ,cex = 0.5,pch = 16 #Dot sizes and shape ,col = col.alpha(rangi2,0.1) #make transparant colors ) Now we can inspect the priors. par(mfrow = c(2,1)) dens(sample.mu) dens(sample.sigma) These should have been more or less normally distributed according to the book, but for some reason they are not. 4.3.4 Finding the posterior distribution with quap This is using quadratic approximation. data(&quot;Howell1&quot;) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] We can spcify the model with the following: \\[h_i \\sim Normal(\\mu,\\sigma)\\] \\[\\mu \\sim Normal(178,20)\\] \\[\\sigma \\sim Uniform(0,150)\\] While the equivilent in code is: height ~ dnorm(mu,sigma) mu ~ dnorm(178,10) sigma ~ dunif(0,50) This can be placed in a list: flist &lt;- alist( height ~ dnorm(mu,sigma) ,mu ~ dnorm(178,10) ,sigma ~ dunif(0,50) ) Now we can fit the model to the data: m4.1 &lt;- quap(flist,data = d2) precis(m4.1) x 154.636775 7.731408 x 0.4117468 0.2913949 x 153.978724 7.265703 x 155.294826 8.197114 4.3.5 Sampling from a quap Now we are going to sample from the quadratic approximation. Covariances is key to quadratic approximation. This can be found with: vcov(m4.1) ## mu sigma ## mu 0.1695354471 0.0008701619 ## sigma 0.0008701619 0.0849109920 This tells us how each parameter (unobserved) relates to every other parameter in the posterior distribution. In this scenario covariance does not matter a lot, but when we have a predictor variable, it will be key. This can be extended to 2 elements: A vector of variances for the parameters A correlation matrix that tells us how changes in any parameter lead to correlated changes in others. diag(vcov(m4.1)) cov2cor(vcov(m4.1)) ## mu sigma ## 0.16953545 0.08491099 ## mu sigma ## mu 1.000000000 0.007252502 ## sigma 0.007252502 1.000000000 We see that a change of 1 in mu will lead to a change of 1 in mu, and it will lead to a change of 0.0018 in sigma. This is very small, hence we can see that there is no correlation between the two of these. Thus, learning the mean will not tell anything about the standard deviation. Getting samples post &lt;- extract.samples(m4.1,n = 10000) head(post) mu sigma 154.2506 7.260139 155.2653 7.831658 155.2295 8.015096 155.4242 7.819525 154.6868 8.088762 154.5212 8.095557 precis(post) x 154.633205 7.736503 x 0.4122893 0.2928388 x 153.971768 7.265452 x 155.291918 8.206378 x ▁▁▅▇▂▁▁▁ ▁▁▁▂▅▇▇▃▁▁▁ We can see that distributions look more or less normal. plot(post,pch = 20 ,col = col.alpha(rangi2,0.1)) 4.4 Linear prediction The procedure that we end up going through: 1. Use link to generate distributions of posterior value for mu. 2. Use summary functions like mean or PI to find averages and lower and upper bounds of mu, for each value of the predictor variable. 3. Finally, use plotting functions like lines and shade to draw the lines and intervels. Or you might plot the distributions of the predictions, or do even something else. Now we are going to regress one variable on another, hence we have a predictor and an outcome variable. The following example is predicting height given the weight. plot(d2$height ~ d2$weight) There is clearly a relationship. 4.4.1 The lienar mdoel strategy To specify this model we will say: let x = weight, thus \\(\\bar{x}\\) is the average weight parameter of the observed variable. Now we see that heights are stochastic and normally distributed and is described by the mean and standard deviation. To predict height, we need the mean of the weight variable. This one is deterministic hence the = and not the ~ sign. It consists of the priors (assumptions), where we express \\(\\alpha\\), \\(\\beta\\) and \\(\\sigma\\). These are all stochastic, where \\(\\alpha\\) and \\(\\beta\\) are normally distributed while \\(\\sigma\\) is uniform. Now one can simulate the priors. set.seed(2971) N &lt;- 100 a &lt;- rnorm(N,178,20) #notice we cannot take the mean of the data, as it is a prior!!! b &lt;- rnorm(N,0,10) This returns 100 pairs of \\(\\alpha\\) and \\(\\beta\\). This can be plotted with: plot(NULL ,xlim=range(d2$weight), ylim=c(-100,400) ,xlab=&quot;weight&quot;, ylab=&quot;height&quot; ) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar) ,from=min(d2$weight), to=max(d2$weight) ,add=TRUE ,col=col.alpha(&quot;black&quot;,0.2)) We see that that we get many predictions where we expect a person to be higher than the worlds largest and smaller than the worlds smallest. We can address with by taking the logarithm, which is done in the following: Hence we see that log is added to the equation. b &lt;- rlnorm(10000,0,1) dens(b,xlim = c(0,5),adj=0.1) Doing the prior predictive simulation again, to compare with the logarithm of beta. par(mfrow = c(1,2)) set.seed(2971) N &lt;- 100 a &lt;- rnorm(N,178,20) b &lt;- rnorm(N,0,10) plot(NULL,xlim=range(d2$weight), ylim=c(-100,400),xlab=&quot;weight&quot;, ylab=&quot;height&quot; ) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar),from=min(d2$weight), to=max(d2$weight),add=TRUE,col=col.alpha(&quot;black&quot;,0.2)) #With beta logarithm set.seed(2971) N &lt;- 100 # 100 lines a &lt;- rnorm(N, 178, 20) b &lt;- rlnorm(N, 0, 1) #RLNORM for logarithm plot(NULL ,xlim=range(d2$weight), ylim=c(-100,400),xlab=&quot;weight&quot;, ylab=&quot;height&quot;) abline(h=0, lty=2) abline(h=272, lty=1, lwd=0.5) mtext(&quot;b ~ dnorm(0,10)&quot;) xbar &lt;- mean(d2$weight) for (i in 1:N ) curve(a[i] + b[i]*(x - xbar),from=min(d2$weight), to=max(d2$weight),add=TRUE,col=col.alpha(&quot;black&quot;,0.2)) (#fig:4.41)Left = Initial model, Right = logarithm of beta Now we see that the joint prior for \\(\\alpha\\) and \\(\\beta\\) are realistict. Joint prior = The predicted height given the wiehgt, which rely on alpha and beta priors that we have made. Then, what is the correct prior? It is a fallacy that there is one unique value that is optimal for the prior. Thus one must reason for the selection of the prior and perhaps try different priors to see what it suggest. Essentially the prior is just information that you give the model and can e.g., work as a constrain, as we see in the example above where the priors initially returned extreme values, which we need to tamper down. Then one could say that we should compare the predictions with the actual sample and then optimize against this. Although one must be weary, as this will just yield to fitting against the sample, and is likely not to be the correct model. Although it boiles down to the purpose of the model. 4.4.2 Finding the posterior distribution We see that we have the following model specification: Notice that = is exchanged with &lt;-, that is by convention and must be used when specifying the model in R. data(&quot;Howell1&quot;) d &lt;- Howell1 d2 &lt;- d[d$age &gt;= 18,] xbar &lt;- mean(d2$weight) #Fit the model m4.3 &lt;- quap( alist( height ~ dnorm(mu,sigma) ,mu &lt;- a + b * (weight - xbar) #Notice each weight is subtracted by the mean, the closer to the mean the smaller effect of b ,a ~ dnorm(178,20) ,b ~ dlnorm(0,1) ,sigma ~ dunif(0,50) ) ,data = d2 ) 4.4.3 Interpreting the posterior distribution One can interpret the posterior distribution in two ways: By assessing tables of information Plotting the posterior distributions It is often easiest to deduct conclusions based on the plots. The following make examples of both the tables and the plots. 4.4.3.1 Tables fo marginal distributions Marginal posterior distribution of the parameters: precis(m4.3 ,prob = 0.89 #PI for 89% percent, also default ) x 154.6013671 0.9032807 5.0718809 x 0.2703077 0.0419236 0.1911548 x 154.1693633 0.8362787 4.7663786 x 155.0333710 0.9702828 5.3773831 For example we see that the mean height increase by a factor of 0.9 if one person is 1 kg heavier, hence the heavier, the taller. The 5.5% and 94.5% indicate that 89% percent of the time one gets between 84cm and 97 cm taller if one is 1kg heavier. One can also assess the covariances: round(vcov(m4.3),3) ## a b sigma ## a 0.073 0.000 0.000 ## b 0.000 0.002 0.000 ## sigma 0.000 0.000 0.037 And we see that there is very little covariance among the parameters. This is a visual representation of the same. rethinking::pairs(m4.3) 4.4.3.2 Plotting posterior inference against the data First the raw data is plotted: par(mfrow = c(1,1)) plot(height ~ weight,data = d2,col = rangi2) post &lt;- extract.samples(m4.3) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) curve(expr = a_map + b_map * (x - xbar),add = TRUE) #Ability to plot a function We see that the function we defined based on the posterior seem reasonable, although there are many plausible lines. Hence we are going to look into dealing with uncertainty. 4.4.3.3 Adding uncertainty around the mean We are going to make many of the lines to interprete where they end up, hence also reflecting uncertainty. post &lt;- extract.samples(m4.3) #Default = 10.000 post[1:5,] a b sigma 154.5789 0.9376825 5.220756 154.4067 0.8937310 4.752735 154.4622 0.9150822 5.341227 154.2649 0.9236067 5.160423 155.1258 0.9495934 5.108891 We see that these are basically all different functions that can be plotted to show the uncertainty. N &lt;- 150 #No. of samples dN &lt;- d2[1:N,] #subsetting #Approximating the the mN &lt;- quap( alist( height ~ dnorm( mu , sigma ) ,mu &lt;- a + b*( weight - mean(weight) ) ,a ~ dnorm( 178 , 20 ) ,b ~ dlnorm( 0 , 1 ) ,sigma ~ dunif( 0 , 50 ) ) ,data=dN ) The following will show one example where we loop of the data data fit the line. n &lt;- 20 #No of loops # extract n samples from the posterior distribution post &lt;- extract.samples(mN ,n=n) # display raw data and sample size plot(x = dN$weight ,y = dN$height ,xlim=range(d2$weight),ylim=range(d2$height) ,col=rangi2 ,xlab=&quot;weight&quot;,ylab=&quot;height&quot;) mtext(concat(&quot;N = &quot;,N,&quot;, Iterations = &quot;,n)) # plot the lines, with transparency for ( i in 1:n) #Draw a and b values from the subset of the data curve(post$a[i] + post$b[i]*(x-mean(dN$weight)) ,col=col.alpha(&quot;black&quot;,0.3) ,add=TRUE) One will see that the more observations we include, the more certain will the model become. One will often experience that we are more confident around the mean and less in the ends of the x-range. In other words, the less data we introduce, the less we rely on the priors that we specified before seeing the data. that is because the prior is updated according to the data. Notice that confidence is not equal to correctness that is due to the data that we have may not reflect a truer perspective than the priors. 4.4.3.4 Plotting regression intervlas and contours Now lets start an example where the weight is fixed to 50 kg. We will see that a person with 50kg is not fixed to one height, but some will have greater certainty. We see that we get 10.000 samples, thus 10.000 priors for both a and b and thus we can simulate the expected height for such a given person. post &lt;- extract.samples(m4.3,n = 10000) mu_at_50 &lt;- post$a + post$b * (50-xbar) dens(x = mu_at_50,col = rangi2,lwd = 2,xlab = &quot;mu|weight=50&quot;) Now what we want to do is the same, but for all weights. For this one can use the link function. mu &lt;- link(m4.3,n = 1000) #1000 is also default str(mu) ## num [1:1000, 1:352] 158 157 157 157 157 ... We see that 352 roes in the data hence we get a matrix 352 columns (one for each individual) with 1.000 rows. # define sequence of weights to compute predictions for these values will be on the horizontal axis weight.seq &lt;- seq(from=25, to=70, by=1) # use link to compute mu for each sample from posterior and for each weight in weight.seq mu &lt;- link( m4.3 , data=data.frame(weight=weight.seq) ) str(mu) ## num [1:1000, 1:46] 136 136 136 137 137 ... Now we see that since we fed 46 values for the weight, we get 46 columns instead of one pr invidual. plot(height ~ weight,data = d2,type=&quot;n&quot;) #use type=&quot;n&quot; to hide raw data # loop over samples and plot each mu value for (i in 1:100) points(weight.seq, mu[i,] ,pch=16, col=col.alpha(rangi2,0.1)) Finally we will summarize the distribution for each weight value. # summarize the distribution of mu mu.mean &lt;- apply(mu, 2, mean) #46 values, one for each weight mu.PI &lt;- apply(mu, 2, PI, prob=0.89) #The 89% PI for each mean mu.mean = the average height (mu) for each weight value. And the PI is just accompanied with this as well. Now we plot the means and the PI ontop of the data. #Raw data plot plot(height ~ weight, data = d2, col = col.alpha(rangi2,0.5)) #Ploatting MAP line (mean mu for each weight) lines(x = weight.seq,y = mu.mean) #Plotting PI intervals for each weight shade(mu.PI,weight.seq) NOTICE THAT THIS IS PREDICTION OF AVERAGE HEIGHTS, IN THE FOLLOWING WE MAKE INTERVALS FOR ACTUAL HEIGHTS 4.4.3.5 Prediction intevals Now we are going to predict actual heights. Also notice that up until now, we have not had to simulate sigma, as it is only relevant for predicting actual heights, and as we saw that mean heights are dependent (deterministic) on alpha and beta prior, but not on sigma. This comes down to, what is sigma, it is an expression of the scatter of the points. The following is an example of actual predictions and generating a band of percentile interval. #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m4.3, data=list(weight=weight.seq)) #n = 1000 str(sim.height) ## num [1:1000, 1:46] 136 131 140 139 136 ... This contains simulated heights and not distributions as we saw previously. Now we can generate the PI that we are going to plot. height.PI &lt;- apply(sim.height,2,PI,prob = 0.89) height.HPDI67 &lt;- apply(sim.height,2,HPDI,prob = 0.67) height.HPDI89 &lt;- apply(sim.height,2,HPDI,prob = 0.89) height.HPDI97 &lt;- apply(sim.height,2,HPDI,prob = 0.97) Lastly we need to plot the predictions and the percentile interval. #Plotting the data points plot(height ~ weight,data = d2,col = col.alpha(rangi2,0.5)) #Draw MAP line lines(weight.seq,y = mu.mean) #Draw HPDI region for simulated heights, notice I added two additional regions shade(height.HPDI67,weight.seq) shade(height.HPDI89,weight.seq) shade(height.HPDI97,weight.seq) Now we see that the region is far wider. 4.5 Curves form lines This is technically the same, but we add complexity in the form of more predictors. We approach this with polynomial regression and splines. 4.5.1 Polynomial regression This is basically using the same variable, but transforming it into second or third order polynomials. When doing polynomials you are at risk of generating very large numbers, hence one should standardize the variable to avoid this. This also means that interpreting the effects from each parameter is more difficult. Now we can specify the model with the following: Notice that this is for a cubic polynomial regression. One can just disregard $/beta_3$ in a quadratic polynomial. The following will plot the example of: #Standardize weights d$weight_s &lt;- ( d$weight - mean(d$weight) )/sd(d$weight) #Add vector of squared values, for the polynomial variable d$weight_s2 &lt;- d$weight_s^2 d$weight_s3 &lt;- d$weight_s^3 #For the cubic model #Specfify models #Linear model m4.4 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) ,data = d ) #Quadratic model (4.65) m4.5 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s + b2*weight_s2 , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , b2 ~ dnorm( 0 , 1 ) , sigma ~ dunif( 0 , 50 ) ) ,data = d ) #Cubic model (4.69) d$weight_s3 &lt;- d$weight_s^3 m4.6 &lt;- quap( alist( height ~ dnorm( mu , sigma ) , mu &lt;- a + b1*weight_s + b2*weight_s2 + b3*weight_s3 , a ~ dnorm( 178 , 20 ) , b1 ~ dlnorm( 0 , 1 ) , b2 ~ dnorm( 0 , 10 ) , b3 ~ dnorm( 0 , 10 ) , sigma ~ dunif( 0 , 50 ) ) ,data=d ) #Plotting par(mfrow = c(1,3)) #Linear weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq ) #adds degree of poly mu &lt;- link( m4.4 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.4 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) #Quadratic weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq , weight_s2=weight.seq^2 ) #adds degree of poly mu &lt;- link( m4.5 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.5 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) #cubic weight.seq &lt;- seq( from=-2.2 , to=2 , length.out=30 ) pred_dat &lt;- list( weight_s=weight.seq , weight_s2=weight.seq^2, weight_s3=weight.seq^3 ) #adds degree of poly mu &lt;- link( m4.6 , data=pred_dat ) #need to change model mu.mean &lt;- apply( mu , 2 , mean ) mu.PI &lt;- apply( mu , 2 , PI , prob=0.89 ) sim.height &lt;- sim( m4.6 , data=pred_dat ) #need to change model height.PI &lt;- apply( sim.height , 2 , PI , prob=0.89 ) plot( height ~ weight_s , d , col=col.alpha(rangi2,0.5),pch = 20) lines( weight.seq , mu.mean ) shade( mu.PI , weight.seq ) shade( height.PI , weight.seq ) Figure 4.1: Comparison between a linear, quadratic and cubic model. Notice that the same variable is used, just with different degree of polynomials. Table output for the polynomial precis(m4.5) x 146.057160 21.733407 -7.803209 5.774538 x 0.3689809 0.2888934 0.2741871 0.1764700 x 145.467457 21.271700 -8.241412 5.492504 x 146.646862 22.195114 -7.365005 6.056571 Now we see that b2 for instance is more complicated, as it is squared values that the parameter is multiplied with. Notice that a is still the intercept with y. Also since the same variable is in the model more than once, one cannot interpret one without the other. Arguments for why polynomials are bad: If you have regions with no data, the model can do all kinds of weird things Makes absurd predictions outside range of data It is likely to get some obscure shapes The model is difficult to interpret as you cannot interpret one parameter without the other,as the polynomial variable is tied to the non polynomial variable. Often having a polynomial makes no sense in real life, even though you may find a relationship In a quadratic model, the fitted lines will always go up and then down, hence it needs to fit the data in that way. The same applies to cubic functions. Actually not very flexible and cannot have monotone shape The spline that we see in the following is the answer to many of these difficulties. 4.5.2 Splines This is an alternative way of adding curvature. We are going to use basis splines, i.e., B-splines. This is also called P-splines, this is for penalty-splines, as the priors adds a penalty. The code is not exemplified yet. Do this. For now, see ?? It is basically just splines as we have seen earlier in ML. It is often a much better model and polynomial regression. During the lecture he shows a really nice representation of which of the basis functions that are active. 4.6 Exercises 4.6.1 4M1 For the model definition below, simulate observed y values from the prior (not the posterior). \\[y_i \\sim Normal(\\mu,\\sigma)\\] \\[\\mu \\sim Normal(0,10)\\] \\[\\sigma \\sim Exponential(1)\\] set.seed(1337) N &lt;- 10000 mu &lt;- rnorm(n = N,0,10) #notice we cannot take the mean of the data, as it is a prior!!! sigma &lt;- rexp(10000,1) prior_h &lt;- rnorm(N,mu,sigma) dens(prior_h) Note, in this example dnorm and rnorm is exactly the same Plotting the priors What we see here, is that the exponential prior keep more information than the {curve(dexp(x,1),from = 0,to = 7 ,xlab = &quot;sigma&quot;,ylab = &quot;Density&quot; ,ylim = c(0,1)) curve(dunif(x,0,1),add = T,col = &quot;red&quot;) #dunif = cut at 1 where all the mass is below. mtext(&quot;Priors&quot;)} 4.6.2 4M8 In the chapter, we used 15 knots with the cherry blossom spline. 1. Increase the number of knots and observe what happens to the resulting spline. 1. Then adjust also the width of the prior on the weights 1. change the standard deviation of the prior and watch what happens. 1. What do you think the combination of knot number and the prior on the weights controls? #Creating a function for the exercise, so you dont have to run it all again to make modifications load_execute &lt;- function( N_knots ,model_spec = alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(6,10) ,w ~ dnorm(0,1) ,sigma ~ dexp(1) ) ) { library(rethinking) #Load data data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp #precis(d) #For inspections #Defining spline specifications num_knots &lt;- N_knots knot_list &lt;- quantile(x = d$year #Data ,probs = seq(0,1,length.out=num_knots) #probabilities ) knot_list &lt;- knot_list[-c(1,num_knots)] #Remove knot at 0 and 1 #Defining the splines library(splines) B &lt;- bs( x = d$year ,knots = knot_list ,degree = 3 #cubic splines ,intercept = TRUE ) #Define model m &lt;&lt;- quap( model_spec ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) #precis(m,depth = 2) } From this we can see all the parameters at the knots, although this does not provide much information anymore before plotting. We see the effect of each spline. Now we can plot the actual fitted lines and the compatibility interval. par(mfrow = c(3,1)) data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp for (i in seq(10,30,10)){ load_execute(N_knots = i) mu &lt;- link(m) mu_PI &lt;- apply(X = mu,MARGIN = 2,FUN = PI,0.97) #0.97 is input to PI #Adding raw data plot(d$year,d$temp ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;Year&quot; ,ylab = &quot;Temp&quot; ,main = paste(&quot;No. of knots:&quot;,i,sep = &quot; &quot;) ) #Adding shade for the intervals shade(object = mu_PI ,lim = d$year ,col = col.alpha(&quot;black&quot;,0.5) ) } We are able to see that the larger amount of knots the more wigglyness do we allow for the model to have. Naturally there are no rule of thumb for an optimal number of knots, that is sensitive to the case. 4.6.3 4H1 The weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. That is, fill in the table below, using model-based predictions. First we define the model to get the parameters, such as mean for beta. Recall that a = the intercept with y. library(rethinking) data(Howell1) d &lt;- Howell1 d2 &lt;- d[d$age&gt;=18,] xbar &lt;- mean(d2$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar) + sigma, a ~ dnorm( 178 , 20 ), b ~ dlnorm( 0 , 1 ), sigma ~ dunif( 0 , 50 ) ), data=d2 ) precis(m) #similar to summary() x 149.5330495 0.9032823 5.0692632 x 0.3305605 0.0419020 0.1908996 x 149.0047500 0.8363148 4.7641687 x 150.0613489 0.9702498 5.3743576 From this we are able to extract samples: post &lt;- extract.samples(m) str(post) ## &#39;data.frame&#39;: 10000 obs. of 3 variables: ## $ a : num 150 149 150 149 149 ... ## $ b : num 0.907 0.951 0.938 0.903 0.787 ... ## $ sigma: num 4.84 4.93 4.61 5.28 5.38 ... ## - attr(*, &quot;source&quot;)= chr &quot;quap posterior: 10000 samples from m&quot; Function to get n samples based on the input weight, it returns the mean y and the compatability interval f &lt;- function( weight ) { y &lt;- rnorm(n = 10000 , post$a + post$b * (weight - xbar) , post$sigma ) return( c( mean(y) , PI(y,prob=0.89) ) ) } weight_list &lt;- c(46.95,43.72,64.78,32.59,54.63) result &lt;- sapply( weight_list , f ) result ## [,1] [,2] [,3] [,4] [,5] ## 151.2486 148.4566 167.3111 138.3709 158.2212 ## 5% 143.0175 140.4629 159.2750 130.0935 149.9860 ## 94% 159.5345 156.5157 175.4989 146.4820 166.4864 setNames(object = data.frame(cbind(weight_list,t(result))) ,nm = c(&quot;Weight&quot;,&quot;Height&quot;,&quot;5%&quot;,&quot;94%&quot;)) Weight Height 5% 94% 46.95 151.2486 143.0175 159.5345 43.72 148.4566 140.4629 156.5157 64.78 167.3111 159.2750 175.4989 32.59 138.3709 130.0935 146.4820 54.63 158.2212 149.9860 166.4864 We see that the heights has now been estimated. Why we use the function: We need to samples to estimate the percentage intervals, thus sampling is required. One could merely also say \\(\\alpha + \\beta * (weight - \\bar{weight})\\), thus find the point estimate given the different weights. Although that will not reflect the compatibility intervals. Finding point estimates. weight = weight_list[2] m@coef[&#39;a&#39;] + m@coef[&#39;b&#39;] * (weight - xbar) ## a ## 148.3854 4.6.4 4H2 Select out all the rows in the Howell1data with ages below 18 years of age. If you do it right, you should end up with a new data frame with 192 rows in it. data(Howell1) d &lt;- Howell1[Howell1$age&lt;18,] dim(d) ## [1] 192 4 Fit a linear regression to these data, using quap. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets? xbar &lt;- mean(d$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar), a ~ dnorm( 178 , 20 ), b ~ dnorm( 0 , 10 ), sigma ~ dunif( 0 , 50 ) ), data = d ) summary(m) x 108.38349 2.71997 8.43732 x 0.6086648 0.0682926 0.4305881 x 107.410730 2.610826 7.749157 x 109.356258 2.829115 9.125483 We see see that for 1 increase in weight the mean is expected to increase by 2.72. While the 89% combatibiity intervals being 2.61 and 2.83, hence there is a positive effect. Thus by 10 kg, we expect someone to grow by a magnitude of 10. Plot the raw data, with height on the vertical axis and weight on the horizontal axis. Superimpose the MAP regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights. plot(x = d$weight,y = d$height,pch = 20,main = &quot;Regression, age &lt; 18&quot; ,xlab = &quot;Weight&quot;,ylab = &quot;Height&quot;) grid() #Adding the model predictions post &lt;- extract.samples(m) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) #Ability to plot a function curve(expr = a_map + b_map * (x - xbar) ,add = TRUE #Add to existing plot ,col = &quot;darkblue&quot; ,lty = 2 ) #Weight sequence for getting intervals weight.seq = seq(floor(min(d$weight)),ceiling(max(d$weight)),by = 1) #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m, data=list(weight=weight.seq)) #n = 1000 - These are predictions str(sim.height) ## num [1:1000, 1:42] 69.6 68.4 72.7 64.8 74.8 ... #mu mu &lt;- link(fit = m,data=list(weight=weight.seq)) mu.mean &lt;- apply(mu,MARGIN = 2,FUN = mean) #2 = columns mu.ci &lt;- apply(mu,MARGIN = 2,FUN = PI,prob = 0.89) #PI + plot height.PI89 &lt;- apply(sim.height,2,PI,prob = 0.89) shade(mu.ci,weight.seq) shade(height.PI89,weight.seq) We see that the linear model does not fit the data well enough in the outer regions. What aspects of the model fit concern you? Describe the kinds of assumptions you would change, if any, to improve the model. You don’t have to write any new code. Just explain what the model appears to be doing a bad job of, and what you hypothesize would be a better model. One could try: Polynomial - as it appears as the relationship is not linear. Splines - same reason as above 4.6.5 4H3 Suppose a colleague of yours, who works on allometry, glances at the practice problems just above. Your colleague exclaims, “That’s silly. Everyone knows that it’s only the logarithm of body weight that scales with height!” Let’s take your colleague’s advice and see what happens. Model the relationship between height (cm) and the natural logarithm of weight (log-kg). Use the entire Howell1data frame, all 544 rows, adults and non-adults. Can you interpret the resulting estimates? #Load data data(Howell1) d &lt;- Howell1 paste0(&quot;data dimensions: &quot;,dim(d)[1]) #Define and approximate the model xbar &lt;- mean(d$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (weight - xbar), a ~ dnorm( 178 , 20 ), b ~ dlnorm( 0 , 1 ), sigma ~ dunif( 0 , 50 ) ), data = d ) precis(m) ## [1] &quot;data dimensions: 544&quot; x 138.279528 1.763640 9.345925 x 0.4006240 0.0272501 0.2833422 x 137.639253 1.720089 8.893090 x 138.919802 1.807191 9.798761 Same principle as earlier, we see that one grows with 1.76 for each increase in kg of weight. While the 89% plausible outcomes are 1.72 and 1.81. Begin with this plot: plot( height ~ weight , data=Howell1 ). Then use samples from the quadratic approximate posterior of the model in (a) to superimpose on the plot: (1) the predicted mean height as a function of weight, (2) the 97% interval for the mean, and (3) the 97% interval for predicted heights. plot(x = d$weight,y = d$height,pch = 20,main = &quot;Regression, all observations&quot; ,xlab = &quot;Weight&quot;,ylab = &quot;Height&quot;) grid() #Adding the model predictions post &lt;- extract.samples(m) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) #Ability to plot a function curve(expr = a_map + b_map * (x - xbar) ,add = TRUE #Add to existing plot ,col = &quot;darkblue&quot; ,lty = 2 ,lwd = 1.5 ) #Weight sequence for getting intervals weight.seq = seq(floor(min(d$weight)),ceiling(max(d$weight)),by = 1) # mean intervals mu &lt;- link(m,data = list(weight = weight.seq)) mu.PI &lt;- apply(X = mu,MARGIN = 2,FUN = PI ,prob=0.97 #input for the function ) shade(object = mu.PI,lim = weight.seq,col = col.alpha(&quot;red&quot;,0.15)) #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m, data=list(weight=weight.seq)) #n = 1000 str(sim.height) #PI + plot intervals for the predictions height.PI97 &lt;- apply(sim.height,2,PI,prob = 0.97) shade(height.PI97,weight.seq,col = col.alpha(&quot;orange&quot;,0.15)) #Adding legend legend(&quot;topleft&quot; ,legend = c(&quot;Linear pred&quot;,&quot;mean PI97&quot;,&quot;pred PI97&quot;) ,col = c(&quot;darkblue&quot;,col = col.alpha(&quot;red&quot;,0.15),col.alpha(&quot;orange&quot;,0.15)) ,lty = 2 ,lwd = 5 ,cex = 0.8 ) ## num [1:1000, 1:60] 80.2 91.6 73.3 85.4 100.6 ... We see that the expected mean is very close to the linear prediction, while the actual predictions are expected to lie in a wider space. 4.6.5.1 The same but using log of weight Now we take log of weight instead. Notice that the log scales is just the opposite of the #Load data data(Howell1) d &lt;- Howell1 paste0(&quot;data dimensions: &quot;,dim(d)[1]) #Define and approximate the model xbar &lt;- mean(d$weight) m &lt;- quap( alist( height ~ dnorm( mu , sigma ), mu &lt;- a + b * (log(weight) - log(xbar)), a ~ dnorm( 178 , 20 ), b ~ dlnorm( 0 , 1 ), sigma ~ dunif( 0 , 50 ) ), data = d ) precis(m) ## [1] &quot;data dimensions: 544&quot; x 144.401902 47.072730 5.134715 x 0.2257108 0.3826302 0.1556694 x 144.041173 46.461213 4.885926 x 144.762632 47.684247 5.383505 plot(x = d$weight,y = d$height,pch = 20,main = &quot;Regression, all observations&quot; ,xlab = &quot;Weight&quot;,ylab = &quot;Height&quot;) grid() #Adding the model predictions post &lt;- extract.samples(m) a_map &lt;- mean(post$a) b_map &lt;- mean(post$b) #Ability to plot a function curve(expr = a_map + b_map * (log(x) - log(xbar)) #ADDED LOG TO THE X VALUES AND MEAN X VALUES ,add = TRUE #Add to existing plot ,col = &quot;darkblue&quot; ,lty = 2 ,lwd = 1.5 ) #Weight sequence for getting intervals weight.seq = seq(floor(min(d$weight)),ceiling(max(d$weight)),by = 1) # mean intervals mu &lt;- link(m,data = list(weight = weight.seq)) mu.PI &lt;- apply(X = mu,MARGIN = 2,FUN = PI ,prob=0.97 #input for the function ) shade(object = mu.PI,lim = weight.seq,col = col.alpha(&quot;red&quot;,0.15)) #Simulate height (Simulates posterior observations for map and map2stan model fits.) sim.height &lt;- sim(m, data=list(weight=weight.seq)) #n = 1000 str(sim.height) #PI + plot intervals for the predictions height.PI97 &lt;- apply(sim.height,2,PI,prob = 0.97) shade(height.PI97,weight.seq,col = col.alpha(&quot;orange&quot;,0.15)) #Adding legend legend(&quot;topleft&quot; ,legend = c(&quot;non-linear pred&quot;,&quot;mean PI97&quot;,&quot;pred PI97&quot;) ,col = c(&quot;darkblue&quot;,col = col.alpha(&quot;red&quot;,0.15),col.alpha(&quot;orange&quot;,0.15)) ,lty = 2 ,lwd = 5 ,cex = 0.8 ) ## num [1:1000, 1:60] 49.5 43.5 42.2 43 47.3 ... The conclusion is that the data clearly need to be mapped with a non linear funciton. 4.6.6 4H6 Simulate the prior predictive distribution for the cherry blossom spline in the chapter. Adjust the prior on the weights and observe what happens. What do you think the prior on the weights is doing? #Load data data(cherry_blossoms) d &lt;- cherry_blossoms d &lt;- d[complete.cases(d$temp),] # complete cases on temp library(splines) #Defining spline specifications num_knots &lt;- 15 knot_list &lt;- quantile(x = d$year #Data ,probs = seq(0,1,length.out=num_knots) #probabilities ) knot_list &lt;- knot_list[-c(1,num_knots)] #Remove knot at 0 and 1 #Defining the splines - notice that m is altered in later code to represent other examples B &lt;- bs( x = d$year ,knots = knot_list ,degree = 3 #cubic splines ,intercept = TRUE ) m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,10) ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) ### PLOTTING! par(mar = c(4, 4, .1, .1),mfrow = c(3,1)) #adjust print window ### Top print p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,10)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) ### Middle print m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,5) #This is altered ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,5)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) ### Bottum print m &lt;- quap( alist( T ~ dnorm(mu,sigma) ,mu &lt;- a + B %*% w ,a ~ dnorm(100,10) ,w ~ dnorm(0,1) #This is altered ,sigma ~ dexp(1) ) ,data = list(T = d$temp,B = B) ,start = list(w = rep(0,ncol(B))) ) p &lt;- extract.prior(m) mu &lt;- link(m,post = p) plot(x = d$year,y = d$doy ,col = col.alpha(rangi2,0.3) ,pch = 16 ,xlab = &quot;year&quot;,ylab = &quot;day in year&quot; ,ylim = c(60,140) ,sub = &quot;w ~ dnorm(0,1)&quot; ) for ( i in 1:20 ) lines( d$year , mu[i,] , lwd=1) We see that the smaller we define the width of w, the less wiggly will the lines be. Hence we add more penalty to the movements, i.e., so we generalize more and fit less to the data. "],["lecture-notes-wrap-up-chapters-geocentric-models.html", "5 Lecture notes - wrap up chapters geocentric models", " 5 Lecture notes - wrap up chapters geocentric models Difference between binomial distribution and gaussian distribution, is that you have respectively binomial and continous data. library(dagitty) "],["chapter-5-the-many-variables-the-spurious-waffles.html", "6 Chapter 5 - The Many Variables &amp; The Spurious Waffles 6.1 Spurious Associations 6.2 Masked Relationship 6.3 Categorical Variables 6.4 Lecture notes - not integrated in the notes", " 6 Chapter 5 - The Many Variables &amp; The Spurious Waffles What is the difference between causal and spurious correlation and how do you define it. As you may see that two variables may be correlated, although it does not mean that one affect the other. Therefore we are going to use multiple regression, to account for effects. Even though it can overcome multiple regression, it can also create a spurious relationship. We are going to use DAGs as a backdoor criterion, to find the causes. 6.1 Spurious Associations This section elaborates on the difference between causal and spurious relationships. This is an example # load data and copy library(rethinking) data(WaffleDivorce) d &lt;- WaffleDivorce # standardize variables d$A &lt;- scale( d$MedianAgeMarriage ) d$D &lt;- scale( d$Divorce ) d$M &lt;- scale( d$Marriage ) m5.1 &lt;- quap( alist( D ~ dnorm( mu , sigma ) , mu &lt;- a + bA * A , a ~ dnorm( 0 , 0.2 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) ,data = d) The following shows plausibile regression lines given the priors. par(mfrow = c(1,1)) set.seed(10) prior &lt;- extract.prior( m5.1 ) mu &lt;- link( m5.1 , post=prior , data=list( A=c(-2,2) ) ) plot( NULL , xlim=c(-2,2) , ylim=c(-2,2),main = &quot;Simulateting priors&quot;) for ( i in 1:50 ) lines( c(-2,2) , mu[i,] , col=col.alpha(&quot;black&quot;,0.4) ) Now we can make the posterior prediction. # compute percentile interval of mean A_seq &lt;- seq( from=-3 , to=3.2 , length.out=30 ) mu &lt;- link( m5.1 , data=list(A=A_seq) ) mu.mean &lt;- apply( mu ,MARGIN = 2,FUN = mean ) mu.PI &lt;- apply( mu ,MARGIN = 2 ,FUN = PI ) # plot it all plot( D ~ A , data=d , col=rangi2,main = &quot;Posterior prediction&quot;) lines( A_seq , mu.mean , lwd=2 ) shade( mu.PI , A_seq ) This follows up by expressing that one must be cautious about what variables that are included and if two or more variables correlate, is it merely because of a shared causal effect that the model does not account for? For this DAGs (directed acyclic graphs) can be used. In the following DAG we see that in the left, A has direct influence on D and M, while also an indirect relationship flowing through M. On the right A influence D and M while M and D are independent but has the same parent. Thus a change in A will lead to a change in M and D, thus if you regress the two of these on each other, it is likely that you’ll find a relationship, although this is just spurious as they just appear to follow the same trends, but it cannot be said that one affects the other. The purpose of the DAG is to capture the causality. Defining DAGS DMA_dag2 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M }&#39;) impliedConditionalIndependencies( DMA_dag2 ) ## D _||_ M | A This (D _||_ M | A) means that D is independent on M conditional on A. DMA_dag1 &lt;- dagitty(&#39;dag{ D &lt;- A -&gt; M -&gt; D }&#39;) impliedConditionalIndependencies( DMA_dag1 ) This has no conditional independence, hence there will be no output. 6.1.1 Defining a multiple regression Lets say we we want to measure divorce rate. Where we have marriage rate and median age at marriage. Then you firs specify the model for the target variable, then to define what the mean consists of, you specify the regression model. We see that alpha = the intercept with y, then \\(\\beta_M\\) is the coefficient for the marriage rate and \\(\\beta_A\\) is the coefficient for the median age at marriage. Lastly we have the standard deviation \\(\\sigma\\), which is the standard deviation from the mean, we need this to make predictions of actual observations and not merely of the mean. 6.1.2 Approximating the posterior m5.3 &lt;- quap( alist( D ~ dnorm( mu , sigma ) , #The likelihood mu &lt;- a + bM*M + bA*A , #Declarative a ~ dnorm( 0 , 0.2 ) , bM ~ dnorm( 0 , 0.5 ) , bA ~ dnorm( 0 , 0.5 ) , sigma ~ dexp( 1 ) ) ,data = d ) precis( m5.3 ) x -0.0000110 -0.0653767 -0.6135168 0.7851600 x 0.0970800 0.1507801 0.1509906 0.0778538 x -0.1551636 -0.3063524 -0.8548289 0.6607347 x 0.1551416 0.1755990 -0.3722047 0.9095854 We see that for marriage rate, it can be bothh positive and negative. While age when entering the marriage is negative, hence the higher the age the less probability of divorcing. We saw earlier that age and marriage rate may be correlated, although we see two different effects of these, when the linear models are run separately as above. This can also be visualized in the following. Here we see that M5.2 implies that there is a relationship between marriage rate and divorce, although when we run the multiple regression, we see that this variable points both positive and negative direction (m5.3). Hence marriage rate may be spurious. Thus we see that once we know the meadian age of marriage, there is no or little value in marriage rate, although if we dont know median age of mariage, there is still information to gain from the marriage rate. To influence the rate of divorce, one could legislate the age at marriage, but the marriage rate will not have effect. Thus it is important to consider whether we are doing inference or prediction. 6.1.3 The flow Define the model, the equations Define the priors Visualize the priors Draw samples from the posterior Making posterior predictions Predictor residual plots. From the residuals plots we are able to see if one variable still contains information about the target value, we expect the residuals of on variable to be horizontal, if not, it means that there is still some information to gain from the variable. Counterfactual plots, that is using standardized values. Posterior prediction plots: Now we look at predictions for the actial observed cases. Never analyze the residuals. Reason: we know the that the regression is internally fitting to the model, hence the rest is just the leftovers, thus they should not correlate with the target variable. 6.1.3.1 Posterior Prediction Plots The benefit of checking the model against the predictions, is to see how well the model performs. Now we can produce a simple posterior predictive check # call link without specifying new data # so it uses original data mu &lt;- link( m5.3 ) # summarize samples across cases mu_mean &lt;- apply( mu , 2 , mean ) mu_PI &lt;- apply( mu , 2 , PI ) # simulate observations # again no new data, so uses original data D_sim &lt;- sim( m5.3 , n=1e4 ) D_PI &lt;- apply( D_sim , 2 , PI ) Now we plot the model against the actual observations. plot( mu_mean ~ d$D , col=rangi2 , ylim=range(mu_PI) , xlab=&quot;Observed divorce&quot; , ylab=&quot;Predicted divorce&quot; ) abline( a=0 , b=1 , lty=2 ) #The perfect prediction for ( i in 1:nrow(d) ) lines( rep(d$D[i],2) , mu_PI[,i] , col=rangi2 ) 6.2 Masked Relationship Sometimes association between outcome of one variable is masked by another variable, hence a mediate effect. Thus, we need both variables, to describe the relationship. I think that the intuition is that in a bivariate model, one predictor may not be sufficient dimensionality to predict the variance in Y. Although if you have multiple variables, then one variable can account for what the other could not. Notice that this comes at a cost of more noise and complexity in the model, also we are able to create what is called confounding colliders. 6.3 Categorical Variables See example in the slides. Basiacally we can encode categoricals as: Dummies Indexes He argued, that indexes is the best way of encoding the data. We see that dummies create a coefficient for the given dummy, while using the index merely creates an \\(\\alpha\\) value for each category in the index. Also it makes the model simpler to write out, as you just have to specify the parameter (alpha for instance) for each index, e.g., these are just snippets with relevant lines: \\[ \\mu_i = \\alpha + \\beta_mm_i \\] \\[\\alpha_j \\sim Normal(0,10)\\] \\[\\beta_m \\sim Normal(0,50)\\] Which merely translates to: \\[\\mu_i = \\alpha_{SEX[i]}\\] \\[\\alpha_j \\sim Normal(178,20), for j = 1...2\\] This we see that the mean is alpha given the gender, where this is just given by the normal distribution for the given gender. 6.4 Lecture notes - not integrated in the notes Never peak at the data before you set the priors.This "],["chapter-6-the-haunted-dag-the-causal-terror.html", "7 Chapter 6 - The Haunted DAG &amp; The Causal Terror 7.1 Multicollinearity 7.2 Post Treatment Bias 7.3 Collider Bias 7.4 Confronting Confounding 7.5 Exercises 7.6 Lecture notes - not integrated in the text", " 7 Chapter 6 - The Haunted DAG &amp; The Causal Terror It is called the haunting DAG, as we see that checking different variables, may create colliders, where doing inference, the actual state will be skewed, e.g., see lecture notes @ref(lecture-notes—not-integrated-in-the-text). The following sections elaborates on three hazards: Multicollinearity Post Treatment Bias Collider Bias 7.1 Multicollinearity Multicollinearity is in its core not a bad effect in your model, although you want to avoid it when doing inference. In the litterature, there is an example of predicting heights based on the length of each leg, we see that the sum of the coefficient will add to the mean of the leg length in general. Hence overall the model will predict the right results although inference wise it is counter intuitive. In another example we have two highly correlated predictor variables that goes the opposite way of each other. We know in this example that these are in fact explained by the same (unobserved) variable, hence when you know one, you will also know the other. L = lactose, F = fat. D = density. When the density is low, it has high lactose and low fat and the opposite. Thus if we know F, then we know F and L and if we know D, we will know both L and F, hence we want to only include one of the observed variables into the model. In this example we saw from precis that when each observed variable is regressed individually the mean and compatibility intervals are far on each side of 0, although when both are included it will for both variables be centered, more or less, at 0 and have CI on both sides of 0. Notice that just because two variable correlate, we do not by default want to remove them! Conclusion: Just because you have many causal explanatory variables at hand, it does not mean that you should use all of them. 7.2 Post Treatment Bias did not get this 7.3 Collider Bias For this I will refer to the examples in the book about the grandchildren and the parents influence on childrens education McElreath (2020) pp. 180 - 182. In its basics, when we include one variable, it might also indirectly include information on an unobserverd variable that is explaning both the predictor and the target variable. Hence the effect in the predictor P will implicitly also be reflected in the model. I.e., the unobserved effect will be hidden / under the radar, thus we will not be able to distinguish this effect from the actual effect of P. 7.4 Confronting Confounding No matter the size of the DAG, the model will consist of the following types: There are scenarios we want to avoid etc. That I could elaborate on in the notes The approach to analyzing the DAG: List all paths (paths disregard directions) connecting X and Y Classify each path by whether it is open or closed. A path is open unless it contains a collider Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering X. If there are any open backdoor paths, decide which variable(s) to condition on o close it (if possible). Pages 186 - 187 has examples on scenarios to avoid and how to close backdoors. We have the following example where we want to analyze relationship on W to D. library(dagitty) dag_6.2 &lt;- dagitty( &quot;dag { A -&gt; D A -&gt; M -&gt; D A &lt;- S -&gt; M S -&gt; W -&gt; D }&quot; ) # coordinates(dag_6.2) &lt;- list(x = c(S=3,W=3,M=2,A=0,D=0) # ,y = c(S=0,W=3,M=2,A=0,D=3)) drawdag(dag_6.2) There are 3 open backdoors: S -&gt; A S -&gt; M S -&gt; W These all flow from S and affects either W or D. Solution: is to control for S, which will We can control for this with adjustmentSets(), which will show variables that we should make indepent given we control for a specific variable. adjustmentSets( dag_6.2 , exposure=&quot;W&quot; , outcome=&quot;D&quot; ) ## { A, M } ## { S } We see that we can either control for A and M or S. We can also let the model find the conditional independences by saying: impliedConditionalIndependencies(dag_6.2) ## A _||_ W | S ## D _||_ S | A, M, W ## M _||_ W | S Thus we see that A and W are independent given we control for S. 7.5 Exercises 7.5.1 6M1 Modify the DAG on page 186 to include the variable V, an unobserved cause of C and Y: C &lt;- V -&gt; Y. Reanalyze the DAG.How many paths connect X to Y? Which must be closed? Which variables should you condition on now? library(dagitty) library(rethinking) dag &lt;- dagitty( &quot;dag { U [unobserved] V [unobserved] X [exposure] Y [outcome] A -&gt; U A -&gt; C U -&gt; B U -&gt; X C -&gt; B C -&gt; Y X -&gt; Y C &lt;- V -&gt; Y }&quot; ) drawdag(dag) Paths (undirected) from X to Y. List all paths (paths disregard directions) connecting X and Y X,Y X,U,B,C,Y X,U,A,C,Y X,U,B,C,V,Y X,U,A,C,V,Y Classify each path by whether it is open or closed. A path is open unless it contains a collider We see that B and C are colliders, hence we will not want to touch those, as it will open up the flows to the leading effects. Classify each path by whether it is a backdoor path. A backdoor path has an arrow entering X. We see that U affects X, hence there is a backdoor. hence we want to close this relationship If there are any open backdoor paths, decide which variable(s) to condition on o close it (if possible). We see that U is a backdoor path, although it is unobserved hence we cannot adjust the model for that. (But can we adjust for A?) 7.5.2 6M3 Learning to analyze DAGs requires practice. For each of the four DAGs below, state which variables, if any, you must adjust for (conditionon) to estimate the total causal influenceof X on Y. We must condition on Z, as the pipe from A to Y is being closed by conditioning on Z. One could Will this also terminate the flow directly from A to Y? (top right): Should not condition on anything. We see that X to Y is causal and open. We see that Z is a collider, hence we dont want that included. (bottom left): We see that Z is a collider, we dont want to control for that, thus by default A and Y are independent. Hence we just include X in the model. We include X and A. We see that in the 4th example that Z is not a collider, that is because if we list all the paths, there will not be any arrows pointing towards each other. X -&gt; Y X -&gt; Z -&gt; Y X &lt;- A -&gt; Z -&gt; Y Where in the third example we had: X -&gt; Z &lt;- Y (We have a collider) X &lt;- A -&gt; Z &lt;- Y (We have a collider) X -&gt; Y library(dagitty) library(rethinking) dag1 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; X -&gt; Y Z -&gt; Y A -&gt; Y }&quot; ) dag2 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; Y A -&gt; Y X -&gt; Z X -&gt; Y }&quot; ) dag3 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; X -&gt; Y A -&gt; Z X -&gt; Z Y -&gt; Z }&quot; ) dag4 &lt;- dagitty( &quot;dag { X [exposure] Y [outcome] A -&gt; Z -&gt; Y A -&gt; X X -&gt; Y X -&gt; Z }&quot; ) adjustmentSets(x = dag1,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## { Z } adjustmentSets(x = dag2,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## {} adjustmentSets(x = dag3,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## {} adjustmentSets(x = dag4,exposure = &quot;X&quot;,outcome = &quot;Y&quot;) ## { A } 7.5.3 6H library(rethinking) library(dagitty) data(foxes) dfs &lt;- foxes dfs$group &lt;- scale(dfs$group) dfs$avgfood &lt;- scale(dfs$avgfood) dfs$groupsize &lt;- scale(dfs$groupsize) dfs$area &lt;- scale(dfs$area) dfs$group &lt;- scale(dfs$weight) dag &lt;- dagitty( &quot;dag { weigth [outcome] weigth &lt;- groupsize area -&gt; avgfood -&gt; groupsize avgfood -&gt; weigth }&quot; ) drawdag(dag) 7.5.4 6H3 Use a model to infer the total causal influence of area on weight. Would increasing the area available to each fox make it heavier (healthier)? You might want to standardize the variables. Regardless, use prior predictive simulation to show that your model’s prior predictions stay within the possible outcome range. library(rethinking) library(dagitty) dag3 &lt;- dagitty( &quot;dag { weigth [outcome] area [exposure] weigth &lt;- groupsize area -&gt; avgfood -&gt; groupsize avgfood -&gt; weigth }&quot; ) drawdag(dag) Possible paths: area &gt; avgfood &gt; weight area &gt; avgfood &gt; groupsize &gt; weight We just need to include Area. Now for the analysis. library(rethinking) data(foxes) d &lt;- foxes d$W &lt;- standardize(d$weight) d$A &lt;- standardize(d$area) m1 &lt;- quap( alist( W ~ dnorm( mu , sigma ), #Weight is a normal distribution mu &lt;- a + bA*A, #the mean weights are defined by an intercept and a slope for A (area) a ~ dnorm(0,0.2), #The intercept prior bA ~ dnorm(0,0.5), #The slope prior sigma ~ dexp(1) #The variance ), data=d ) precis(m1) x 0.0000008 0.0188331 0.9912657 x 0.0836086 0.0908958 0.0646664 x -0.1336220 -0.1264359 0.8879162 x 0.1336235 0.1641021 1.0946151 We see that the intercept is 0 although with a standard deviation of 0.08. We then see for each unit of area the weight is increasing by 0.02, while the standard deviation is 0.09m thus the weight is expected to both go up and down. Thus statistically we cannot infer anything about the relationship. 7.5.5 6H4 Now infer the causal impact of adding food to a territory. Would this make foxes heavier? Which covariates do you need to adjust for to estimate the total causal influence of food. Now we want to see if food is increasing weight, thus X is avgfood and Y is Weight We see that there is a pipe from avgfood through groupsize, that implies that there is a direct effect from avgfood to weight, but also an indirect effect through groupsize. The task just says to measure the overall effect of food. Hence we want to construct a model only with food. We can do that as there are no backdoors to our explanatory variable. library(rethinking) data(foxes) head(foxes) group avgfood groupsize area weight 1 0.37 2 1.09 5.02 1 0.37 2 1.09 2.84 2 0.53 2 2.05 5.33 2 0.53 2 2.05 6.07 3 0.49 2 2.12 5.85 3 0.49 2 2.12 3.25 d &lt;- foxes d$W &lt;- standardize(d$weight) d$AF &lt;- standardize(d$avgfood) m2 &lt;- quap( alist( W ~ dnorm(mu,sigma), mu &lt;- a + bAF*AF + sigma, a ~ dnorm(0,0.2), bAF ~ dnorm(0,0.5), sigma ~ dexp(1) ) ,data = d ) precis(m2) x -0.7849851 -0.0243044 0.9319074 x 0.0855213 0.0856174 0.0550372 x -0.9216646 -0.1611374 0.8439473 x -0.6483056 0.1125287 1.0198675 We see that there is a great deal of uncertainty as the mean beta coefficient is -0.02, although the standard deviation suggests that it can both go up and down (the weight) Also notice that we did find that area had no effect on weight, and it makes sense that there is cause from area to avgfood, and thus increasing avgfood is similar to increasing area, thus it is expected with no effect. Although it would also be an explanation that a larger area would enable them to roam more around, hence food density would probably have been a better variable or at least interesting. 7.5.6 6H5 Consider your own research questing. Draw a DAG to represent it. What is the testatable implications of your DAG. Are there any variables you could condition on to close all backdoor paths? Are there unobserved variables that yoy have omitted? Would a reasonable colleague imagine additional threats to causal inference that you have ignored? Lets just take the same data as above and add groupsize instead. d &lt;- foxes d$W &lt;- standardize(d$weight) d$G &lt;- standardize(d$groupsize) m3 &lt;- quap( alist( W ~ dnorm(mu,sigma), mu &lt;- a + bG*G + sigma, a ~ dnorm(0,0.2), bG ~ dnorm(0,0.2), sigma ~ dexp(1) ) ,data = d ) precis(m3) x -0.7787903 -0.1359307 0.9212543 x 0.0848406 0.0789734 0.0545018 x -0.9143819 -0.2621456 0.8341500 x -0.6431987 -0.0097159 1.0083587 We see that group size tend to have a negative effect on the weight, hence by one group size unit increase standard deviation we will expect to weigh less. 7.6 Lecture notes - not integrated in the text There are the following types: The fork: We see that Z is a commen cause, thus when we know the outcome of Z, there is no relationship between X and Y. The pipe: We see that Z is a mediator. We see that if we know (Condition) on Z, we remove the relationship between X and Y. We see that this is very similar to the fork. The collider: We see that there is a relationship from X to Z and Y to Z. Although if we control for Z, there is no relationship between X and Z. Thus, if we make the model as a linear regression, we will form a spurious relationship between X and Y, that we want to avoid. The following is an example. They must both be on, for the light to be on, but if switch is on and electicity is off, then the light is also off, hence if we know one of the parents, then we know the state of the other. Collider confounding: That is to identify if there actually is a relationship between two variables that explain the target variable. We see in the following example, we control for marriage, and the two subsets of data will make the age appear to have a negative effect on happiness, while we know that that is not true (the data was simulated.) Summary: For the pipe we dont want to control for Z, as that will terminate the relationship from C to Y. For the Collider we need to take care and not create a spurious relationship. We want to leave the collider alone, hence we do not touch it. "],["chapter-7-model-comparison.html", "8 Chapter 7 - Model comparison 8.1 The problem with parameters 8.2 Entropy and Accuracy 8.3 Golem Taming: Regularization 8.4 Predicting predictive accuracy 8.5 Model Comparison 8.6 Exercies", " 8 Chapter 7 - Model comparison 8.1 The problem with parameters The section elaborates on overfitting and underfitting. It introduce \\(R^2\\) and some reasoning for AIC and WAIC. It starts elaborating on why \\(R^2\\) is useless. p-values will never be an indicator for predictive accuracy!!, what p-value does is only care about type 1 error. In general, do not pay too much attention to these. 8.2 Entropy and Accuracy We see that entropy is able to reflect how wrong we are, instead of just measuring hit rate as accuracy does. When we calculate the entropy of a model, we will try to minize the entropy, that is because the large the entropy, the more surprised we are of the actual outcomes. The larger the entropy the larger is q (our model) to p (the actual outcome), the distance from our model to the true model is also called the Divergence. The equation: \\[Divergence= \\sum_i p_i(log(p_i) - log(q_i))\\] Divergence example: Go to the eath / mars example in the book. In its essence, we see that earth is a HIGH entropy place, as there are a lot of land and a lot of water (hence there is lots of both). When you then go to mars, you will be surprised. But the surprise going from mars to earth will be greater, as there is low entropy on mars (as almost 100% of the planet is covered in land), hence you will be very surprised going to a place with higher entropy. The simpler model is better than complicated models, that is because simpler model has higher entropy, as they generelize more than complicated models. Where we see complicated models can be compared with mars, where it is very certain about different outcomes (typical characteristics of overfitting) To estimate divergence for a model, we will use the log-score, this will be a distribution, this can be foind with lppd() from the rethinking package (log-pointwise-predictive-density). \\[Deviance = logscore * -2.\\] We will look at following information criteria: AIC WAIC PSIS We see that deviance has the same properties as R2, thus it will not penalize for introducing more variables and also overfitting. Then we also see that CV can make out of sample performance estimations based on testing the model on unseen data. Notice that nonsens models can have good predictions, but their inference and causal relationships may be totally off, hence prediction and inference is two totally different ways of approaching a problem. Definition of PSIS: Recall that with leave one out CV, we estimate the model always leaving out one observation. With this technique we get a very good out of sample estimate. Although it it cumbersome. PSIS does something similar, with this we are also able to see the score for each observation PSISk(), thus if the score is high, we see that the model predictions are very different from what the observation may look like. This also implies that high PSIS values for observations indicates that the observation may have a large influence on the bayesian updating. We see an example in section H2, where students t distribution is used instead. 8.3 Golem Taming: Regularization As we have seen in ML classes, we can regularize the model to make it harder to capture variance in the training / calibration process. We regularize the priors, we see that we have a prior (striped) that can take on many values, while two versions of regularizations is more conservative, hence it needs more extreme data to overwhelm the priors. 8.4 Predicting predictive accuracy We see that: the loo function is a very accurate out of sample performance estimate AIC: Information criteria to approximate out of sample performance, we see that WAIC always (almost) outperforms the AIC is WAIC: we see that lppd is the loglik that we have in the AIC. One can use the compare() function to compare different modes, that will be done on the WAIC, and it will show the weight, hence if you have several models, it will say what weights different models should have in a prediction scenario. pWAIC is the effective number of parameters in the model. One must remember that the information criteria is an indication of the overfit, i.e., the model overconfidence. It should in fact never be used for model selection, as these criteria does not care about causality. 8.5 Model Comparison 8.6 Exercies 8.6.1 7M3 When comparing models with an information criterion, why must all models be fit to exactly the same observations? What would happen to the information criterion values, if the models were fit to different numbers of observations? Perform some experiments, if you are not sure. Because the information is reflecting how surprised a model is when it sees some data. 8.6.2 7M4 What happens to the effective number of parameters, as measured by PSIS or WAIC, as a prior becomes more concentrated? Why? Perform some experiments, if you are not sure. We see that the effective number of parameters are decreasing, that is because you constrain the possibilities of the model, hence less flexible, i.e., less complex and therefore less effective number of parameters. If we use WAIC() we will see a penalty term, this is representitive of the effective numbers parameters. 8.6.3 7M5 Provide an informal explanation of why informative priors reduce overfitting. If we have flat priors (some without information) we will see that the model can fit to any scenario. If we impose information in the priors we can manipulate the model to not model for scenarios that are impossible or extremely unlikely. 8.6.4 7M6 Provide an informal explanation of why overly informative priors result in underfitting. This is basically because we constrain the model too much. Hence when the model is fitted it needs very extreme cases to adjust the priors. 8.6.5 7H1 In 2007, The Wall Street Journal published an editorial (“We’re Number One, Alas”) with a graph of corporate tax rates in 29 countries plotted against tax revenue. A badly fit curve was drawn in (reconstructed at right), seemingly by hand, to make the argument that the relationship between tax rate and tax revenue increases and then declines, such that higher tax rates can actually produce less tax revenue. I want you to actually fit a curve to these data, found in data(Laffer).Do: Consider models that use tax rate to predict tax revenue. Compare, using WAIC or PSIS, a straight-line model to any curved models you like. What do you conclude about the relationship between tax rate and tax revenue? library(rethinking) data(Laffer) d &lt;- Laffer #Add vector of squared values, for the polynomial variable #d$tax_rate2 &lt;- d$tax_rate^2 #NOTICE: I moved the quadratic transformation to the model ## Starndardizing ## #&#39; I could have standardized the tax rate #linear model ml &lt;- quap( alist( tax_revenue ~ dnorm( mu , sigma ) , mu &lt;- a + b1*tax_rate + sigma, a ~ dnorm( 10 , 2 ) , #mean b1 ~ dlnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dunif( 0 , 50 ) ) ,data = d ) #quadratic model mq &lt;- quap( alist( tax_revenue ~ dnorm( mu , sigma ) , mu &lt;- a + b1*tax_rate + b2*tax_rate^2 + sigma, a ~ dnorm( 10 , 2 ) , #mean b1 ~ dlnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect b2 ~ dnorm( 0 , 1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dunif( 0 , 50 ) ) ,data = d ) WAIC(ml) WAIC(mq) WAIC lppd penalty std_err 126.4331 -55.82019 7.396335 24.86687 WAIC lppd penalty std_err 128.139 -56.26048 7.809026 23.21052 We see that the WAIC is increasing in the more complex model, we also see the penalty is increasing, meaning that the effective number of parameters is increasing. PSIS(ml) PSIS lppd penalty std_err 130.7289 -65.36447 9.537887 29.43002 PSIS(mq) PSIS lppd penalty std_err 133.1163 -66.55817 10.39137 28.86921 Another approach can be: compare(ml,mq,func = PSIS) x 134.3800 135.6509 x 32.87344 31.30131 x 0.000000 1.270912 x NA 2.858863 x 11.25874 11.59884 x 0.6537255 0.3462745 We can look at the plot to see if they can be said to be different. We see that dPSIS is the difference to the best model, and the dSE is the standard error, if the dSE is smaller than dPSIS, then we can certainly say that the given model is worse than the linear model. The weights are the AIC weights, so if we were to predict, this is the weight that each model should have in a prediction setting. plot(compare(ml,mq,func = PSIS)) We see the there is no statistical evidence that the more complicated model is any better than the linear model. (We look at the small gray range in the middle) PSISk(ml) ## [1] 0.38 -0.04 0.28 -0.06 -0.07 -0.04 0.13 0.43 0.01 -0.13 0.36 2.06 ## [13] 0.27 0.10 -0.03 -0.16 -0.05 -0.11 0.21 -0.08 0.00 -0.05 0.01 0.12 ## [25] 0.26 0.38 0.45 0.49 0.30 We see in that one value spikes (theres is one value for each observation). What can we conclude? precis(ml) precis(mq) x 0.6829483 0.0462522 1.6116390 x 0.6075766 0.0203702 0.1969248 x -0.2880766 0.0136968 1.2969151 x 1.6539731 0.0788077 1.9263629 x 0.9022411 0.1097593 -0.0024818 1.6268564 x 0.8451227 0.0635355 0.0016652 0.2138892 x -0.4484283 0.0082173 -0.0051431 1.2850202 x 2.2529104 0.2113014 0.0001794 1.9686926 We see that in the linear model the intercept says that the tax return will be 2.18 if tax rate is 0. We see that the there is a positive effect from tax rate to tax return. And there is typically always a positive effect. We see in the quadratic model that the squared values are really just 0, while a greater effect is put on the first order. 8.6.6 7H2 In the Laffer data, there is one country with high tax revenue that is an outlier. Use PSIS and WAIC to measure the importance of this outlier in the models you fit in the previous problem. Then use robust regression with a Students t distribution to revisit the curve fitting problem. How much does does a curved relationship depend upon the outlier point? The exercise is separated into three parts: Identify the outlier Show the predictions for the two models Construct a more robust model, with students t distribution, as it has fatter tails. 8.6.6.1 (1) Identify the outlier We can inspect PSIS values. par(mfrow = c(2,1)) plot(PSISk(ml),pch = 16,cex = 0.8,main = &quot;Linear vs. Quadratic model&quot;,col = &quot;#5ab4ac&quot;,ylab = &quot;PSISk&quot;,xlab = &quot;Observation&quot;) points(PSISk(mq),pch = 16,cex = 0.8,col = &quot;#d8b365&quot;) abline(v = which.max(PSISk(ml)),lty = 2,col = &quot;lightgrey&quot;) abline(h = 1,lty = 2,col = &quot;lightgrey&quot;) legend(&quot;topright&quot;,legend = c(&quot;Linear&quot;,&quot;Quadratic model&quot;),col = c(&quot;#5ab4ac&quot;,&quot;#d8b365&quot;),pch = 16,cex = 1) plot(d$tax_revenue,pch = 20,ylab = &quot;Tax revenue&quot;) abline(v = which.max(PSISk(ml)),lty = 2,col = &quot;lightgrey&quot;) From this we see that both models concludes that observation no. 12 is an outlier as it locates above 1. This is a nation with a very high tax revenue. 8.6.6.2 (2) Inspect model predictions #Grid of values for predictions T_seq &lt;- seq(0,35) #Making predictions ml_pred &lt;- link(ml,data = list(tax_rate = T_seq)) #use list() to enable the model to map the vector to the variable mq_pred &lt;- link(mq,data = list(tax_rate = T_seq)) ## Plotting! par(mfrow = c(2,1),mar = c(4, 4, 1, .8)) #Linear model plot(d$tax_rate,y = d$tax_revenue,pch = 20,xlab = &quot;Tax rate&quot;,ylab = &quot;Tax revenue&quot;,main = &quot;Linear model&quot;) lines(T_seq,colMeans(ml_pred)) shade(object = apply(X = ml_pred,MARGIN = 2,FUN = PI,prob = 0.89),lim = T_seq) #Quadratic model plot(d$tax_rate,y = d$tax_revenue,pch = 20,xlab = &quot;Tax rate&quot;,ylab = &quot;Tax revenue&quot;,main = &quot;Quadratic model&quot;) lines(T_seq,colMeans(mq_pred)) shade(object = apply(X = mq_pred,MARGIN = 2,FUN = PI,prob = 0.89),lim = T_seq) We see that the quadratic linear model captures some curvature, while the linear model naturally does not. Although the effect is not strong enough to account for the outlier, if it even should. Now we can try to construct a model that has fatter tails, thus is less critical of outliers. 8.6.6.3 (3) Using students t distribution Notice that i use the same code, thus it is just more compressed Notice that the following alterations are done: I had to standardize the tax rate for the students t distribution to work. Set the mean of the distributions to 0 and decreased the standard deviation Exchanged distribution for sigma to be exponentional d$tax_rate_s &lt;- standardize(d$tax_rate) d$tax_revenue_s &lt;- standardize(d$tax_revenue) #quadratic model - normal distribution mq &lt;- quap( alist( tax_revenue_s ~ dnorm( mu , sigma ) , mu &lt;- a + b1*tax_rate_s + b2*tax_rate_s^2 + sigma, a ~ dnorm(0, 0.2) , #mean b1 ~ dnorm(0, 0.1) , #We say that the beta can be higher and lower than 0, negative or positive effect b2 ~ dnorm(0, 0.1) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dexp(0.01) ) ,data = d ) #quadratic model students t mq_t &lt;- quap( alist( tax_revenue_s ~ dstudent( mu , sigma ) , mu &lt;- a + b1*tax_rate_s + b2*tax_rate_s^2 + sigma, a ~ dnorm( 0 , 0.2 ) , #mean b1 ~ dnorm( 0 , 0.1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect b2 ~ dnorm( 0 , 0.1 ) , #We say that the beta can be higher and lower than 0, negative or positive effect sigma ~ dexp(0.01) ) ,data = d ) Notice that I got an error including Start values for parameters may be too far from MAP. Try better priors or use explicit start values. I simply had to restrict the priors. #Grid of values for predictions T_seq_s &lt;- seq(-3,1,by = 0.1) #Making predictions mq_pred &lt;- link(mq,data = list(tax_rate_s = T_seq_s)) #use list() to enable the model to map the vector to the variable mq_t_pred &lt;- link(mq_t,data = list(tax_rate_s = T_seq_s)) ## Plotting! plot(x = d$tax_rate_s,y = d$tax_revenue_s,pch = 20) lines( T_seq_s , colMeans(mq_pred) ) lines( T_seq_s , colMeans(mq_t_pred), col = &quot;red&quot; ) shade( apply( mq_pred , 2 , PI ) , T_seq_s ) shade( apply( mq_t_pred , 2 , PI ) , T_seq_s, col = col.alpha(&quot;red&quot;,0.15) ) I get a bit different results than the solution suggestions. In general what we experience with the students t distribution is that the it assigns more probability to the tails, hence it is less surprised than a regular normal distribution. Thus one could say that students t distribution has a higher entropy, and it is less prone to being surprised, for more explanations I refer to section 11.1. 8.6.7 7H5 Revisit the urban fox data from the previous chapters practice problems. Use WAIC or PSIS based model comparison on five different models each using weight as the outcome and containing these sets of predictor variables: avgfood + groupsize + area avgfood + groupsize groupsize + area avgfood area Can you explain the relative difference in WAIC scores using the fox DAG from the previous chapter? Be sure to pay attention to the standard error of the score differences (dSE). #loading the data library(rethinking) data(foxes) d &lt;- foxes d &lt;- scale(d) %&gt;% data.frame() head(d) group avgfood groupsize area weight -2.025170 -1.924829 -1.524089 -2.239596 0.4141347 -2.025170 -1.924829 -1.524089 -2.239596 -1.4270464 -1.900212 -1.118035 -1.524089 -1.205508 0.6759540 -1.900212 -1.118035 -1.524089 -1.205508 1.3009421 -1.775255 -1.319734 -1.524089 -1.130106 1.1151348 -1.775255 -1.319734 -1.524089 -1.130106 -1.0807692 # 1. avgfood + groupsize + area m1 &lt;- quap( alist( weight ~ dnorm(mu,sigma), mu &lt;- a + bGS*groupsize + bAF*avgfood + bA*area + sigma, a ~ dnorm(0,0.2), bGS ~ dnorm(0,0.2), bAF ~ dnorm(0,0.2), bA ~ dnorm(0,0.2), sigma ~ dexp(1) ),data = d) # 2. avgfood + groupsize m2 &lt;- quap( alist( weight ~ dnorm(mu,sigma), mu &lt;- a + bGS*groupsize + bAF*avgfood + sigma, a ~ dnorm(0,0.2), bGS ~ dnorm(0,0.2), bAF ~ dnorm(0,0.2), sigma ~ dexp(1) ),data = d) # 3. groupsize + area m3 &lt;- quap( alist( weight ~ dnorm(mu,sigma), mu &lt;- a + bGS*groupsize + bA*area + sigma, a ~ dnorm(0,0.2), bGS ~ dnorm(0,0.2), bA ~ dnorm(0,0.2), sigma ~ dexp(1) ),data = d) # 4. avgfood m4 &lt;- quap( alist( weight ~ dnorm(mu,sigma), mu &lt;- a + bAF*avgfood + sigma, a ~ dnorm(0,0.2), bAF ~ dnorm(0,0.2), sigma ~ dexp(1) ),data = d) # 5. area m5 &lt;- quap( alist( weight ~ dnorm(mu,sigma), mu &lt;- a + bA*area + sigma, a ~ dnorm(0,0.2), bA ~ dnorm(0,0.2), sigma ~ dexp(1) ),data = d) compare(m1,m2,m3,m4,m5) x 329.1435 330.0579 331.1008 337.3852 337.5071 x 16.71357 16.78985 16.91487 15.48986 15.44320 x 0.0000000 0.9144073 1.9573129 8.2416927 8.3636141 x NA 1.216568 2.504086 4.388906 4.501271 x 4.013940 3.800891 3.469019 2.570548 2.839409 x 0.4901075 0.3102632 0.1841901 0.0079548 0.0074844 #Identify lower and upper boundaries lb &lt;- compare(m1,m2,m3,m4,m5,sort = F)$WAIC - compare(m1,m2,m3,m4,m5,sort = F)$SE #Lower bound ub &lt;- compare(m1,m2,m3,m4,m5,sort = F)$WAIC + compare(m1,m2,m3,m4,m5,sort = F)$SE #Upper bound WAICComp &lt;- compare(m1,m2,m3,m4,m5,sort = F)$WAIC dev.off() ## null device ## 1 plot(y = 1:5,x = WAICComp ,ylab = &quot;Model m:&quot;,xlab = &quot;WAIC&quot; ,pch = 20,xlim = c(300,375) ,main = &quot;Model comparison&quot; ,frame.plot = T ) for(i in 1:5) abline(h = i,lty = 2,col = &quot;darkgrey&quot;) #Plot the standard errors for(i in 1:5){ lines(x = c(lb[i],ub[i]),y = c(i,i)) } We see that: None of the models are superior Three similar models, that being 1, 2 and three all being multivariate. The two bivariate models respectively with avgfood or area are basically the same, the being the reason that area and avgfood is a pipe from avgfood to weight. It is expected that they do not differ two much, as we have causality from area, to avgfood which has both directly on weight and indirectly flowing through groupsize. "],["chapter-8-model-comparison.html", "9 Chapter 8 - Model comparison 9.1 Building an interaction 9.2 Symmetry of interactions 9.3 Continuous interactions 9.4 Exercises", " 9 Chapter 8 - Model comparison Introduction, using the case of the planes coming back with holes in the wings, does that mean that one should make the wings stronger? A model will suggest that, because it only sees scenarios of where the holes are in planes that came back, hence it is conditional on planes returning, we rarely see holes in the center part. Since we only see planes coming back, we dont get to see the other condition. The following sections elaborate on how we can model interactions, meaning how one outcome may result another. 9.1 Building an interaction This section elaborates on what an interaction can be used at what for. library(rethinking) data(rugged) d &lt;- rugged # make log version of outcome d$log_gdp &lt;- log( d$rgdppc_2000 ) # extract countries with GDP data dd &lt;- d[ complete.cases(d$rgdppc_2000) , ] # rescale variables dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) #8.2 m8.1 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a + b*( rugged_std - 0.215 ) , a ~ dnorm( 1 , 1 ) , b ~ dnorm( 0 , 1 ) , sigma ~ dexp( 1 ) ) , data=dd ) #8.3 set.seed(7) prior &lt;- extract.prior( m8.1 ) # set up the plot dimensions plot( NULL , xlim=c(0,1) , ylim=c(0.5,1.5) , xlab=&quot;ruggedness&quot; , ylab=&quot;log GDP&quot; ) abline( h=min(dd$log_gdp_std) , lty=2 ) abline( h=max(dd$log_gdp_std) , lty=2 ) # draw 50 lines from the prior rugged_seq &lt;- seq( from=-0.1 , to=1.1 , length.out=30 ) mu &lt;- link( m8.1 , post=prior , data=data.frame(rugged_std=rugged_seq) ) for ( i in 1:50 ) lines( rugged_seq , mu[i,] , col=col.alpha(&quot;black&quot;,0.3) ) sum( abs(prior$b) &gt; 0.6 ) / length(prior$b) ## [1] 0.545 m8.1 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a + b*( rugged_std - 0.215 ) , a ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp(1) ) , data=dd ) 9.1.1 Adding an indicator variable isn’t enough. One may argue that indicator variables may be sufficient to reflect certain situations, e.g., if something is on Africa or not, hence an indicator (i.e., dummy variable) may be included in the model. Although that will just reflect the mean value of that given outcome as it is either in or out. #8.7 # make variable to index Africa (1) or not (2) dd$cid &lt;- ifelse( dd$cont_africa==1 , 1 , 2 ) #8.8 m8.2 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=dd ) #8.9 compare( m8.1 , m8.2 ) x -252.2694 -188.7489 x 15.30363 13.29716 x 0.00000 63.52044 x NA 15.14767 x 4.258180 2.693351 x 1 0 #8.10 precis( m8.2 , depth=2 ) x 0.8804170 1.0491586 -0.0465124 0.1123923 x 0.0159377 0.0101860 0.0456887 0.0060917 x 0.8549455 1.0328794 -0.1195318 0.1026565 x 0.9058885 1.0654378 0.0265069 0.1221281 #8.11 post &lt;- extract.samples(m8.2) diff_a1_a2 &lt;- post$a[,1] - post$a[,2] PI( diff_a1_a2 ) ## 5% 94% ## -0.1990056 -0.1378378 #8.12 rugged.seq &lt;- seq( from=-0.1 , to=1.1 , length.out=30 ) # compute mu over samples, fixing cid=2 mu.NotAfrica &lt;- link( m8.2 , data=data.frame( cid=2 , rugged_std=rugged.seq ) ) # compute mu over samples, fixing cid=1 mu.Africa &lt;- link( m8.2 , data=data.frame( cid=1 , rugged_std=rugged.seq ) ) # summarize to means and intervals mu.NotAfrica_mu &lt;- apply( mu.NotAfrica , 2 , mean ) mu.NotAfrica_ci &lt;- apply( mu.NotAfrica , 2 , PI , prob=0.97 ) mu.Africa_mu &lt;- apply( mu.Africa , 2 , mean ) mu.Africa_ci &lt;- apply( mu.Africa , 2 , PI , prob=0.97 ) plot(dd$log_gdp,dd$rugged,ylim = c(0.7,1.4)) grid() lines(mu.NotAfrica_mu,col = &quot;black&quot;) lines(mu.Africa_mu,col = &quot;blue&quot;) Thus we see that it is far better including an index instead of a dummy variable!!! 9.2 Symmetry of interactions About the index approach. 9.3 Continuous interactions 9.3.1 A Winter Flower This is an example with a flower that depends on sun and water to make photosynthesis, although having only one of the two is not useful at all, hence we need to deal with this using an interaction. This is the data: library(rethinking) data(tulips) d &lt;- tulips str(d) ## &#39;data.frame&#39;: 27 obs. of 4 variables: ## $ bed : Factor w/ 3 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;: 1 1 1 1 1 1 1 1 1 2 ... ## $ water : int 1 1 1 2 2 2 3 3 3 1 ... ## $ shade : int 1 2 3 1 2 3 1 2 3 1 ... ## $ blooms: num 0 0 111 183.5 59.2 ... 9.3.2 The models We are going to center W (water) and S (shade). d$blooms_std &lt;- d$blooms / max(d$blooms) d$water_cent &lt;- d$water - mean(d$water) d$shade_cent &lt;- d$shade - mean(d$shade) We have the non centered model: We set the priors to (we correct the sd in the following): Alpha = 0.5 means that when shade and sun is at its mean, the model expects blooms to be halfway to the observed maximum. We see that the slopes (betas) for W and S is 0, meaning that we have no idea if there is going to be relatively more shade or water, hence it will just be centered at 0. But what does the standard deviation of 1 imply? We see that the blooming should not go below 0 and above 1. Lets us inspect the alpha parameter and what the prior expects to see. a &lt;- rnorm(n = 1e4 ,mean = 0.5 ,sd = 1) #Intercept alpha less than 0 and higher than 1 sum( a &lt; 0 | a &gt; 1 ) / length( a ) ## [1] 0.6189 It implies that the priors assign most of the probability outside of the range of 0 to 1. Hence we try to constrain the priors a bit: a &lt;- rnorm(n = 1e4,mean = 0.5,sd = 0.25) sum( a &lt; 0 | a &gt; 1 ) / length( a ) ## [1] 0.0443 Now we see that only 5% is less than 0 and higher than 1. That is much better. Notice that we want to allow the model to reach the edges and probably go a bit above / below so we can fit data within this region. Regarding the beta values we see that the bw &lt;- rnorm(n = 1e4,mean = 0,sd = 1) dens(bw,main = &quot;bw prior&quot;) We also see that bw &lt;- rnorm(n = 1e4,mean = 0,sd = 0.25) dens(bw,main = &quot;bw prior&quot;) Now we see that the PI for 95% is between -0.5 and 0.5. PI(samples = bw,0.95) ## 3% 98% ## -0.4858059 0.4828205 In general the goal of the priors in this example is to set priors with weak information but also constrain the model from overfitting. The is how the model looks when finalized: #Non interaction model! m8.4 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) ,data=d ) precis(m8.4) x 0.3587658 0.2050338 -0.1125315 0.1581543 x 0.0302189 0.0368895 0.0368757 0.0214437 x 0.3104702 0.1460771 -0.1714660 0.1238831 x 0.4070615 0.2639904 -0.0535971 0.1924255 Building the interaction model The model now look like this: #With interaction m8.5 &lt;- quap( alist( blooms_std ~ dnorm( mu , sigma ) , mu &lt;- a + bw*water_cent + bs*shade_cent + bws*water_cent*shade_cent , a ~ dnorm( 0.5 , 0.25 ) , bw ~ dnorm( 0 , 0.25 ) , bs ~ dnorm( 0 , 0.25 ) , bws ~ dnorm( 0 , 0.25 ) , sigma ~ dexp( 1 ) ) ,data = d) 9.3.3 Plotting posterior predictions To do this, we must: Draw samples from #Plotting the non interaction model par(mfrow=c(2,3)) # 3 plots in 1 row models &lt;- list(m8.4,m8.5) for(m in models){ for(s in -1:1) { #Create index of observations given the shade level idx &lt;- which(d$shade_cent==s) #We have three levels -1, 0 and 1. #Plot each water levels given the shade level plot(d$water_cent[idx] ,d$blooms_std[idx] ,xlim=c(-1,1),ylim=c(0,1) ,xlab=&quot;water&quot;,ylab=&quot;blooms&quot; ,pch=16,col=rangi2 ) #Draw samples mu &lt;- link(m #Non interaction model ,data = data.frame(shade_cent=s,water_cent=-1:1)) #Plotting 20 posterior lines for(i in 1:20) lines(x = -1:1,y = mu[i,] ,col=col.alpha(&quot;black&quot;,0.3)) } } Figure 9.1: Top row = no interaction model, bottum row = innteraction model. 9.3.4 Plotting prior predictions Now we are going to extract priors to plot these. set.seed(7) prior &lt;- extract.prior(m8.5) #Plotting the non interaction model par(mfrow=c(2,3)) # 3 plots in 1 row models &lt;- list(m8.4,m8.5) for(m in models){ for(s in -1:1) { #Create index of observations given the shade level idx &lt;- which(d$shade_cent==s) #We have three levels -1, 0 and 1. #Plot each water levels given the shade level plot(d$water_cent[idx] ,d$blooms_std[idx] ,xlim=c(-1,1),ylim=c(-0.5,1.5) ,xlab=&quot;water&quot;,ylab=&quot;blooms&quot; ,pch=16,col=rangi2 ) abline(h = c(0,1),lty = 2,col = &quot;grey&quot;) #Draw samples mu &lt;- link(m #Non interaction model ,data = data.frame(shade_cent=s,water_cent=-1:1) ,post = prior) #Plotting 20 prior lines for(i in 1:20) lines(x = -1:1,y = mu[i,] ,col=col.alpha(&quot;black&quot;,0.3)) } } Figure 9.2: Top row = no interaction model, bottum row = innteraction model. We see that the priors both for the interaction and no interaction model is typically within the limits, so that is good. We can say that the priors are: Harmless Weakly realistic The priors include non or very low bias to positive or negative effects as most things can happen, although the priors tend to be in a reasonable range, hence we are doing better then a flat prior, but we do not include much information either. 9.4 Exercises 9.4.1 8M4 Repeat the tulips analysis, but this time use priors that constrain the effect of water to be positive and the effect of shade to be negative. Use prior predictive simulation. What do these prior assumptions mean for the interaction prior, if anything? Notes: Need to be changed, so the prior distribution is higher than 0 and less than 0, thus the distribution of the two distributions must be on each side of 0. We can do this by manipulating the mean or the standard deviation of the dsitruibutions. There are two difficulties with this: You can end up having an unrealistic mean. To avoid that, you can manipulate the standard deviation, although that may make too much certainty Also a normal distribution will never be strictly within some region, as it never touches 0. Thus we can use an exponential distribution, although that only has one tail. Thus we can make a log normal distribution instead. The model is from code chunk 8.24 - m8.5. #Loading data library(rethinking) data(&quot;tulips&quot;) d &lt;- tulips #&#39; water and shade variables are three levels, ordered after amount of water/shade. #&#39; Cluster of plants in the same section of the greenhouse. #&#39; Blooms = the outcome variable which we have to predict. #Transforming the data d$blooms_std &lt;- d$blooms / max(d$blooms) d$water_cent &lt;- d$water - mean(d$water) d$shade_cent &lt;- d$shade - mean(d$shade) 9.4.1.1 Interaction model Model specification #The model m &lt;- quap( alist( blooms_std ~ dnorm(mu,sigma), mu &lt;- a + bw * water_cent - bs * shade_cent + bws * water_cent * shade_cent, #Notice there is minus bs a ~ dnorm(0.5,0.25), bw ~ dlnorm(0,0.25), bs ~ dlnorm(0,0.25), bws ~ dlnorm(0,0.25), sigma ~ dexp(1) ) ,data = d ) precis(m) x 0.3822041 0.5410913 0.5081694 0.4926979 0.6048014 x 0.1058485 0.0990471 0.0967641 0.1027979 0.1203730 x 0.2130377 0.3827949 0.3535217 0.3284070 0.4124220 x 0.5513704 0.6993877 0.6628172 0.6569888 0.7971807 We see that the more shade, the less blooms and the more water the more blooms. Although the combination of water and shade will lead to less blooms, I guess because the plant will be drowning in water. Prior predictive simulation par(mfrow = c(1,1)) simulations &lt;- 20 prior &lt;- extract.prior(m) for (s in -1:1) { idx &lt;- which(d$shade_cent == s) plot(x = d$water_cent[idx] ,y = d$blooms[idx] ,xlim = c(-1,1), ylim = c(-0.5,1.5) ,xlab = &quot;water&quot; ,ylab = &quot;blooms&quot; ,pch = 16 ,col = rangi2 ,main = paste(&quot;Priors predictive simulation, S = &quot;,s) ) abline(h = c(0,1),lty = 2) #Call link function to make predictions mu &lt;- link(fit = m ,data = data.frame(shade_cent = s, water_cent = -1:1) ,post = prior #posterior is just the prior, hence we introduce no data ) for (i in 1:simulations) { lines(-1:1 ,mu[i,] ,col = col.alpha(&quot;black&quot;,0.3) ) } } We see that we are going to regularize the priors even more, to make better priors. #The model m &lt;- quap( alist( blooms_std ~ dnorm(mu,sigma), mu &lt;- a + bw * water_cent - bs * shade_cent + bws * water_cent * shade_cent, #Notice there is minus bs a ~ dnorm(0.5,0.25), bw ~ dlnorm(-2,0.25), #Set mean to -2 bs ~ dlnorm(-2,0.25), #Set mean to -2 bws ~ dlnorm(-2,0.25), #Set mean to -2 sigma ~ dexp(1) ) ,data = d ) simulations &lt;- 20 prior &lt;- extract.prior(m) for (s in -1:1) { idx &lt;- which(d$shade_cent == s) plot(x = d$water_cent[idx] ,y = d$blooms[idx] ,xlim = c(-1,1), ylim = c(-0.5,1.5) ,xlab = &quot;water&quot; ,ylab = &quot;blooms&quot; ,pch = 16 ,col = rangi2 ,main = paste(&quot;Priors predictive simulation, S = &quot;,s) ) abline(h = c(0,1),lty = 2) #Call link function to make predictions mu &lt;- link(fit = m ,data = data.frame(shade_cent = s, water_cent = -1:1) ,post = prior #posterior is just the prior, hence we introduce no data ) for (i in 1:simulations) { lines(-1:1 ,mu[i,] ,col = col.alpha(&quot;black&quot;,0.3) ) } } 9.4.1.2 Conclusion We see that the regularized priors are more realistic. Never plot the priors with the actual data points, that is just done for practical reasons. 9.4.2 8H3 "],["chapter-9-markov-chain-monte-carlo.html", "10 Chapter 9 - Markov Chain Monte Carlo 10.1 Good King Markov and his island kingdom 10.2 Metropolis algorithms 10.3 Hamiltonian Monte Carlo 10.4 Easy HMC: ulam 10.5 Care and feeding of your Markov chain 10.6 Exercises 10.7 Lecture notes", " 10 Chapter 9 - Markov Chain Monte Carlo 10.1 Good King Markov and his island kingdom 10.2 Metropolis algorithms it is sampling randomly starting from the ending point of the previous accepted sample. This is inefficient, as the randomness will take time to find samples that are acceptet. Hence we rarely use this today. HMC is dealing with this flaw, notice that Metropolis Hastings will always converge, but it will just take time. 10.3 Hamiltonian Monte Carlo This is a more effect approach for sampling. It is two hyper parametersÆ Leapfrog steps (number of steps to take / time to run) Stepsize, that is how long the steps are We see that it is will start from where it ended, and then it is running for the n number of leapfrogs, where the gravity will force it towards a dense region. We see that HMC is able to make a chain that is going in circles. The folllowing is an example. This is not very effective. Although over time it will converge to a stable model. We want a scenario as in the left window. The right window is having U-Turns, we dont want these. The problem with U-Turns is that the step size and the number of steps that is taken, is unfortunate letting the model end the same place as it started. Hence for HMC to be effective, one must really tune these parameters correctly. Then how are we dealing with U-Turns? We see that Ulam is able to find appropriate step sizes and number of steps (leapfrog steps), through a warmup phase. Notice that these do not account as samples, although the samples drawn with the tuned hyperparameters are. This is an example of a sampler that is inefficient: &lt;https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&amp;target=banana&gt; (needs to be seet to HMC, standard dist, and increase leapfrog steps) Fancy models: scientist has invented an adaptive model chain, where it will end if it sees a U-Turn, this is called (NUTS - No U-Turn sampler). This is essentially pointing in both directions. to find out if the sampler started going backwards. 10.4 Easy HMC: ulam 10.4.1 Fitting using quap The following is an example of how the ulam function works. We are going to use the example from chapter 8 about predicting log GDP based on ruggedness of a nation. library(rethinking) data(rugged) d &lt;- rugged d$log_gdp &lt;- log(d$rgdppc_2000) dd &lt;- d[ complete.cases(d$rgdppc_2000) , ] dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) dd$cid &lt;- ifelse( dd$cont_africa==1 , 1 , 2 ) The following is an example of how to specify the model and fit it using the quap method. m8.3 &lt;- quap( alist( log_gdp_std ~ dnorm( mu , sigma ) , mu &lt;- a[cid] + b[cid]*( rugged_std - 0.215 ) , a[cid] ~ dnorm( 1 , 0.1 ) , b[cid] ~ dnorm( 0 , 0.3 ) , sigma ~ dexp( 1 ) ) , data=dd ) precis( m8.3 , depth=2 ) x 0.8865639 1.0505696 0.1325055 -0.1425763 0.1094900 x 0.0156751 0.0099362 0.0742018 0.0547474 0.0059347 x 0.8615121 1.0346896 0.0139167 -0.2300732 0.1000051 x 0.9116158 1.0664496 0.2510943 -0.0550794 0.1189748 We see that we have an index model instead of indicator variables, thus one alpha and beta for each category. Where 1 = Africa and 2 = all other countries. Thus we see that alpha parameter (intercept) is positive for both countries, we also set the mean to be positive. And we see that the coefficient for the ruggedness is positive for Africa and negative for the rest of the world. We will also fit this model using Hamiltonian Monte Carlo 10.4.2 Fitting using HMC This section will be separated in two parts: 10.4.2.1 Preparing data To do this, one must transform the variables just as we did with the quap method. Although for this method, we will arrange the data in a list. The benfit is that the different does not have to be of the same length, when arranged in lists. dat_slim &lt;- list( log_gdp_std = dd$log_gdp_std, rugged_std = dd$rugged_std, cid = as.integer(dd$cid) ) str(dat_slim) ## List of 3 ## $ log_gdp_std: num [1:170] 0.88 0.965 1.166 1.104 0.915 ... ## $ rugged_std : num [1:170] 0.138 0.553 0.124 0.125 0.433 ... ## $ cid : int [1:170] 1 2 2 2 2 2 2 2 2 1 ... 10.4.2.2 Sampling from the posterior Now we are going to fit the model using ulam, which relies on the stan engine. m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu,sigma), mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1,0.1), b[cid] ~ dnorm(0,0.3), sigma ~ dexp(1) ) ,data = dat_slim ,chains = 1 #no of independent chains to sample from ) ## ## SAMPLING FOR MODEL &#39;f3314e777e4c586121dcc9de98266129&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 4.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.44 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.057138 seconds (Warm-up) ## Chain 1: 0.038179 seconds (Sampling) ## Chain 1: 0.095317 seconds (Total) ## Chain 1: We see that there is a warmup, that is something about calibrating the model to find the distribution that it should draw from, and then it starts drawing samples. precis(m9.1,depth = 2) x 0.8860046 1.0508447 0.1323439 -0.1424482 0.1116964 x 0.0173372 0.0100159 0.0756798 0.0573397 0.0059156 x 0.8584532 1.0353497 0.0133625 -0.2318686 0.1027950 x 0.9135241 1.0669182 0.2555184 -0.0522092 0.1213616 x 923.3674 650.3322 741.0804 689.9623 674.4641 x 1.0020717 0.9985137 0.9988082 0.9996360 0.9980212 Now we see that the outcome is the same, although we have two new outputs: n_eff: crude estimate of the number of independent samples you managed to get. Due to anti-correlation, we are able to get more effective number of samples than actual samples. With MC, the samples will be correlated in the way, that it is not just taking samples randomly, hence we will always take a sample in a new region. A low n_eff means that the chain is having a hard time exploring the distriubtion. Rhat4: An indicator of the convergence of the Markov Chains to the target distribution. ‘4’ is just the version of the Rhat (\\(\\hat{R}\\)). It is the in chain variation compared to the between chain variation. 10.4.2.3 Sampling again, in parallel You can sample different chains in the same time, one for each core in your computer. m9.1 &lt;- ulam( alist( log_gdp_std ~ dnorm(mu,sigma), mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1,0.1), b[cid] ~ dnorm(0,0.3), sigma ~ dexp(1) ) ,data = dat_slim ,chains = 4 #no of independent chains to sample from ,cores = 4 #We use four cores ) 10.4.2.4 Visualization you can plot the sampling, to see the distributions that you find. rethinking::pairs(m9.1) Here we see the distribution of the parameters. 10.4.2.5 Checking the chain We have two visual tools: traceplots: here we look for: Stationarity: we want the line to show no trend and stay within the same boundaries over time good mixing: that the chain rapidly explores the full region and not being stuck in regions. convergence: that multiple independent chains stick around the same region. When you plot multiple chains it starts getting difficult to see how each chain compares to the others. That is what trank plots is dealing with. trankplots (trace rank plots): We want to see the histograms of the independent chains overlapping each other. 10.5 Care and feeding of your Markov chain This section has some illustrations of bad chains. Basically that is just when we see that the probability region is not explored efficiently. 10.5.1 How many samples do you need? Terminalogy (with defaults): iter = total number of samples warmup = iter/2 Then how many samples do you need to estimate the posterior? It is all about the effective number of samples. If the chain is autocorrelated, then the n_eff will be lower than the number of iterations, although if it is anti-correlated, then you can have more effective samples than the number of iterations, that is merely due to the fact that some methods are able to outperform randomness of drawing samples. Now you must ask yourself, do you want to estimate something around the mean or toward the tails of a distribution. If towards the mean, then you dont need many samples (rule of thumb a 100 or a couple 100), but if you want to explore the tails, then you need a lot of samples, as these are more difficult to explore. Thus there is no good rule of thumb in general terms. Also the shape of the posterior distribution that you are trying to map is affecting, if it is simple, then it is easier to map than if it is complex. Notice that stan will let you know if it is uncertain around the tails, thus you need more samples. 10.5.2 How many chains do you need? Always start with one chain when building the model, as the stan will only return error messages and not just errors, when running 1 chain. Then run multiple independent chains to cross validate the chains and see how they converge. Typically 3 or 4 chains are sufficient. 10.5.3 Taming a wild chain Basically flat priors or almost flat priors will lead to crazy sampling, as it can explore any region, also totally unrealistic regions. Hence we want to regularize this. If you have no idea of the priors then go for weakly informative priors, instead of stupid priors, as the likelihood will always outperform these priors. 10.6 Exercises 10.6.1 E4 Explain the difference between the effective number of samples, n_eff as calculated by Stan, and the actual number of samples. n_eff is the effective number of independent samples, hence samples that are not autocorrelated. We see that very efficient samples can get more effective samples than the actual number of samples being drawn. 10.6.2 E5 Which value should Rhat approach, when a chain is sampling the posterior distribution correctly? It should approach 1. 10.6.3 E6 Sketch a good trace plot for a Markov chain, one that is effectively sampling from the posterior distribution. What is good about its shape? Then sketch a trace plot for a malfunctioning Markov chain. What about its shape indicates malfunction? You want it to be stationary and quickly explore the whole region. Notice that you should run multiple independent chains and then compare their progress, they should be similar otherwise there is something wrong, perhaps you need to regularize the model. 10.6.4 E7 The same as above, just with a good and a bad traceplot Bad example: We see that they are not overlapping at all. we want them to overlap (them = independent chains) also they must stay within the same area. Good example: Now we see an example where the chains visit the same regions and overlap each other. 10.6.5 M1 Reestimate the terrain ruggedness model from the chapter, but now using a uniform prior for the standard deviation, sigma. The uniform prior should be dunif(0,1). Use ulam to estimate the posterior. Does the different prior have any detectible influence on the posterior distribution of sigma? Why or why not? New sigma prior plot(dunif(1:10, min = -10, max = 10),type = &#39;l&#39;) ### Loading the data library(rethinking) data(rugged) d &lt;- rugged d$log_gdp &lt;- log(d$rgdppc_2000) dd &lt;- d[ complete.cases(d$rgdppc_2000) , ] dd$log_gdp_std &lt;- dd$log_gdp / mean(dd$log_gdp) dd$rugged_std &lt;- dd$rugged / max(dd$rugged) dd$cid &lt;- ifelse( dd$cont_africa==1 , 1 , 2 ) ### Arrange data in lists dat_slim &lt;- list( log_gdp_std = dd$log_gdp_std, rugged_std = dd$rugged_std, cid = as.integer(dd$cid) ) #str(dat_slim) ### Fitting the model m &lt;- ulam( alist( log_gdp_std ~ dnorm(mu,sigma), mu &lt;- a[cid] + b[cid] * (rugged_std - 0.215), a[cid] ~ dnorm(1,0.1), b[cid] ~ dnorm(0,0.3), sigma ~ dunif(0,1)#dexp(1), this is made uniform now ) ,data = dat_slim ,chains = 4 #no of independent chains to sample from ,cores = 4 ) precis(m,depth = 2) x 0.8873346 1.0504715 0.1345301 -0.1447311 0.1114339 x 0.0157199 0.0099947 0.0784333 0.0557697 0.0062897 x 0.8619283 1.0345449 0.0142203 -0.2330243 0.1021082 x 0.9120454 1.0666702 0.2618421 -0.0561387 0.1218582 x 2182.530 2780.618 2885.868 2678.708 1972.704 x 0.9999159 0.9991234 0.9987891 0.9994833 1.0022353 rethinking::pairs(m) traceplot(m) ## [1] 1000 ## [1] 1 ## [1] 1000 trankplot(m) It is difficult to see from the traceplot, as the y scale is so large, hence we inspect the trank plot. We see that sigma in the trank plot is controlled randomness, as it is stationary. Based on the n_eff, we also have an indication of a healthy chain. 10.6.6 H2 Recall the divorce rate example from Chapter 5. Repeat that analysis, using ulam this time, fitting models m5.1, m5.2, and m5.3. Use compare to compare the models on the basis of WAIC or PSIS. To use WAIC or PSIS with ulam, you need add the argument log_log=TRUE. Explain the model comparison results. #load data and copy library(rethinking) data(WaffleDivorce) d &lt;-WaffleDivorce # standardize variables d$D &lt;-standardize(d$Divorce) d$M &lt;-standardize(d$Marriage) d$A &lt;-standardize(d$MedianAgeMarriage) dl &lt;- list( D = d$D, M = d$M, A = d$A ) # Models m5.1 &lt;-ulam( alist( D ~ dnorm(mu,sigma), mu &lt;- a+bA*A, a ~ dnorm(0,0.2), bA ~ dnorm(0,0.5), sigma ~ dexp(1) ) ,data = dl ,chains = 4 ,cores = 4 ,log_lik = TRUE ) m5.2 &lt;- ulam( alist( D ~dnorm(mu,sigma), mu &lt;-a+bM*M, a ~dnorm(0,0.2), bM ~dnorm(0,0.5), sigma ~dexp(1) ) ,data = dl ,chains = 4 ,cores = 4 ,log_lik = TRUE ) m5.3 &lt;- ulam( alist( D ~dnorm(mu,sigma), mu &lt;-a+bM*M+bA*A, a ~dnorm(0,0.2), bM ~dnorm(0,0.5), bA ~dnorm(0,0.5), sigma ~dexp(1) ) ,data = dl ,chains = 4 ,cores = 4 ,log_lik = TRUE ) compare(m5.1,m5.2,m5.3,func = WAIC) x 125.7336 127.1738 139.1511 x 12.507739 12.584734 9.769107 x 0.000000 1.440259 13.417504 x NA 0.6498585 9.0641562 x 3.635466 4.531016 2.882316 x 0.6720839 0.3270960 0.0008201 We see that if we had to bet money on a model, then 73% of the time m5.1 will win. Although m5.1 and m5.3 is very close to each other. We can plot this, to get some more information. We see in the following that the difference between the two bebst models is not overlapping, there is evidance for m5.1 being the best model. plot(compare(m5.1,m5.2,m5.3,func = WAIC)) We can also infer m5.3 with precis: precis(m5.3) x 0.0008215 -0.0546710 -0.6016799 0.8259737 x 0.0999101 0.1527332 0.1521546 0.0853732 x -0.1587279 -0.2952619 -0.8421115 0.7040122 x 0.1638977 0.1896988 -0.3568247 0.9696448 x 1588.9394 1000.0775 1062.0161 874.7076 x 1.0027230 1.0005710 0.9996928 1.0006627 We see that marriage has a mean of -0.07 and the standard deviation is on both sides of 0, hence marriage does not include any information, as all information is in the age variable. 10.7 Lecture notes Why Markov Chains? We see that markov chains is not only used in bayesian data analysis, but also for frequentist statistics. Recap: Why cant we use quadratic approximation: it is effective for simple models, we see that with flat priors it is the same as classical statistics. Although, when it gets more complicated, this approach will not be feasibile. Case with Markov Chains: Will always find the posterior distribution after time. Although it only works on the long run. But the MCMC method may need to run in a long long long time, hence that is a drawback of MCMC. This is also called the metropolis algorithm. That is from the researcher Nicholas Metropolis, this originates in the research of making fusion bombs. What is a chain? It is a simulation of chained events. WHere Markov Chain is a sequential simulation, where you can go to different outcomes, based on the current state and not the passed states. Hence it has no memory, this is often an advantage. Monte Carlo is coming from the city, where people gamble, hence there is not too much about this. MCMC is an engine of making integration. Hence it takes something very hard make simplify it. Different strategies of MCMC: Metropolis Metropolis Hastings Gibbs sampling: an efficient version of bullet 2, although in a Silow dimensional setting Hamiltonian Monte Carlo (HMC): This does not guess and check. This is more efficient even with very complicated models. Metropolis and Gibbs: is a guess and check method. This is now a thing of the past This is an example, where MH sampling is not working well: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=RandomWalkMH&amp;target=banana The site also have different examples. Key tuning parameter is the step size, we see that the larger the step size, the more different places it will suggest. We see that the HMC runs a physical simulation, where it is trying to map the probability distribution. How: It is set randomly, and the follows the slope of the probability mass. Then it is being flicked, and then it follows the slope again. HMC has the following tuning parameters: Momentum, is random Step size, must be tuned. Adaptive speed, when it goes downwards, then the velocity is decreasing and when it goes upwards, then the velocity is decreasing. See a video on how this works here: https://chi-feng.github.io/mcmc-demo/app.html?algorithm=HamiltonianMC&amp;target=banana we see that the direction may be set, although the ‘gravitation’ will bend the direction. It is calculating the gradient in each point, to estimate the slope, thus it is mathematically more complicated, although it means that all proposals are acceptet. It will run for some fixed time! This is also a tuning parameter. This is called a leapfrog step. We define how many jumps the algorithm can take before the sample proposal is drawn. Hence HMC, is flicking the particle, that will roll according to the slope, where the momentum is random each time. We see that each step takes more time to compute in HMC compared to the random samplers, such as Gibbs. Although it requires way less observations. HMC is basically a gradient descent method to end in a region with a probability mass, where it should always tend towards the probability mass. HMC problems: The U-turn phenomenon: We see that we end up in the same place as we started in the right scenario. Try to follow this link:https://chi-feng.github.io/mcmc-demo/app.html?algorithm=NaiveNUTS&amp;target=banana, select the standard distribution and increase the leopfrog steps(about 65) . There is a NUTS (no u turn) method. This method is going back and forwards and seeing when it starts turning. Then it stops and takes a sample. This means that we dont need to define the no. of leapfrog steps, hence it is adaptive to where it starts. The problem of no u turn sampler, is if we have a multimodal distribution (multiple hills), it has a tendency to get stuck in one of the hills. As it needs enough speed to break lose the gravity around probability mass. For diagnosis, see the slides, there are some examples and notes. "],["big-entropy-and-the-generalized-linear-model.html", "11 Big Entropy and the Generalized Linear Model 11.1 Maximum Entropy 11.2 Generalized linear models 11.3 Maximum entropy priors", " 11 Big Entropy and the Generalized Linear Model 11.1 Maximum Entropy It can be beneficiary to find a distribution that can as many of the possible outcomes as possible. We call such distributions maximum entropy, as this is the distribution that can be found the most ways. Thus we want to pick the flattest distribution within the constraints that we are having. A constraint on the mean or the variance. Here are some examples of the outcome distribution (maximum entropy distribubtion) given the constraint: That also implies, that the maximum entropy distribution is the most conservative. What is our goal? Connect linear model to outcome variable. Our model is still geocentric Strategy: Pick an outcome distriubtion. Notice that you cannot look at the data before specifying the model. As we want to use our knowledge to build the model, hence we dont want to be perceived by the data. Always go for maximum entropy. The following are some good guidelines (in in the section 11.2.1): Distances and durations: exponential, gamma (survival or event history) Counts: Poisson, binomial, multinomial, geometric. Monsters: Ranks and ordered categories. Mixtures: Bbeta/binomial, gamma-Poisson, zero-inflated process, occupancy models. Model its parameters using links to linear models In a linear model, we see that the outcome is the same scale as the input, e.g., predicting height will still be height. Although if you want to connect a linear model to a probability of success (p), then it is unitless. Thus you may have an input of a count, but an output as a probability. Notice that in this case we substitute $\\mu$ with a $p$. As the outcome of a model will not be on the same scale, we need a function for getting the probability. This is where the link function comes in play. Hence we will have $f(p_i) = \\alpha + \\beta x_i$. Compute the posterior With a GLM it is harder to search for the parameters, OLS can be used, but for some reason it is not optimal. We will just use MCMC where we also can use priors. (quap works sometimes but not always, hence we will just rely on MCMC.) We can model multivariate relationships and non/linear responses Notice that this is the building blocks of a multilevel model. 11.1.1 Binomial Here are some examples in a binomial scenario: # build list of the candidate distributions p &lt;- list() p[[1]] &lt;- c(1/4,1/4,1/4,1/4) p[[2]] &lt;- c(2/6,1/6,1/6,2/6) p[[3]] &lt;- c(1/6,2/6,2/6,1/6) p[[4]] &lt;- c(1/8,4/8,2/8,1/8) par(mfrow = c(2,2)) for (i in 1:4) plot(p[[i]],type = &#39;b&#39;,ylim = c(0,1),pch = 20) %&gt;% grid() We see that we have four distributions, one with even outcomes possibilities, and three with different outcome possibilities. We can now calculate the entropy of each distritution. sapply(p,function(p) -sum(p*log(p))) ## [1] 1.386294 1.329661 1.329661 1.213008 par(mfrow = c(1,1)) sapply(p,function(p) -sum(p*log(p))) %&gt;% plot(type = &#39;l&#39;) We see that the entropy is decreasing as the distribution gets less uniform 11.1.1.1 Example from the lecture We see that the outcome scale will now impose interactions between the parameters no matter what you specify, as the outcome of a model is now condition. E.g., We see that the lizard can only die once and live once, the floor and ceiling effects will put the model output on a fixed scale, hence it does not matter how much we feed the lizard, if it is simply just too cold. 11.2 Generalized linear models When we apply the principle of maximum entropy, we see that we have dealt with unreal scenarios, while attempted to create a model, most outcomes. This results in generalized linear model. this looks the following: \\[y_i ~ Binomial(n,p_i)\\] \\[f(p_i) = \\alpha + \\beta(x_i-\\bar{x})\\] hence we see that \\(\\mu\\) is exchanged with an \\(f\\). This represents a link function. 11.2.1 Meet the family This section introduce some of the ddistributions from the exponential family, these are often widely used as they are all maximum entropy distributions given certain constraints. We see that these are also often used in traditional statistics, although we often arrive at these in different ways. Explanation: We see that the exponential distribution is the core of the family. Binomial distribution: is when we count events underlying the exp distribution. e.g., coin flips. We need n trials and p probability of a given event. Hence the expected outcome of a sequence of trials is then \\(n * p\\). Poisson: it is basically the binomial where the probability of a given event is very low. gamma: eg., lets say that you have a binomial event, that a washing machine is breaking. The probability of the machine breaking is in general low. But if we sum the probability of breaking over time, we will see that we get a gamma distribution. Hence the distribution is in this case reflecting the waiting time. If the mean is large, then it is a gaussian distribution. Gaussian: the gamma but with a large mean. conclusion: we see that max entropy leads to a distribution that will explain the problem at hand given the constraints, although this family of distributions are related, which is the key takeaway. 11.2.2 Linking linear models to distributions First, lets clarify the purpose of the link function. It is effectively to map a linear function onto a non linear space of a given parameter (\\(\\theta\\)). In general, we are having two link functions: logit link (log-odds). It looks the following: \\[ y_i \\sim Binomial(n,p_i) \\\\ logit(p_i) = \\alpha + \\beta x_i \\] We see that the value is now linear to the logit (log-odds) of p. Hence we see that we take the probability p and map it onto the log-odds scale. Hence the model may output the log-odds, but the logit function, can map the log-odds onto the probability scale. One can then take the inverse-link of the function, that is called the logistic. Summary: log-odds of the log of the odds. We can get back to probabilities, by using the inverse link on the log-odds. log link. Log-odds scale vs. the probability scale: We see that log-odds = 0, is 50% probability, while log-odds = 3 is 95% probability. Notice it is symmetric, log-odds of -3 is 5% of the time. This is important for defining priors. The log-odds scale goes from negative to positive infinity. The logit link: We see that the logit link is able to transform the log-odds of the model into probability, meaning we go from a linear model to map this on a y range between 0 and 1, this is typically also called a logistic function. It can look as the following: what are the odds of an event? It is merely the probability of an event happening divided by the probability of the event not happening. Thus this can be written with: \\[ log\\frac{p_i}{1-p_i} = \\alpha + \\beta x_i \\] , where $p_i$ = \\(p_i=\\frac{exp(\\alpha + \\beta x_i)}{1 + exp(\\alpha + \\beta x_i)}\\), this is also called the logistic or the inverse-logit. The log link: Basically this is a link function that is applied whenever an exponential relationship is being inferred / predicted. It takes the following shape: Keep in mind that the link functions are assumptions, if they do not work well, then try other methods. 11.3 Maximum entropy priors This is explained in the earlier sections of the chapter, the following image summarize it. It is basically about selecting a prior that captures the distribution that can be constructed the most possible ways given the constraints. "],["god-spiked-the-integers.html", "12 God Spiked the Integers 12.1 Binomial Regression 12.2 Poisson Regression 12.3 Multinomial Regression 12.4 Exercies", " 12 God Spiked the Integers The following sections makes examples using a binomial outcome and exponential outcome (poisson). Also the last section is about a multinomial problem, where there is more than two outcomes. In general it is very good practice not to create proportions before running the model, as the counts indicate the magnitude of the model, while the proportions will not reflect such. 12.1 Binomial Regression Binomial regression is when you have an outcome variable with two outcomes, e.g., yes/no, reject/accept etc. We see that it takes the following formula: \\[ y \\sim Binomial(n,p) \\] Where y is a count. P = a probability of a certain trial success and n is the number of trials. 12.1.0.1 Logistic regression: Prosocial chimpanzees: In the following an example with chimpanzees is presented. In real life they had a table with a chimpanzee in one end, and then some food on a table. This food could be delivered in each ends of the table, depending on what level a chimpanzee pulls. The trick is then for every second chimpanzee there will be another chimpanzee in the other end of the table. Hence they want to investigate if a chimpanzee is more likely to get food for both ends of the table, if there is another chimpanzee present. Prosocial = is about caring, hence if there is a partnering chimp. A proscial option is when there is food on the other side of the table, that can be send to the other chimpanzee. (opposite of prosocail = asocial) Notice that one should account for the handedness of a chimpanzee (left or right), as that may be a backdoor if we were to draw the DAG. We will create a model with a binomial distribution explaining the outcome variable (if one or the other lever is pulled). Then we will have a logit function (logistic) explaining the probably outcome (likelihood) and we will include two parameters, an intercept (\\(\\alpha\\)) and a coefficient (\\(\\beta\\)). There will be one (\\(\\alpha\\)) for each unique combination of outcomes. Notice that we do not use dummy variables, but instead an index to model the interaction. For explanation of this, it is referred to CH5. #Loading data 11.1 library(rethinking) data(chimpanzees) d &lt;- chimpanzees #Create condition, treatment 1, 2, 3 and 4. see p. 326 d$treatment &lt;- 1 + d$prosoc_left + 2*d$condition xtabs( ~ treatment + prosoc_left + condition , d ) #if condition = 1, then partner present ## , , condition = 0 ## ## prosoc_left ## treatment 0 1 ## 1 126 0 ## 2 0 126 ## 3 0 0 ## 4 0 0 ## ## , , condition = 1 ## ## prosoc_left ## treatment 0 1 ## 1 0 0 ## 2 0 0 ## 3 126 0 ## 4 0 126 Now we see the different outcomes given the condition (if there is a partner present). Notice that the \\(\\alpha\\) prior is based on log-odds. We see that the log odds larger or smaller than 4 and -4 means great certainty on 0 or 1, see the picture of this in a earlier section. Hence in the following example, we see that the log-odds spans all the way up to &gt;15, hence it is simply just var too extreme, hence it says that pulling the left lever is either always or never happens. Hence when we transform to the probability scale, it will be either 0 or 1 (more or less). This is a horrible priors. The following is just an example prior, which is almost flat, although the outcome leads to very non flat results, as we break the usual scale of the log-odds. m11.1 &lt;- quap( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a , a ~ dnorm( 0 , 10 ) #Mean = 0, as log-odds is centered at 0. sd of 10 is very wide. ) , data=d ) #Sampling from the prior set.seed(1999) prior &lt;- extract.prior(m11.1,n = 1e4) #Using inverse-link function to convert the parameter to the outcome scale p &lt;- inv_logit(prior$a) dens(p,adj = 0.1) Se see that this prior will lead to great certainty in pulling the left or the right lever. #Prior plot, plotting prior x &lt;- seq(-5, 5, length=100) hx &lt;- dnorm(x,mean = 0,sd = 10) plot(x, hx, type=&quot;l&quot;, lty=2, xlab=&quot;x value&quot;, ylab=&quot;Density&quot;, main=&quot;Comparison of prior Distributions&quot;) We see that the prior is very wide and despite a bad prior, we get very certain outcomes of 0 or 1. We will go for an sd of 1.5. We will also set the beta prior for dnorm of mean of 0 and standard deviation of 0.5. In the book they also examplifies with beta = dnorm(0,10), this is equally bad, hence it is regularized. m11.3 &lt;- quap( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a + b[treatment] , a ~ dnorm( 0 , 1.5 ), b[treatment] ~ dnorm( 0 , 0.5 ) #the treatment paramtere ) ,data=d ) set.seed(1999) prior &lt;- extract.prior( m11.3 , n=1e4 ) #Get 4 vectors of samples based on the priors p &lt;- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) ) #Compute the difference between two vectors mean( abs( p[,1] - p[,2] ) ) ## [1] 0.09838663 Now we see that on average, there is 10% difference. From this we can tell that the samples deviate, but not very much. We want to limit the model from modeling unnatural outcomes, while actually being able to adapt. Hence we want the model to be skeptical of large differences. Now we can estimate the mode using Hamiltonian Monte Carlo #Estimating the posterior distribution using Hamiltonian Monte Carlo # prior trimmed data list dat_list &lt;- list( pulled_left = d$pulled_left, actor = d$actor, treatment = as.integer(d$treatment) ) # Constructing the model 11.11 m11.4 &lt;- ulam( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a[actor] + b[treatment] , #actor = chimp, treatment = the experiment setup a[actor] ~ dnorm( 0 , 1.5 ), b[treatment] ~ dnorm( 0 , 0.5 ) ) ,data=dat_list , chains=4 , log_lik=TRUE ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;82480dff1a626a42c2ca9de938d65b9d&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000109 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.09 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.341279 seconds (Warm-up) ## Chain 1: 0.306452 seconds (Sampling) ## Chain 1: 0.647731 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;82480dff1a626a42c2ca9de938d65b9d&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5.3e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.53 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.339352 seconds (Warm-up) ## Chain 2: 0.267315 seconds (Sampling) ## Chain 2: 0.606667 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;82480dff1a626a42c2ca9de938d65b9d&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5.2e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.52 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.377494 seconds (Warm-up) ## Chain 3: 0.280099 seconds (Sampling) ## Chain 3: 0.657593 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;82480dff1a626a42c2ca9de938d65b9d&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 5.5e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.55 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.353908 seconds (Warm-up) ## Chain 4: 0.340683 seconds (Sampling) ## Chain 4: 0.694591 seconds (Total) ## Chain 4: # OUTPUT = mean = log-odds of outcomes for each parameter. precis( m11.4 , depth=2 ) x -0.4691437 3.8591587 -0.7731752 -0.7801181 -0.4733026 0.4574861 1.9302531 -0.0126179 0.4963272 -0.3631956 0.3859976 x 0.3355974 0.7245205 0.3424777 0.3415426 0.3374260 0.3361617 0.4328352 0.2913041 0.2919672 0.2958807 0.2874828 x -1.0202393 2.7666512 -1.3319969 -1.3300130 -1.0240518 -0.0677018 1.2805477 -0.4757591 0.0392419 -0.8341717 -0.0661710 x 0.0720931 5.0802776 -0.2386032 -0.2413658 0.0680157 1.0099898 2.6511620 0.4591869 0.9630114 0.1006613 0.8474066 x 705.8014 1253.8186 678.3468 725.1492 697.1381 694.0357 823.7525 620.2070 697.4281 594.8777 653.0517 x 1.005217 1.000421 1.004455 1.003907 1.002598 1.002623 1.001609 1.002723 1.002708 1.004414 1.004958 Thus, we see a bunch of different alpha values and the four different beta values (one for each treatment). Interpretation: Alpha = for each chimp. Alpha mean = the log-odds of pulling the left lever (success = left lever pull) When alpha &gt; 0 we know that the probability is &gt;50%, we have three chimps with positive numbers. where it is already clear that chimp 2 almost always pulled the left lever (i think it was a lefty chimp). In general we see that there is a tendency for right handedness, as most chimps have a preference of the right lever (alpha mean (log odds) less than 50%) beta = for each type of treatment. beta mean = the log-odds of pulling the left lever given the treatment. We see that it typically spans in the negative and positive region, hence an indication of the treatment not having a great effect. more about this in the following. Let us take a look of the tendency of pulling the LEFT lever. In general what we see is the preference for left and right handedness for each chimp. This is the reason for controlling for each actor (chimp) post &lt;- extract.samples(m11.4) p_left &lt;- inv_logit( post$a ) plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) ) We see that that chimpanzee 1, 3, 4, 5 have a preference to pull the right lever, that is because the probability of pulling the left lever is below 50%. Chimpanzee no. 6 is a bit on both sides, while chimpanzee 2 and 7 tends to pull the left lever. Let us inspect the actual cases for chimpanzee no. 2. d[d$actor == 2,&quot;pulled_left&quot;] %&gt;% table() ## . ## 1 ## 72 We see that this guy in fact only pulled the left lever, hence the great certainty. Now we want to measure the treatment effect. We can derive the effect from the \\(\\beta\\) coefficient. labs &lt;- c(&quot;R/N&quot;,&quot;L/N&quot;,&quot;R/P&quot;,&quot;L/P&quot;) #R = right, L = Left, P = Partner, N = No partner {plot( precis( m11.4 , depth=2 , pars=&quot;b&quot; ) , labels=labs) mtext(&quot;R = Right, L = Left, N = no partner, P = Partner&quot;)} It already seems as if they are overlapping. But what is really interesting is the difference between the scenarios. This we inspect in the following: diffs &lt;- list( db13 = post$b[,1] - post$b[,3], #No partner / partner treatment db24 = post$b[,2] - post$b[,4] ) plot( precis(diffs) ) We see some tendencies, although as both ranges on both sides of 0, there is no strong evidance of a pattern. In the book they also compare the posterior predictions compared with the osberved events. This will not be shown. Ultimately it also compares the model with a non interaction model (index for oth chimpanzee and treatment). So a modell looks the following: #Creating data for condition and pulled lever d$side &lt;- d$prosoc_left + 1 # right 1, left 2 d$cond &lt;- d$condition + 1 # no partner 1, partner 2 #Specify model dat_list2 &lt;- list( pulled_left = d$pulled_left, actor = d$actor, side = d$side, cond = d$cond ) m11.5 &lt;- ulam( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a[actor] + bs[side] + bc[cond] , a[actor] ~ dnorm( 0 , 1.5 ), bs[side] ~ dnorm( 0 , 0.5 ), bc[cond] ~ dnorm( 0 , 0.5 ) ) , data=dat_list2 , chains=4 , log_lik=TRUE #Keep log_lik for comparabillity ) ## ## SAMPLING FOR MODEL &#39;8cec853a8473faa0bc2e49bbbdc68fa7&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 8.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.89 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.740642 seconds (Warm-up) ## Chain 1: 0.618738 seconds (Sampling) ## Chain 1: 1.35938 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;8cec853a8473faa0bc2e49bbbdc68fa7&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 6.2e-05 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.62 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.701595 seconds (Warm-up) ## Chain 2: 0.611112 seconds (Sampling) ## Chain 2: 1.31271 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;8cec853a8473faa0bc2e49bbbdc68fa7&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 6.9e-05 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.69 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.720881 seconds (Warm-up) ## Chain 3: 0.617073 seconds (Sampling) ## Chain 3: 1.33795 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;8cec853a8473faa0bc2e49bbbdc68fa7&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 6.1e-05 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.61 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.677613 seconds (Warm-up) ## Chain 4: 0.671734 seconds (Sampling) ## Chain 4: 1.34935 seconds (Total) ## Chain 4: #compare the model with the previous model: compare( m11.5 , m11.4 , func=PSIS ) x 531.3755 532.2422 x 19.15631 18.95302 x 0.0000000 0.8667209 x NA 1.393694 x 8.016393 8.460029 x 0.6066758 0.3933242 We see that the simpler model is working just as fine. That was expected as we did not discover any effects in the original model, hence there should not be one either in the simpler model. 12.1.0.2 Relative shark and absolute deer Notice that logistic regression should be seen as measuring relative effects, this is the relative change in odds of an outcome. We can calculate the proportional odds by saying: case we switch from a scenario with no partner (2) and to a scenario where we have a partner (4). post &lt;- extract.samples(m11.4) mean(exp(post$b[,4] - post$b[,2])) ## [1] 0.9299786 # 2 = two food items and no partner # 4 = two food items and a partner 0.93 = 7% reduction of odds of the monkey pulling the left lever (0.93 % relative chance of success (pulling left lever)). Thus notice that the realtive scale is perceptive, as change from one category to another, e.g., from the chimpanzee example from one treatment to another, may seem large on a relative scale, although in absolute number, you may be working on a very small magnitude. Example: lung cancer is rare and we see that smoking increases your odds of getting lung cancer by three. lung cancer is still rare, although a third of lung cancer patients just smoked. Another example from the lecture: 12.1.0.3 Aggregated binomial: Chimpanzees again, condensed. This is the same example just as the model above, here the data is basically just aggregated, so we have one row for each chimpanzee and not one for each experiment. Now the model is just specified with: \\[ leftpuls \\sim Binomial(18,p) \\] As \\(Binomial(N,p)\\) and N = number of osbervations, as we have 18 experiments per chimpanzee, we will use N = 18. Notice that N can only be fixed, if there is an equal number of observations pr. row in the underlying data for the aggregated data (the following section deals with this). m11.6 &lt;- ulam( alist( left_pulls ~ dbinom( 18 , p ) , #18 instead of 1 logit(p) &lt;- a[actor] + b[treatment] , a[actor] ~ dnorm( 0 , 1.5 ) , b[treatment] ~ dnorm( 0 , 0.5 ) ) , data=dat , chains=4 , log_lik=TRUE ) For the rest of the example, I refer to the book. the only difference we see, is that the PSIS is a bit different (due to a bit different parametization), although the model performs equally well. 12.1.0.4 Aggregated binomial: Graduate school admissions In this example we see that there is not an equal number of observations per aggregation category. The following example show two different models: One without index variable for department One with index variable for department, to adjust for the mediating effect the department has on the DAG G = Gender, D = Department, A = Acceptance #Loading the data library(rethinking) data(UCBadmit) d &lt;- UCBadmit d dept applicant.gender admit reject applications A male 512 313 825 A female 89 19 108 B male 353 207 560 B female 17 8 25 C male 120 205 325 C female 202 391 593 D male 138 279 417 D female 131 244 375 E male 53 138 191 E female 94 299 393 F male 22 351 373 F female 24 317 341 We see that there is different number of applications for each department for both males and females. We deal with this, merely by inserting a variable in N’s place in the equation. dat_list &lt;- list( admit = d$admit, applications = d$applications, gid = ifelse( d$applicant.gender==&quot;male&quot; , 1 , 2 ) ) m11.7 &lt;- ulam( alist( admit ~ dbinom( applications , p ) , logit(p) &lt;- a[gid] , #index for gender a[gid] ~ dnorm( 0 , 1.5 ) ) ,data=dat_list , chains=4 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;92bf1800293ab05bf80648b931ab8346&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.2 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.010075 seconds (Warm-up) ## Chain 1: 0.008091 seconds (Sampling) ## Chain 1: 0.018166 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;92bf1800293ab05bf80648b931ab8346&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.009735 seconds (Warm-up) ## Chain 2: 0.007687 seconds (Sampling) ## Chain 2: 0.017422 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;92bf1800293ab05bf80648b931ab8346&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.008559 seconds (Warm-up) ## Chain 3: 0.009595 seconds (Sampling) ## Chain 3: 0.018154 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;92bf1800293ab05bf80648b931ab8346&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 6e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.009241 seconds (Warm-up) ## Chain 4: 0.00917 seconds (Sampling) ## Chain 4: 0.018411 seconds (Total) ## Chain 4: precis( m11.7 , depth=2 ) #Output mean ) log odds x -0.2202347 -0.8299381 x 0.0392477 0.0507154 x -0.2803993 -0.9096241 x -0.1554518 -0.7485576 x 1366.756 1533.751 x 1.0001290 0.9983079 We see the log odds, recall that 0 = &lt;50% probability, females look to be more certainly not being admitted. We see from the first example, that females (2) appear to be accepted less than males. Although how much higher? We can show the contrast on the aboslute and relative scale. #Compute the difference in probability measures. post &lt;- extract.samples(m11.7) diff_a &lt;- post$a[,1] - post$a[,2] #Going from log odds to probability diff_p &lt;- inv_logit(post$a[,1]) - inv_logit(post$a[,2]) precis( list( diff_a=diff_a , diff_p=diff_p ) ) x 0.6097035 0.1414188 x 0.0652006 0.0147087 x 0.5065589 0.1180769 x 0.7157854 0.1649805 x ▁▁▁▃▇▇▅▂▁▁ ▁▁▂▃▇▇▅▂▁▁▁ On a probability scale there is somewhere between 0.12 and 0.16 probability larger difference for males. Hence either there is huge discrimination going on or there is something lurking in the data. We see that based on the difference of a, we see that there is certainly a somewhat difference. This we can also interpret visually. postcheck( m11.7 ) # draw lines connecting points from same dept for ( i in 1:6 ) { x &lt;- 1 + 2*(i-1) y1 &lt;- d$admit[x]/d$applications[x] y2 &lt;- d$admit[x+1]/d$applications[x+1] lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 ) text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 ) } (#fig:11.31)Blue = obbserved proportions in raw data, and the black points are the expected difference. We are very much off here. Notice that the first dot on the line is male, and the second dot is female. We see in the first case, there is a larger posterior prediction for males compared to females, although it is observed that in real life, the actual prob of admission is greater for females. We do in fact see that females are admitted more in all but 2 departments. Thus there is something totally off here. So what is happening? Lets look at the DAG. On the right we have the suggested adjusted model, to include the departments, hence the effect within each department instead of across all departments. We see that there is a backdoor to admission, hence we must include department in the model. But did the model do anything wrong? no, we just told it that it can calculate log-odds across departments, but now it is clear that we must direct the model elsewise. Now we see that we are very much off. Hence we can try to include the department into the model. Lets add an index variable for department dat_list$dept_id &lt;- rep(1:6,each=2) m11.8 &lt;- ulam( alist( admit ~ dbinom( applications , p ) , logit(p) &lt;- a[gid] + delta[dept_id] , #Added index for department a[gid] ~ dnorm( 0 , 1.5 ) , delta[dept_id] ~ dnorm( 0 , 1.5 ) ) , data=dat_list , chains=4 , iter=4000 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 ## ## SAMPLING FOR MODEL &#39;eefef2dd38cd530185967a7d0d74fe47&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.7e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.17 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 1: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 1: Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 1: Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 1: Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1: Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1: Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 1: Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 1: Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 1: Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 1: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.21787 seconds (Warm-up) ## Chain 1: 0.205867 seconds (Sampling) ## Chain 1: 0.423737 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;eefef2dd38cd530185967a7d0d74fe47&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 8e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 2: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 2: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 2: Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 2: Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 2: Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 2: Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 2: Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 2: Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 2: Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 2: Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 2: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.220694 seconds (Warm-up) ## Chain 2: 0.212825 seconds (Sampling) ## Chain 2: 0.433519 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;eefef2dd38cd530185967a7d0d74fe47&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 8e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 3: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 3: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 3: Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 3: Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 3: Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 3: Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 3: Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 3: Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 3: Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 3: Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 3: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.21909 seconds (Warm-up) ## Chain 3: 0.202852 seconds (Sampling) ## Chain 3: 0.421942 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;eefef2dd38cd530185967a7d0d74fe47&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 7e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 4: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 4: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 4: Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 4: Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 4: Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 4: Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 4: Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 4: Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 4: Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 4: Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 4: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.23273 seconds (Warm-up) ## Chain 4: 0.202543 seconds (Sampling) ## Chain 4: 0.435273 seconds (Total) ## Chain 4: # Precis where we include department precis( m11.8 , depth=2 ) x -0.5060728 -0.4090259 1.0871730 1.0420536 -0.1738616 -0.2056515 -0.6477765 -2.2064013 x 0.5231841 0.5232166 0.5252742 0.5288346 0.5259279 0.5257558 0.5272430 0.5367216 x -1.3076943 -1.2178163 0.2083305 0.1580663 -1.0557751 -1.0891502 -1.5475111 -3.1120745 x 0.3726761 0.4696824 1.8919884 1.8625786 0.6327715 0.6031325 0.1757279 -1.3620805 x 568.2421 566.8179 571.2311 581.8204 568.1861 576.9245 566.3875 584.7413 x 1.014264 1.014132 1.014131 1.014501 1.014180 1.013421 1.014135 1.014613 First of all we see that the means are very similar now (to each other, male is lower), where the model also accounts for each department. And we also see that each department is very different in the effect that it has on acceptance rate. We also see that some department (most) tend to admit more females than males, by looking within each department. Now we can estimate the differences as before: post &lt;- extract.samples(m11.8) diff_a &lt;- post$a[,1] - post$a[,2] #on the relative scale diff_p &lt;- inv_logit(post$a[,1]) - inv_logit(post$a[,2]) #on the absolute scale (probability scale) precis( list( diff_a=diff_a , diff_p=diff_p ) ) x -0.0970469 -0.0217861 x 0.0805260 0.0184385 x -0.2242904 -0.0516160 x 0.0314741 0.0069762 x ▁▁▁▁▂▅▇▇▅▂▁▁▁▁ ▁▁▂▇▇▂▁▁ We see that on average the difference is -0.1 and the intervals are mostly negative. On the probability scale, we see that there is a negative difference, hence a small disadvantage for males. Although we see that the probability difference is very close to zero, it is likely that the difference between males and females is very small. We see that the difference is much smaller now where we account for the department which is a mediator effect that we see in the DAG in the beginning for the section. 12.2 Poisson Regression Sometimes there will be an exponential effect in the data. This we will not be able to model using a binomial approach, hence we need the poisson distribution. It looks the following: \\[ y_i \\sim Poisson(\\lambda) \\] Where lambda is the expected value of outomce y, i.e., expected variance of the counts y. When going for poisson regression one will typically also go for the log link function, which is a new link function, it looks the following \\[ y_i \\sim Poisson(\\lambda_i) \\] \\[ log(\\lambda_i)= \\alpha + \\beta(x_i-\\bar{x}) \\] The log link ensures that \\(\\lambda_i\\) is always positive. 12.2.0.1 Negative binomial (gamma-poisson) models. It is common practice for some reason to swap the Poisson distribution with a negative binomial distribution, also sometimes called a gamma-poisson distribution. It is basically a mixture of different poisson distributions. 12.3 Multinomial Regression In a binomial setting you will have two different outcomes, although that is not always the case, hence you must apply models that can take on multiple categories. Such models are called multinomial regression. The output is a multinomial logit, i.e., the softmax. They present some different approaches also in stan. I will not replicate the stan code. The example is with admission rates for UC Berkely. # binomial model of overall admission probability m_binom &lt;- quap( alist( admit ~ dbinom(applications,p), logit(p) &lt;- a, a ~ dnorm( 0 , 1.5 ) ) ,data=d ) # Poisson model of overall admission rate and rejection rate # &#39;reject&#39; is a reserved word in Stan, cannot use as variable name dat &lt;- list( admit=d$admit , rej=d$reject ) m_pois &lt;- ulam( alist( admit ~ dpois(lambda1), rej ~ dpois(lambda2), log(lambda1) &lt;- a1, log(lambda2) &lt;- a2, c(a1,a2) ~ dnorm(0,1.5) ), data=dat , chains=3 , cores=3 ) binomial probability of admission across the entire data set. #Binomial model inference inv_logit(coef(m_binom)) ## a ## 0.3878044 We see the probability of a female being accepted is 38%. #log odds for binomial model precis(m_binom) x -0.4565506 x 0.0305001 x -0.5052957 x -0.4078055 We can see the same for the poisson model: k &lt;- coef(m_pois) a1 &lt;- k[&#39;a1&#39;] a2 &lt;- k[&#39;a2&#39;] exp(a1)/(exp(a1)+exp(a2)) ## a1 ## 0.3873817 We see that the result is very similar. #log-rate for the poisson precis(m_pois) x 5.439913 4.982862 x 0.0196338 0.0238672 x 5.408989 4.944060 x 5.470680 5.019213 x 1115.3278 921.2894 x 0.9992379 1.0016325 12.3.1 Example from the lecture Relationship between tools, population and contact with the world. library(rethinking) data(Kline) d &lt;- Kline d$P &lt;- scale( log(d$population) ) d$contact_id &lt;- ifelse( d$contact==&quot;high&quot; , 2 , 1 ) dat &lt;- list( T1 = d$total_tools , #I named it T1, as T is reserved for TRUE P = d$P , cid = d$contact_id ) # intercept only m11.9 &lt;- ulam( alist( T1 ~ dpois( lambda ), log(lambda) &lt;- a, a ~ dnorm(3,0.5) ) , data=dat , chains=4 , log_lik=TRUE ) #Log like to be able to compare ## ## SAMPLING FOR MODEL &#39;2a800616debdca1af3c840a8294e1e8c&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.3e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.004463 seconds (Warm-up) ## Chain 1: 0.004593 seconds (Sampling) ## Chain 1: 0.009056 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;2a800616debdca1af3c840a8294e1e8c&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 5e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.004833 seconds (Warm-up) ## Chain 2: 0.004977 seconds (Sampling) ## Chain 2: 0.00981 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;2a800616debdca1af3c840a8294e1e8c&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.004866 seconds (Warm-up) ## Chain 3: 0.004629 seconds (Sampling) ## Chain 3: 0.009495 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;2a800616debdca1af3c840a8294e1e8c&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 5e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.004712 seconds (Warm-up) ## Chain 4: 0.004312 seconds (Sampling) ## Chain 4: 0.009024 seconds (Total) ## Chain 4: # interaction model m11.10 &lt;- ulam( alist( T1 ~ dpois( lambda ), log(lambda) &lt;- a[cid] + b[cid]*P, a[cid] ~ dnorm( 3 , 0.5 ), b[cid] ~ dnorm( 0 , 0.2 ) ), data=dat , chains=4 , log_lik=TRUE ) #Log like to be able to compare ## ## SAMPLING FOR MODEL &#39;5aa61820e8e2a3b5e20709d24f8ee9f2&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.8e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.18 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.014867 seconds (Warm-up) ## Chain 1: 0.01194 seconds (Sampling) ## Chain 1: 0.026807 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;5aa61820e8e2a3b5e20709d24f8ee9f2&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 7e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.016762 seconds (Warm-up) ## Chain 2: 0.01379 seconds (Sampling) ## Chain 2: 0.030552 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;5aa61820e8e2a3b5e20709d24f8ee9f2&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 7e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.07 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.016266 seconds (Warm-up) ## Chain 3: 0.015227 seconds (Sampling) ## Chain 3: 0.031493 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;5aa61820e8e2a3b5e20709d24f8ee9f2&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 8e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.016892 seconds (Warm-up) ## Chain 4: 0.014223 seconds (Sampling) ## Chain 4: 0.031115 seconds (Total) ## Chain 4: #Lets compare these, pLOO = effective number of parameters #compare( m11.9 , m11.10 , func=PSIS ) compare( m11.9 , m11.10 , func=LOO ) #Should return the pLOO x 85.56739 141.92082 x 13.34944 33.87679 x 0.00000 56.35343 x NA 33.18588 x 7.140887 8.514346 x 1 0 We see from this model, that the effective number of parameters (pLOO), hence the intuitively more complicated model is less prone to overfitting, due to the lower effective number of parameters. The following plots represent the posterior predictions par(mfrow = c(1,2)) k &lt;- PSIS( m11.10 , pointwise=TRUE )$k plot( dat$P , dat$T , xlab=&quot;log population (std)&quot; , ylab=&quot;total tools&quot; , col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 , ylim=c(0,75) , cex=1+normalize(k) ) # set up the horizontal axis values to compute predictions at ns &lt;- 100 P_seq &lt;- seq( from=-1.4 , to=3 , length.out=ns ) # predictions for cid=1 (low contact) lambda &lt;- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) ) lmu &lt;- apply( lambda , 2 , mean ) lci &lt;- apply( lambda , 2 , PI ) lines( P_seq , lmu , lty=2 , lwd=1.5 ) shade( lci , P_seq , xpd=TRUE ) # predictions for cid=2 (high contact) lambda &lt;- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) ) lmu &lt;- apply( lambda , 2 , mean ) lci &lt;- apply( lambda , 2 , PI ) lines( P_seq , lmu , lty=1 , lwd=1.5 ) shade( lci , P_seq , xpd=TRUE ) # code chunk 11.48 plot( d$population , d$total_tools , xlab=&quot;population&quot; , ylab=&quot;total tools&quot; , col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 , ylim=c(0,75) , cex=1+normalize(k)) ns &lt;- 100 P_seq &lt;- seq( from=-5 , to=3 , length.out=ns ) # 1.53 is sd of log(population) # 9 is mean of log(population) pop_seq &lt;- exp( P_seq*1.53 + 9 ) lambda &lt;- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) ) lmu &lt;- apply( lambda , 2 , mean ) lci &lt;- apply( lambda , 2 , PI ) lines( pop_seq , lmu , lty=2 , lwd=1.5 ) shade( lci , pop_seq , xpd=TRUE ) lambda &lt;- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) ) lmu &lt;- apply( lambda , 2 , mean ) lci &lt;- apply( lambda , 2 , PI ) lines( pop_seq , lmu , lty=1 , lwd=1.5 ) shade( lci , pop_seq , xpd=TRUE ) (#fig:11.47_48)Dashed = low contact and solid line = high contact 12.4 Exercies 12.4.1 E1 12.4.2 E2 12.4.3 E3 12.4.4 M7 12.4.5 H1 12.4.6 H2 12.4.7 H3 "],["ch-13-models-with-memory.html", "13 Ch 13 - Models with Memory 13.1 Example: Multilevel tadpoles 13.2 Varying effects and the underfitting / overfitting trade-off 13.3 More than one type of cluster 13.4 Divergent transitions and non-centered priors 13.5 Multilevel posterior predictions 13.6 Exercises", " 13 Ch 13 - Models with Memory The multilevels is able to contain more knowledge as we not only having a category or index for each category/cluster in the data, but we will have a model, in which parameters can be defined. This is often beneficiary if you have a model, where you need to interpret the outcome for different levels, i.e., clusters in the data. In the chapter he use the chimpanzee data again, where we will interpret the preference for the left or right lever depending on the unique actor (chimpanzee) and the treatment, thus we will again obtain a model with parameters for each chimp and this is based on priors that are specific for the different chimpanzees. The purpose of the chapter is to introduce the multi level models, here we modify the mathematical models, where the distribution of \\(\\alpha\\) now can be defined by other paramereters, namely hyperparameters, the distribution for the hyperparameters is called hyperpriors. It is said that multilevel models has memory, that originates in the fitting procedure where we normally will update the whole model as new data / observations are introduced, hence it will simply just disregard previous information. In the multilevel models, we are able to tweak the model seperately, as we have a specific prior for each cluster, hence when we observe one cluster, the prior for this is updated and when we look at another, we (the model) will tweak another prior. Hence by separating this, recall the café visiting robot that measures waiting times. The traditional robot would always update the one waiting time prior for each café, but the smarter robot will have a prior for each café and then monitor waiting times in one parameter per café (cluster). More on clusters: these are individuals or observations from the same population that share the same characteristics, but you want to observe where these individuals differ. Three drawbacks (i.e. costs): Define some distributions from where clusters form There are some challneges with estimation THey are hard to understand as they make prediction on different levels of the data. 13.1 Example: Multilevel tadpoles The outcome of this example is that the multilevel model is able to extract more information from the data. Although it is still a bit abstract. We had parameters for the parameters (namely hyperparameters) and we now got multiple intercepts. In this example we have different tanks with different number of tadpoles in each tank (density). We want to measure how many tadpoles that survives in each tank. Thus we can fit two different models: Dummy variable for each tank. Using index for each tank. Thus each alpha is assigned the same prior, although recall that from the precis, we will still have an intercept for each tank, it is just not fitted as they are different tanks. Multilevel model with varying intercepts by tank, thus we will have a prior for each tank, hence can account for survival defaults for each particular tank. Continouing on the multilevel model: We will see that there is added priors inside the priors (which makes it multilevel) We are now able to learn the data from the inside of the data. We see that we have have two levels in the model: Priors for the binomial distribution Priors for the parameter alpha. We see that ulam will all be updated simultaniously during the markov chain sampling. Comparing the performance of the fixed model and the multilevel model we see that the latter is the best and it even has less effective parameters (pWAIC), despite initially more paramters. Thus the model is less flexible and fits better. The reason is that we are now having better priors. 13.2 Varying effects and the underfitting / overfitting trade-off 13.2.1 The model In the examples that we have seen we introduce \\(\\bar{\\alpha}\\) and \\(\\sigma\\) . Sigma is in these cases reflecting the difference that we assume the cluster will have. Hence if sigma = 0, then we say that all clusters are effectively the same (complete pooling) or if we on the other hand set sigma = positive infinity, we will say that the cluster are entirely different, hence we will not pool information (share information). We want to set a prior that is between 0 and positive infinity (which is why exp(1) is neat), as this will enable partial pooling and hence we acknowledge cluster being similar but different. This leads us back to \\(\\bar{\\alpha}\\), being the mean across all groups and ultimately the mean of the specific clusters (groups) can be defined by the combination of the overall mean and the standard deviation we define under \\(\\sigma\\). Thus, having sigma 0 will lead to total underfit, and a very high sigma will lead to a total overfit. we are dancing in the middle of total underfitting and total overfitting. Ultimately this is about twisting the last information out of the data. 13.2.2 Assign values to the parameters 13.2.3 Simulate survivors Logit link function, thus the probability is defined by the logistic function. 13.3 More than one type of cluster This is an example with the chimpanzee prosocial example (if they select the option benefiting other chimps in the experiment). Here we will see a cluster for each actor and block. Notice that actor = chimpanzee, and block = a sequence of experiments happening the same day. Hence we add a level for each actor and each block as we see in the following model. We see that gamma is relating to each block, the mean of gamma is just 0. It seems as the model would just add alpha bar with gamma bar, if we were to add this. Notice that you can select clusters yourself, although the problem is mostly how complicated model can the user actually cope with. 13.4 Divergent transitions and non-centered priors Divergent transitions is when you are sampling and the energy is not the same in the beginning as it is in the end, hence the energy is diverging. This typically happens whenever the posterior distribution is very steep in some places but not in others. i.e., that is when there are steep changes in probability. ulam() (HMC) will warn about diverging transitions and this typically implies that there are probability regions that are difficult to explore. E.g., this can happen if there is a valley or trench in a given probability region, like a marian trench. This can be overcome with the following: Tune the sampler to not overshoot these. This implies a longer warmup phase, with a higher acceptance rate (so it can go farther). You must use adapt_delta. Rewrite the model, i.e., reparameterization. Many identitical model can be written mathematically the same, but numerically different. With reparameterization we distinguish between centered and non-centered model: A centered model is when the you include a parameter to scale onto a normal distribution with a mean of 0 and standard deviation of 1. The following are two sections are examples. 13.4.0.1 The Devil’s Funnel: (example) Lets take an example with two parameters, one depending on the other, hence a typical multilevel model. \\[ \\nu \\sim Normal(0,3)\\\\ x \\sim Normal(0,exp(\\nu)) \\] This model may seem simple, but apparantly it is horrible to sample from. m13.7 &lt;- ulam( alist( v ~ normal(0,3), x ~ normal(0,exp(v)) ), data = list(N=1), chains = 4 ) ## ## SAMPLING FOR MODEL &#39;3a5530af995ac1418f5e59b7607ceed3&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.044146 seconds (Warm-up) ## Chain 1: 0.008162 seconds (Sampling) ## Chain 1: 0.052308 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;3a5530af995ac1418f5e59b7607ceed3&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 4e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.04 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.017961 seconds (Warm-up) ## Chain 2: 0.013401 seconds (Sampling) ## Chain 2: 0.031362 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;3a5530af995ac1418f5e59b7607ceed3&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 6e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.019092 seconds (Warm-up) ## Chain 3: 0.018295 seconds (Sampling) ## Chain 3: 0.037387 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;3a5530af995ac1418f5e59b7607ceed3&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.018647 seconds (Warm-up) ## Chain 4: 0.017982 seconds (Sampling) ## Chain 4: 0.036629 seconds (Total) ## Chain 4: We see that there are many divergent transition, we see that this results in an ineffective sampling (we look at the traceplot after), where the effective sample size is low, hence the model means may be reliable. From the precis below, we see how few effective samples we are having. precis(m13.7) x 1.2347085 -0.9807996 x 1.766463 45.705586 x -1.381203 -18.629713 x 4.358443 33.916971 x 36.64383 96.03279 x 1.115658 1.045561 Lets at the traceplot. traceplot(m13.7) ## [1] 1000 ## [1] 1 ## [1] 1000 We see that v appear is somewhat OK, although it might not be entirely stationary. But we know that x depends on v and the exploration of x does look horrible, we are stuck in several places for many iterations. Althougnh we do get fewer effectie samples from the v parameter, even though it might look more hairy than x (this might be due to the scaling issues). Thus we can look at the trank plot. We can also take look at the trankplot() and it becomes clear why we get fewer effective samples from v, as it is showing much randomness. Hence due to that the few effective number of samples, we deem this an unhelathy chain. trankplot(m13.7) Why is this model not working: We see that as \\(\\nu\\) changes the distriubtion of \\(x\\) is changing. x = -10:10 y = dnorm(x = x,mean = 0,sd = 3) plot(x,y,type = &#39;l&#39;,main = &quot;v prior&quot;) Hence we see that v can output a lot of different numbers, thus the distribution of x can take many shapes! Hence the dsitribubtino of x is conditional on another parameter, this is called centered parametrization. This can be addressed with non-centered parametrization, where one parameter is is not conditional on one or more other parameters. We can then specify the model with the following: \\[ \\nu \\sim Normal(0,3)\\\\ z \\sim Normal(0,1)\\\\ x = z * exp(\\nu) \\] Now we see that the distribution of \\(z\\) is fixed, and then \\(x\\) is defined by z and \\(\\nu\\). Essentially what z is doing, is that it is standardizing the values, hence onto a scale, with a mean of 0 and a standard deviation of 1. Now we can fit the model: m13.7 &lt;- ulam( alist( v ~ normal(0,3), z ~ normal(0,1), gq&gt; real[1]:x &lt;&lt;- z*exp(v) ),data = list(N=1),chains = 4 ) ## ## SAMPLING FOR MODEL &#39;5d4720f25ae8319c74760e2bb9522016&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.1 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.005291 seconds (Warm-up) ## Chain 1: 0.004643 seconds (Sampling) ## Chain 1: 0.009934 seconds (Total) ## Chain 1: ## ## SAMPLING FOR MODEL &#39;5d4720f25ae8319c74760e2bb9522016&#39; NOW (CHAIN 2). ## Chain 2: ## Chain 2: Gradient evaluation took 3e-06 seconds ## Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 2: Adjust your expectations accordingly! ## Chain 2: ## Chain 2: ## Chain 2: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 2: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 2: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 2: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 2: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 2: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 2: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 2: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 2: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 2: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 2: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 2: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 2: ## Chain 2: Elapsed Time: 0.005597 seconds (Warm-up) ## Chain 2: 0.004954 seconds (Sampling) ## Chain 2: 0.010551 seconds (Total) ## Chain 2: ## ## SAMPLING FOR MODEL &#39;5d4720f25ae8319c74760e2bb9522016&#39; NOW (CHAIN 3). ## Chain 3: ## Chain 3: Gradient evaluation took 5e-06 seconds ## Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds. ## Chain 3: Adjust your expectations accordingly! ## Chain 3: ## Chain 3: ## Chain 3: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 3: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 3: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 3: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 3: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 3: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 3: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 3: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 3: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 3: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 3: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 3: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 3: ## Chain 3: Elapsed Time: 0.005799 seconds (Warm-up) ## Chain 3: 0.004916 seconds (Sampling) ## Chain 3: 0.010715 seconds (Total) ## Chain 3: ## ## SAMPLING FOR MODEL &#39;5d4720f25ae8319c74760e2bb9522016&#39; NOW (CHAIN 4). ## Chain 4: ## Chain 4: Gradient evaluation took 3e-06 seconds ## Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.03 seconds. ## Chain 4: Adjust your expectations accordingly! ## Chain 4: ## Chain 4: ## Chain 4: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 4: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 4: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 4: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 4: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 4: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 4: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 4: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 4: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 4: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 4: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 4: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 4: ## Chain 4: Elapsed Time: 0.00539 seconds (Warm-up) ## Chain 4: 0.004921 seconds (Sampling) ## Chain 4: 0.010311 seconds (Total) ## Chain 4: Now we do not get any warnings, lets look at the precis. precis(m13.7) x -0.0329243 0.0432956 33.0935344 x 2.953853 1.010432 1550.941620 x -4.729130 -1.536735 -14.849053 x 4.594237 1.662892 30.606227 x 1227.935 1148.040 1286.947 x 1.000933 1.002020 1.000723 We see that we have imensely mroe effective samples. traceplot(m13.7) trankplot(m13.7) ## [1] 1000 ## [1] 1 ## [1] 1000 Now we see the hairy cattarpillar, as it is stationary and hairy. Thus, it is controlled randomness where much more of the probability space is being explored. 13.4.0.2 Non-centered chimpanzees (example) This is another example. Here we will aim for increasing the acceptance rate (adapt_delta). THe default is 0.95, we will fix it to 0.00. We will initially use m13.4 and then update it with the adept_delta. First the initial model is specified, and run, we will see that we get some divergent transitions, hence ineffective sampling. library(rethinking) data(&quot;chimpanzees&quot;) d &lt;- chimpanzees d$treatment &lt;- 1 + d$prosoc_left + 2*d$condition dat_list &lt;- list( pulled_left = d$pulled_left ,actor = d$actor ,block_id = d$block ,treatment = as.integer(d$treatment) ) set.seed(13) m13.4 &lt;- ulam( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a[actor] + g[block_id] + b[treatment] , b[treatment] ~ dnorm( 0 , 0.5 ), ## adaptive priors a[actor] ~ dnorm( a_bar , sigma_a ), g[block_id] ~ dnorm( 0 , sigma_g ), ## hyper-priors a_bar ~ dnorm( 0 , 1.5 ), sigma_a ~ dexp(1), sigma_g ~ dexp(1) ) ,data=dat_list ,chains=4 ,cores=4 ,log_lik=TRUE ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 Now we can inspect the model parameters, we have a lot, hence we can also plot these. precis(m13.4,depth = 2) x -0.1202342 0.3996430 -0.4660319 0.2875241 -0.3798811 4.6449805 -0.6794950 -0.6664071 -0.3561609 0.5669153 2.1134034 -0.1587560 0.0344409 0.0511169 0.0072702 -0.0288834 0.1064717 0.6014632 2.0260519 0.2030002 x 0.3053177 0.2992567 0.2937477 0.2861123 0.3630696 1.2494691 0.3629784 0.3612484 0.3532933 0.3700828 0.4460654 0.2089596 0.1690865 0.1775561 0.1655645 0.1697480 0.1920594 0.7452654 0.6691123 0.1653719 x -0.6059643 -0.0929259 -0.9095364 -0.1592338 -0.9846915 2.9914082 -1.2710037 -1.2691111 -0.9317731 -0.0062391 1.4166228 -0.5595175 -0.2004000 -0.2013607 -0.2616322 -0.3248914 -0.1264497 -0.5676737 1.2087750 0.0270603 x 0.3572745 0.8912879 0.0142545 0.7554046 0.1997286 6.9044857 -0.1031714 -0.1016952 0.2275181 1.1644807 2.8553954 0.0685569 0.3194674 0.3670101 0.2688334 0.2181303 0.4662653 1.7480839 3.2321398 0.4955708 x 352.1113 455.1016 498.1411 432.0849 357.3686 785.7080 394.0742 403.6395 432.0090 438.0302 696.6778 480.6903 1157.4440 962.7915 1009.6172 1034.6540 664.7862 867.2054 1030.2236 276.1293 x 1.012940 1.004142 1.007090 1.007902 1.015387 1.003714 1.012216 1.009959 1.006313 1.009233 0.999571 1.006836 1.000301 1.001331 1.001226 1.004045 1.006587 1.004924 1.004084 1.012830 plot(precis(m13.4,depth = 2)) 13.4.0.2.1 Solution 1: Centered parameterization (Increasing acceptance rate) Let us now run the same mode, with a higher acceptance rate: set.seed(13) m13.4b &lt;- ulam(m13.4,chains = 4,cores = 4,control = list(adapt_delta = 0.99)) #divergent(m13.4b) #cannot see what this does Now we see that we do not get any divergent transitions, although the sampling is still not perfect. Lets inspect the precis. precis(m13.4b,depth = 2) plot(precis(m13.4b,depth = 2)) x -0.1302746 0.3954946 -0.4706571 0.2805859 -0.3665617 4.6898717 -0.6538192 -0.6699735 -0.3633258 0.5752311 2.1024196 -0.1628439 0.0380150 0.0509748 0.0072183 -0.0341142 0.1058120 0.6060141 2.0131269 0.2011608 x 0.3022919 0.3074949 0.3061363 0.3055050 0.3580875 1.3010291 0.3722451 0.3761301 0.3637012 0.3693000 0.4655697 0.2141647 0.1722306 0.1711380 0.1726144 0.1660861 0.1877883 0.7318509 0.6467462 0.1589038 x -0.6046522 -0.1031126 -0.9466180 -0.1862253 -0.9292501 2.9658521 -1.2825710 -1.2625864 -0.9328041 -0.0210326 1.3624848 -0.5761011 -0.2309838 -0.1778659 -0.2640067 -0.3285829 -0.1273451 -0.5610347 1.1941015 0.0260726 x 0.3631622 0.8901949 0.0057508 0.7859333 0.2005385 6.9467640 -0.0934477 -0.0643183 0.2157645 1.1697997 2.8644421 0.0715798 0.3258351 0.3356304 0.2921987 0.2112754 0.4471857 1.7324659 3.2222602 0.4973964 x 467.6474 501.3390 492.3328 492.9944 473.7843 633.9770 557.1852 490.6244 507.0977 522.6226 689.1397 416.9034 889.1386 681.2203 962.7928 712.6298 526.7992 1094.1482 907.2029 234.9112 x 1.0022732 1.0024040 1.0042299 1.0029014 1.0037398 1.0017902 1.0017314 1.0036969 1.0019566 1.0043043 1.0013331 1.0105290 1.0001291 0.9996887 1.0002676 1.0055593 1.0049964 1.0006677 1.0015137 1.0189586 Notice that the effective number of samples is not much different. prec1 = precis(m13.4,depth = 2) prec2 = precis(m13.4b,depth = 2) plot(prec1$n_eff,type = &#39;l&#39;,col = &#39;blue&#39;,ylab = &#39;n_eff&#39;,xlab = &#39;Parameters&#39;,ylim = c(0,1200),xaxt = &#39;n&#39;) #xtick &lt;- row.names(prec1) xtick&lt;-seq(1, nrow(prec1), by=1) axis(side=1, at=xtick, labels = FALSE) text(x=xtick,y = par(&quot;usr&quot;)[3], labels = row.names(prec1), srt = 45, pos = 1, xpd = TRUE) grid() lines(prec2$n_eff,col = &#39;red&#39;,lty = 2) legend(&quot;topleft&quot;,legend = c(&quot;Initial model w. div. trans.&quot;,&quot;Iterated model&quot;),lty = 1:2,col = c(&quot;blue&quot;,&quot;red&quot;),cex = 0.8) We see that the effective number of samples tends to be higher, although not always. We had four chains with 500 samples from each, hence 2000 samples, so not a perfect exploration. Improving through reparametirization We see that we can improve the effective number of samples, by doing the same trick as in the previous example. Thus we will add a parameter z and x to standardize the results, the model then looks the following: Thus, we see that z gives the standardized intercept for each actor, and the evctor x gives the standardiced intercept for each block. We still have a linear model that is mapped on to the logistic function, hence the logit(p), where all of the initial parameters still appear. Notice that the intercept for each actor (chimp) is: \\[ \\alpha_j = \\bar{\\alpha} + z_j * \\sigma_\\alpha \\] Thus we have mean alpha plus the parameter for each chimp times sampling from the exponential distribution, where values tends towards 0. 13.4.0.2.2 Solution 2: Non-centered version (reparameterize) Thus we have reparametized and the model can now be specified with the following and also sampled from. set.seed(13) m13.4nc &lt;- ulam( alist( pulled_left ~ dbinom( 1 , p ) , logit(p) &lt;- a_bar + z[actor]*sigma_a + # actor intercepts x[block_id]*sigma_g + # block intercepts b[treatment] , b[treatment] ~ dnorm( 0 , 0.5 ), z[actor] ~ dnorm( 0 , 1 ), x[block_id] ~ dnorm( 0 , 1 ), a_bar ~ dnorm( 0 , 1.5 ), sigma_a ~ dexp(1), sigma_g ~ dexp(1), gq&gt; vector[actor]:a &lt;&lt;- a_bar + z*sigma_a, gq&gt; vector[block_id]:g &lt;&lt;- x*sigma_g ) ,data=dat_list ,chains=4 ,cores=4 ) Notice that we do not get any warnings precis(m13.4nc,depth = 2) x -0.1373449 0.3888675 -0.4870408 0.2675311 -0.5139835 2.1279913 -0.6837229 -0.6869212 -0.5172174 -0.0016439 0.8286856 -0.6899485 0.1683883 0.1883826 0.0736860 -0.1579854 0.4404815 0.5966687 1.9868039 0.2055282 -0.1659935 0.0426013 0.0505519 0.0143613 -0.0349257 0.1072077 -0.3404283 4.6517792 -0.6469905 -0.6567907 -0.3469747 0.5924776 2.1128719 x 0.3010033 0.3102741 0.3093515 0.2960859 0.3967577 0.6480550 0.4158494 0.4094490 0.3919167 0.3684629 0.4546931 0.9201783 0.8296726 0.8872955 0.8757323 0.8647679 0.8309913 0.7245536 0.6247502 0.1678048 0.2177411 0.1784686 0.1781149 0.1766735 0.1749633 0.1960510 0.3653333 1.2637287 0.3668659 0.3726603 0.3726781 0.3747484 0.4706430 x -0.6152752 -0.0889388 -0.9839393 -0.2053236 -1.1678480 1.1422862 -1.3577541 -1.3445955 -1.1403130 -0.5776370 0.1351885 -2.1405682 -1.2074554 -1.3445549 -1.3284011 -1.5691454 -0.9035997 -0.5512172 1.1935664 0.0170036 -0.5702435 -0.2168633 -0.1941667 -0.2641924 -0.3471585 -0.1279120 -0.9132766 3.0195252 -1.2332358 -1.2479211 -0.9282008 0.0034196 1.3943356 x 0.3434729 0.8914550 0.0070807 0.7265044 0.0977745 3.2224403 -0.0245178 -0.0236561 0.1034564 0.5809684 1.5921906 0.7927253 1.4459473 1.5454720 1.5004526 1.1965665 1.7467720 1.7547745 3.0806919 0.5116822 0.0761933 0.3449451 0.3582977 0.2970075 0.2205464 0.4622580 0.2629761 6.7913091 -0.0507023 -0.0427602 0.2647663 1.2016657 2.8918980 x 1109.2015 1126.9353 1149.3090 1139.5972 570.2012 834.7693 569.5761 568.8557 590.5219 556.9492 577.7904 1879.6265 1988.3562 1599.5739 1872.5986 1965.0102 2034.0889 457.5062 548.7522 1035.0490 1360.4306 2239.1875 1811.4633 1916.4529 1695.1049 1506.2322 1281.8621 1291.0811 1220.5624 1359.9741 1230.4185 1213.6924 1700.6873 x 1.0059588 1.0036525 1.0040338 1.0042296 1.0100756 1.0032596 1.0078938 1.0068966 1.0084350 1.0089914 1.0072388 1.0002059 0.9990201 0.9993475 0.9996990 1.0002215 1.0004454 1.0100190 1.0102338 1.0019622 1.0015619 1.0000254 0.9994864 0.9992842 1.0004592 1.0011128 1.0016656 1.0026933 1.0034946 1.0053463 1.0063695 1.0080017 1.0021015 prec1 = precis(m13.4,depth = 2) prec2 = precis(m13.4b,depth = 2) prec3 = precis(m13.4nc,depth = 2) plot(prec1$n_eff,type = &#39;l&#39;,col = &#39;blue&#39;,ylim = c(0,2000),xaxt = &#39;n&#39; ,main = &#39;Comparison on model variates&#39;,ylab = &#39;n_eff&#39;,xlab = &#39;Parameters&#39;) #xtick &lt;- row.names(prec1) xtick&lt;-seq(1, nrow(prec1), by=1) axis(side=1, at=xtick, labels = FALSE) text(x=xtick,y = par(&quot;usr&quot;)[3], labels = row.names(prec1), srt = 45, pos = 1, xpd = TRUE) grid() lines(prec2$n_eff,col = &#39;red&#39;,lty = 2) lines(prec3$n_eff,col = &#39;purple&#39;,lty = 2) legend(&quot;topleft&quot;,legend = c(&quot;Initial model w. div. trans.&quot;,&quot;Delta_adapt = 0.99&quot;,&quot;Reparameterized&quot;),lty = c(1,2,2),col = c(&quot;blue&quot;,&quot;red&quot;,&quot;purple&quot;),cex = 0.8) Now we see that the sampling is far far better. 13.5 Multilevel posterior predictions This section elaborates on how one can make predictions from a multilevel model. He distinguish two types of predictions: Posterior prediction for same clusters Posterior prediction for new clusters What is a cluster? In the chimpanzee example, we have 7 unique chimps, these are the clusters, hence we have one cluster for each actor. Thus we distinguish between predicting a known cluster and an unknown cluster. Notice that to evaluate model predictions we can use information criteria, such as AIC or WAIC. 13.5.1 Posterior prediction for same clusters In this example we predict outcomes for a given actor, hence for a cluster that we already know. chimp &lt;- 2 d_pred &lt;- list( actor = rep(chimp,4), treatment = 1:4, block_id = rep(1,4) ) p &lt;- link(m13.4,data = d_pred) p_mu &lt;- apply(p,2,mean) p_ci &lt;- apply(p,2,PI) now we can extract priors from the model. post &lt;- extract.samples(m13.4) str(post) ## List of 6 ## $ b : num [1:2000, 1:4] -0.35208 -0.27668 -0.35208 0.00864 -0.12984 ... ## $ a : num [1:2000, 1:7] -0.276 -0.515 -0.276 -1.146 -0.495 ... ## $ g : num [1:2000, 1:6] -0.0241 0.0597 -0.0241 -0.2009 -0.0879 ... ## $ a_bar : num [1:2000(1d)] 0.888 1.365 0.888 -0.778 0.423 ... ## $ sigma_a: num [1:2000(1d)] 1.54 1.84 1.54 2 1.96 ... ## $ sigma_g: num [1:2000(1d)] 0.0181 0.5022 0.0181 0.1027 0.0549 ... ## - attr(*, &quot;source&quot;)= chr &quot;ulam posterior: 2000 samples from object&quot; We see that the actors are on the columns, hence we want to select the given column for the chimp. In this case, we select chimp intersects form the alpha object. We can plot the posterior distribution for the intersect for chimp 5. Hence: dens(post$a[,5]) #Actor 5 Now we build our own link finction to create predictions. p_link &lt;- function( treatment , actor=1 , block_id=1 ) { logodds &lt;- with( post , a[,actor] + g[,block_id] + b[,treatment] ) #We select the columns with the input in the func. return( inv_logit(logodds) ) } p_raw &lt;- sapply(X = 1:4 , function(i) p_link(treatment = i , actor=2 , block_id=1 ) ) #We loop over 1 to 4, where treatment = i (1 to 4) p_mu &lt;- apply( p_raw , 2 , mean ) p_ci &lt;- apply( p_raw , 2 , PI ) plot(p_mu,type = &#39;l&#39;,ylim = c(0.9,1),xaxt = &#39;n&#39;,xlab = &#39;Treatment&#39;) axis(1,at = 1:4,labels = c(1,2,3,4),lwd.ticks = 1,lwd = 0) shade(p_ci,1:4) We see that for all treatments, the chimp will favour pulling the left lever. 13.5.2 Posterior prediction for new clusters Now we can do the same, but for a chimp that is not in the experiment that we have, imagine having to estimate the same experiment for a whole new population of chimps. For this we will construct an average chimp. #13.36 p_link_abar &lt;- function( treatment ) { logodds &lt;- with( post , a_bar + b[,treatment] ) return( inv_logit(logodds) ) } #13.37 - extract samples and get the mean and compability intervals post &lt;- extract.samples(m13.4) p_raw &lt;- sapply( 1:4 , function(i) p_link_abar( i ) ) p_mu &lt;- apply( p_raw , 2 , mean ) p_ci &lt;- apply( p_raw , 2 , PI ) #Recall default = 0.89 #Plotting the outcome par(mfrow = c(1,2)) plot( NULL , xlab=&quot;treatment&quot; , ylab=&quot;proportion pulled left&quot; , ylim=c(0,1) , xaxt=&quot;n&quot; , xlim=c(1,4),main = &#39;Average actor&#39;) axis( 1 , at=1:4 , labels=c(&quot;R/N&quot;,&quot;L/N&quot;,&quot;R/P&quot;,&quot;L/P&quot;) ) lines( 1:4 , p_mu ) shade( p_ci , 1:4 ) #13.38 - simulate chimps a_sim &lt;- with( post #use rnorm to sample random chimps ,rnorm(n = length(post$a_bar) #We want as many samples extracted samples ,mean = a_bar #mean = the average chimp ,sd = sigma_a #The standard deviation ) ) p_link_asim &lt;- function( treatment ) { logodds &lt;- with( post , a_sim + b[,treatment] ) return( inv_logit(logodds) ) } p_raw_asim &lt;- sapply(1:4 #Simulate over treatment 1 to 4 ,function(i) p_link_asim(treatment = i) #apply function with input i using function p_link_asim() with treatm. = i ) #13.39 - plotting the simulated actors plot( NULL , xlab=&quot;treatment&quot; , ylab=&quot;proportion pulled left&quot; , ylim=c(0,1) , xaxt=&quot;n&quot; , xlim=c(1,4),main = &#39;Simulated actors&#39;) axis( 1 , at=1:4 , labels=c(&quot;R/N&quot;,&quot;L/N&quot;,&quot;R/P&quot;,&quot;L/P&quot;) ) for ( i in 1:100 ) lines( 1:4 , p_raw_asim[i,] , col=grau(0.25) , lwd=2 ) Recall that the treatments were: Two food items on right and no partner Two food items on left and no partner Two food items on right and partner present Two food items on left and partner present We see that whenever the foods is on the left, there is a tendency to to pull the left lever, no matter if a partner is present or not. Hence we can use this information to simulate examples. 13.6 Exercises 13.6.1 E1 Which of the following priors will produce more shrinkage in the estimates? (a) αtank ∼ Normal(0, 1); (b) αtank ∼ Normal(0, 2). The first will have the most shrinkage as the standard deviation of the normal distribution is the smallest. 13.6.2 E2 Make the following model into a multilevel model. \\[ y_i \\sim Binomial(1,p_i)\\\\ logit(p_i) = \\alpha_{GROUP[i]} + \\beta x_i\\\\ \\alpha_{GROUP} \\sim Normal(0,10)\\\\ \\beta \\sim Normal(0,1) \\] This can be transformed into: \\[ \\begin{align*} y_i \\sim Binomial(1,p_i)\\\\ logit(p_i) = \\alpha_{GROUP[i]} + \\beta x_i \\\\ \\alpha_{GROUP} \\sim Normal(\\bar{\\alpha},\\sigma_\\alpha) &amp;&amp; \\text{Insert hyperparameter in the prior} \\\\ \\beta \\sim Normal(0,1)\\\\ \\bar{\\alpha} \\sim Normal(0,10) &amp;&amp; \\text{Define distributions for new hyperpriors} \\\\ \\sigma_\\alpha \\sim Normal(0,1) &amp;&amp; \\text{Define distributions for new hyperpriors} \\\\ \\end{align*} \\] 13.6.3 E3 Make the following model into a multilevel model. \\[ \\begin{align*} y_i \\sim Binomial(\\mu_i,\\sigma) &amp;&amp; \\text{Notice that we already have a sigma}\\\\ \\mu_i = \\alpha_{GROUP[i]} + \\beta x_i\\\\ \\alpha_{GROUP} \\sim Normal(0,10)\\\\ \\beta \\sim Normal(0,1) \\\\ \\sigma \\sim HalfCaucy(0,2) \\end{align*} \\] Now we already have a sigma, we will just define a new sigma for each alpha (cluster), that will have their own intersect. \\[ \\begin{align*} y_i \\sim Binomial(\\mu_i,\\sigma) &amp;&amp; \\text{Notice that we already have a sigma}\\\\ \\mu_i = \\alpha_{GROUP[i]} + \\beta x_i\\\\ \\alpha_{GROUP} \\sim Normal(\\bar{\\alpha},\\sigma_\\alpha) &amp;&amp; \\text{We add the hyperparameters} \\\\ \\beta \\sim Normal(0,1) \\\\ \\sigma \\sim HalfCaucy(0,2) \\\\ \\bar{\\alpha} \\sim Normal(0,10) &amp;&amp; \\text{The new hyperpriors} \\\\ \\sigma_\\alpha \\sim Normal(0,1) &amp;&amp; \\text{The new hyperpriors} \\end{align*} \\] 13.6.4 E4 Write an example mathematical model formula for a Poisson regression with varying intercepts. Notice that the poisson distribution relates to an exponential scenario, thus the logit cannot be used, we will then use the log function. \\[ \\begin{align*} y_i \\sim Poisson(\\lambda_i)\\\\ log(\\lambda_i) = \\alpha_{GROUP[i]} + \\beta x_i\\\\ \\alpha_{GROUP} \\sim Normal(\\bar{\\alpha},\\sigma_\\alpha) &amp;&amp; \\text{We add the hyperparameters} \\\\ \\beta \\sim Normal(0,1) \\\\ \\bar{\\alpha} \\sim Normal(0,10) &amp;&amp; \\text{The hyperprior} \\\\ \\sigma_\\alpha \\sim Normal(0,1) &amp;&amp; \\text{The hyperprior} \\end{align*} \\] 13.6.5 M1 Revisit the Reed frog survival data, data(reedfrogs), and add the predation and size treatment variables to the varying intercepts model. Consider models with either main effect alone, both main effects, as well as a model including both and their interaction. Instead of focusing on inferences about these two predictor variables, focus on the inferred variation across tanks. Explain why it changes as it does across models. library(rethinking) data(reedfrogs) d &lt;- reedfrogs dat &lt;- list( S = d$surv, #Survival - absolute numbers n = d$density, #Density in tadpole tank = 1:nrow(d), #Tank number pred = ifelse( d$pred==&quot;no&quot; , 0L , 1L ), #pred = predator, if pred then predator size_ = ifelse( d$size==&quot;small&quot; , 1L , 2L ) #Size of the tadpoles ) Now we can define the varying intercepts model. This is basically just as in the book. m1.1 &lt;- ulam( alist( S ~ binomial( n , p ), logit(p) &lt;- a[tank], #We have simply just an intercept for each tank (mean) a[tank] ~ normal( a_bar , sigma ), a_bar ~ normal( 0 , 1.5 ), #Mean of 0 with sd of 1 sigma ~ exponential( 1 ) ), data=dat , chains=1 , cores=1 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;1ef62b17e4bbddcc8fc770128ef6a5e3&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.7e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.27 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.085278 seconds (Warm-up) ## Chain 1: 0.061321 seconds (Sampling) ## Chain 1: 0.146599 seconds (Total) ## Chain 1: precis(m1.1,depth = 1) x 1.361531 1.629813 x 0.2516674 0.2152455 x 0.961441 1.348826 x 1.785437 1.987467 x 673.8166 467.2529 x 1.006070 1.001797 In general we are not having super high n_eff but there are no warnings, hence the model should be fine, but lets look at the trace plots to make sure. Notice that this is the equivalent of saying depth = 2 in the precis. traceplot(m1.1) ## [1] 1000 ## [1] 1 ## [1] 1000 We see that the scales are wide, although it does look stationary and a bit hairy, one may inspect each plot individually. Although in general we are having a lot of effective observations. Now we can define a model with a slope parameter for the presence of predators in the tadpoles, this is named bp. # pred m1.2 &lt;- ulam( alist( S ~ binomial( n , p ), logit(p) &lt;- a[tank] + bp*pred, #Adding parameter for predator a[tank] ~ normal( a_bar , sigma ), #abar is the mean, and then there is a deviatino for each tadpole bp ~ normal( -0.5 , 1 ), #We say on average there are none, but the right tail does allow preds a_bar ~ normal( 0 , 1.5 ), sigma ~ exponential( 1 ) ), data=dat , chains=1 , cores=1 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;30bd937c2236571336d74d4dcabb67f0&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.32 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.124008 seconds (Warm-up) ## Chain 1: 0.075185 seconds (Sampling) ## Chain 1: 0.199193 seconds (Total) ## Chain 1: precis(m1.2) x -2.3609286 2.4880386 0.8359907 x 0.2918959 0.2320355 0.1372327 x -2.7859163 2.1319842 0.6349304 x -1.890161 2.861323 1.082454 x 66.69556 71.02393 145.18904 x 1.010995 1.012844 0.998147 Here we see that this model is much more difficult to sample from, as we have way less effective observations. We may plot the trace or trank plot, but we already have a good idea on the conclusion. Now lets define a model where we include tank size instead. Notice there are two different tank sizes # size m1.3 &lt;- ulam( alist( S ~ binomial( n , p ), logit(p) &lt;- a[tank] + s[size_], #tank size is included. a[tank] ~ normal( a_bar , sigma ), s[size_] ~ normal( 0 , 0.5 ), a_bar ~ normal( 0 , 1.5 ), sigma ~ exponential( 1 ) ), data=dat , chains=1 , cores=1 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;882322dfdb46bbfb1391f578a8c30357&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 4.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.42 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.137928 seconds (Warm-up) ## Chain 1: 0.138459 seconds (Sampling) ## Chain 1: 0.276387 seconds (Total) ## Chain 1: precis(m1.3) x 1.291846 1.605351 x 0.4028207 0.2057248 x 0.7171857 1.3289565 x 1.976167 1.982488 x 78.70042 329.48565 x 0.9980789 1.0128424 Again we see bad sampling Now we can add both size and predators. # pred + size m1.4 &lt;- ulam( alist( S ~ binomial( n , p ), logit(p) &lt;- a[tank] + bp*pred + s[size_], #We add bo and size parameter a[tank] ~ normal( a_bar , sigma ), bp ~ normal( -0.5 , 1 ), s[size_] ~ normal( 0 , 0.5 ), a_bar ~ normal( 0 , 1.5 ), sigma ~ exponential( 1 ) ), data=dat , chains=1 , cores=1 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;0e675c8df28d2ad5c2819f9e36312eb1&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.5e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.35 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.165988 seconds (Warm-up) ## Chain 1: 0.149228 seconds (Sampling) ## Chain 1: 0.315216 seconds (Total) ## Chain 1: precis(m1.4,depth = 2) x 2.4771550 2.9136843 1.7578238 2.9219656 2.3194876 2.3710867 2.8494837 2.3474214 2.2878734 3.5590450 3.0339121 2.7838502 2.8156882 2.2655710 3.3348469 3.3425995 2.9274375 2.6073668 2.2949636 3.2568300 2.3704517 2.3657926 2.4108749 1.8316763 1.6874695 2.6316585 1.3878355 2.0793434 2.2852381 3.2846964 1.6250318 1.9131946 3.0816411 2.8074775 2.8141200 2.3403966 2.0396916 3.2396923 2.5509884 2.2778548 1.0671055 2.0029023 2.1367090 2.2182559 2.6633589 1.6633016 3.7237814 2.1643224 -2.4546259 0.3019165 -0.1379876 2.4733671 0.7873200 x 0.6923661 0.7784815 0.7178170 0.8068278 0.7703186 0.7543905 0.7771233 0.7926425 0.6207433 0.6768836 0.6831504 0.6364716 0.6603162 0.6916507 0.7091986 0.6982087 0.6696434 0.6728213 0.6195318 0.7278106 0.6728598 0.6652085 0.7188463 0.6207204 0.5843400 0.5537190 0.6388316 0.5656499 0.5914751 0.5959129 0.5791448 0.5652221 0.7034046 0.6131150 0.6409084 0.5864754 0.5658912 0.6913627 0.6184896 0.6256696 0.6215430 0.5604977 0.5383791 0.5137033 0.5414420 0.5430657 0.6127110 0.5261141 0.2969180 0.3823975 0.3820561 0.3918555 0.1517908 x 1.3798451 1.7642227 0.6154546 1.6920156 1.1309406 1.1774854 1.6389159 1.0402539 1.3184488 2.5904321 1.9586427 1.7018986 1.8104963 1.0967298 2.2746065 2.2565234 1.8827146 1.5583125 1.3754785 2.2053246 1.2973326 1.3213316 1.3756013 0.8443049 0.7396635 1.7397385 0.4199569 1.2260138 1.3079884 2.2644143 0.6917854 0.9961783 2.0347501 1.7853720 1.8003289 1.4325333 1.1559406 2.1718181 1.6122717 1.3544190 0.0205600 1.0300356 1.2966061 1.3595211 1.8118484 0.7872056 2.7970115 1.3316634 -2.9371578 -0.2985405 -0.7894670 1.8407229 0.5395324 x 3.5928519 4.3219192 2.9401314 4.2698098 3.5273943 3.6177425 4.0929147 3.6388793 3.2906372 4.6419870 4.0929630 3.8071863 3.9398477 3.2943510 4.4410736 4.4015989 4.0261927 3.7086587 3.2641751 4.4609918 3.4277539 3.4988242 3.6084756 2.8066935 2.6133122 3.4868826 2.3834943 2.9815444 3.1576667 4.1929541 2.5805805 2.7574810 4.2167542 3.7325600 3.8353151 3.3296858 2.9524117 4.3945800 3.5120094 3.2444058 2.0161833 2.9122641 2.9340527 3.0426673 3.5476564 2.4898155 4.7291609 2.9809618 -1.9966936 0.8953079 0.4525977 3.0944928 1.0471074 x 168.04083 149.91205 106.28229 114.17813 105.23858 143.29671 144.30143 91.94152 62.03763 73.92943 80.55349 73.68801 83.74826 66.51195 135.47087 104.75179 111.19286 112.70912 95.51479 149.21443 88.31904 84.79470 96.24659 78.60948 61.01608 50.66947 65.35194 43.62721 40.81995 47.11339 51.73169 54.18072 75.37265 79.26929 95.28144 64.85739 67.69409 138.71307 161.03893 110.01743 69.83844 49.07875 55.98422 45.41175 49.92010 54.22685 65.01668 56.29243 234.17647 34.72233 39.04280 28.01418 123.49372 x 0.9989824 0.9983222 1.0078910 1.0003393 1.0090890 1.0068573 1.0032217 1.0047926 1.0011457 1.0015192 1.0011611 1.0008017 0.9987091 1.0022147 0.9981371 0.9996607 0.9987557 1.0010548 1.0089008 0.9980171 1.0000764 1.0032145 1.0075210 1.0097545 0.9997106 1.0026959 1.0030639 1.0047830 1.0003458 1.0025057 1.0060940 1.0018245 1.0016866 1.0135760 1.0090495 1.0155574 1.0108961 0.9993986 1.0051738 1.0089259 1.0072140 0.9995856 1.0005369 1.0027182 1.0039361 1.0052361 0.9996073 1.0008632 1.0024107 1.0247102 1.0314935 1.0083496 1.0016651 Again bad sampling. Notice that there is a parameter for each size and one slope and then one intercept for each tank, we have 48 tanks. Lastly we can add an interaction between size and predators. That is done in the following: # pred + size + interaction m1.5 &lt;- ulam( alist( S ~ binomial( n , p ), logit(p) &lt;- a_bar + z[tank]*sigma + bp[size_]*pred + s[size_], z[tank] ~ normal( 0 , 1 ), bp[size_] ~ normal( -0.5 , 1 ), s[size_] ~ normal( 0 , 0.5 ), a_bar ~ normal( 0 , 1.5 ), sigma ~ exponential( 1 ) ), data=dat , chains=1 , cores=1 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;60015360a9d7da9f89bef1ab32ed4232&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 3.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.273927 seconds (Warm-up) ## Chain 1: 0.193449 seconds (Sampling) ## Chain 1: 0.467376 seconds (Total) ## Chain 1: precis(m1.5,depth = 2) x -0.1153235 0.5197372 -0.9699717 0.4963131 0.0436215 0.0206212 0.5143355 0.0155139 -0.1080988 1.4806376 0.8471621 0.5413419 0.1672866 -0.4074641 0.9802790 0.8908402 0.4028508 0.1014327 -0.3157045 0.9218839 0.1242347 0.1020162 0.0858499 -0.5723078 -0.8530168 0.4053936 -1.3343679 -0.3136633 -0.5082626 0.7963553 -1.3569405 -1.0593141 0.7565605 0.3128181 0.3493013 -0.2941924 -0.2394897 1.1632261 0.3247300 0.0829280 -1.7104539 -0.3721769 -0.2797636 -0.1093638 -0.0254432 -1.3572026 1.4572816 -0.6883068 -1.8260683 -2.7578237 0.1174367 0.1636636 2.2891628 0.7438350 x 0.8639631 0.9029272 0.8088562 0.9234433 0.8382403 0.8445370 0.9193128 0.8400856 0.7042002 0.7590260 0.7164856 0.6821084 0.6653944 0.6701567 0.7711516 0.7203191 0.8198968 0.7742633 0.7689766 0.7708259 0.6931961 0.7239492 0.7102248 0.6749388 0.6082269 0.5784653 0.6247488 0.5540545 0.5985078 0.5638106 0.6084079 0.5272863 0.7678521 0.6906001 0.7213234 0.6697476 0.7117313 0.7436892 0.6702170 0.6622256 0.5328094 0.4895048 0.5226678 0.5177830 0.5486614 0.5286729 0.5806755 0.5259269 0.3637371 0.3719497 0.3987345 0.4210306 0.4199710 0.1446627 x -1.3932118 -0.9000052 -2.1986354 -0.8834881 -1.2788849 -1.3449403 -0.8688098 -1.2444301 -1.2513164 0.2786718 -0.2967429 -0.4846918 -0.7976301 -1.5030959 -0.1982709 -0.2350568 -0.8121479 -1.1287173 -1.5033027 -0.2797523 -0.9736381 -1.0560632 -1.0196851 -1.6250972 -1.8185241 -0.5100641 -2.4016779 -1.1570831 -1.4297089 -0.1279154 -2.4520246 -1.8875789 -0.4963371 -0.6699795 -0.7336354 -1.3327754 -1.3638714 -0.0167292 -0.7008438 -0.9970125 -2.5087859 -1.1909691 -1.1164376 -0.9189135 -0.8667862 -2.1985662 0.5721366 -1.5665976 -2.4068624 -3.3806831 -0.5186704 -0.4583291 1.5868593 0.5461868 x 1.2745132 1.9641382 0.3424146 2.0821613 1.3769367 1.3610176 2.0290225 1.3784753 0.9486234 2.7093873 2.0291606 1.6379934 1.2424422 0.6173739 2.1388198 1.9738576 1.7184563 1.3536240 1.0456288 2.1523167 1.2503758 1.1897632 1.2703433 0.4687983 0.1229330 1.3662660 -0.3639317 0.5563689 0.3930794 1.7248589 -0.4220820 -0.1923222 1.9967709 1.4870117 1.5847743 0.7484465 0.8924862 2.3392347 1.3260869 1.1307043 -0.8223271 0.4065698 0.5359732 0.7362403 0.8273327 -0.5408593 2.4087355 0.1232260 -1.1966251 -2.2020682 0.7476334 0.8182299 2.9339418 1.0057026 x 430.7386 429.1301 314.8921 619.4823 585.6092 513.3068 917.7932 913.5876 480.1378 603.4762 565.4953 445.3568 485.4756 627.4079 455.4732 373.5280 639.6237 507.7065 333.8782 662.6942 638.1195 557.7132 631.8800 541.9059 402.8169 475.5641 434.0297 355.8454 427.9503 341.7179 467.6596 368.7005 489.6262 511.4781 598.1338 434.8483 840.5236 506.6950 980.1278 456.7503 320.7311 379.7922 321.2166 329.8934 450.2244 360.3019 344.6227 323.2708 220.2232 172.3991 253.0559 241.3268 209.8273 248.8095 x 1.0005124 0.9982934 0.9990346 1.0033473 0.9987939 1.0008830 0.9996417 1.0000854 0.9989126 0.9980810 1.0064893 0.9989339 0.9980294 0.9980344 1.0010846 0.9988842 1.0014795 1.0009348 0.9980018 0.9980749 1.0016495 1.0005275 1.0021284 0.9981118 0.9987894 0.9997519 0.9979981 0.9986003 0.9987801 1.0018680 0.9991248 0.9982647 0.9979984 0.9999945 0.9980874 1.0002922 0.9979987 0.9980327 0.9998886 0.9983017 0.9989242 1.0060273 0.9983703 0.9992583 0.9980331 1.0041958 0.9987158 0.9992460 0.9980009 1.0014701 0.9989268 0.9998291 0.9980659 1.0055777 Now we have one overall mean, then a z score for each tank that is to the magnitude of the standard deviation. Then we have a slope for the predators (beta p) and a coefficient for each size of tadpoles, notice we have two - small and big. Lastly we can look at the posterior predictions for sigma as we want to see which model that has the largest variance. plot( coeftab( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 ), pars=&quot;sigma&quot;) We see that m1.1 and m1.3 both have a larger variance, these differ as they exclude predators. Thus adding predator effects in the model makes the variance lower, i.e., we include meaningful information. 13.6.6 H1 library(rethinking) data(bangladesh) d &lt;- bangladesh d$district_id &lt;- as.integer(as.factor(d$district)) dat_list &lt;- list( C = d$use.contraception, did = d$district_id ) 13.6.6.1 Fixed effects model: \\[ c_i \\sim Binom(1,p_i) \\\\ p_i = \\alpha_{did} \\\\ \\alpha \\sim N(\\alpha, \\sigma{\\bar{\\alpha}}) \\\\ \\bar{\\alpha} \\sim N(0,1.5) \\\\ \\sigma \\sim Exp(1) \\] did = district id. m13H1.1 &lt;- ulam( alist( C ~ bernoulli( p ), #bernoulli distribution logit(p) &lt;- a[did], #Intercept for a given district, if we were to remove [did], we would assume that all had the same mean a[did] ~ normal( 0 , 1.5 ) #alpha is given by normal dist. There is no pooling, as each distrct is its own ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE ) 13.6.6.2 Varying intercepts model: m13H1.2 &lt;- ulam( alist( C ~ bernoulli( p ), logit(p) &lt;- a[did], a[did] ~ normal( a_bar , sigma ), a_bar ~ normal( 0 , 1.5 ), sigma ~ exponential( 1 ) ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE ) 13.6.6.3 Extract samples Now we can extract from each model to see how they estimate the data. post1 &lt;- extract.samples( m13H1.1 ) post2 &lt;- extract.samples( m13H1.2 ) p1 &lt;- apply( inv_logit(post1$a) , 2 , mean ) p2 &lt;- apply( inv_logit(post2$a) , 2 , mean ) # compute raw estimate from data in each district t3 &lt;- table( d$use.contraception , d$district_id ) n_per_district &lt;- colSums( t3 ) p_raw &lt;- as.numeric( t3[2,]/n_per_district ) nd &lt;- max(dat_list$did) { plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab=&quot;prob use contraception&quot; , xlab=&quot;district&quot; ) points( 1:nd , p1 , pch=16 , col=rangi2 ) points( 1:nd , p2 ) points( 1:nd , p_raw , pch=3 ) abline( h=mean(inv_logit(post2$a_bar)) , lty=2 ) #identify( 1:nd , p_raw , labels=n_per_district ) #include in rmd to interact with the plot } Figure 13.1: Blue = Fixed effects, Black = open circels, Plus = raw data In general we see that the varying effects is shring towards the mean relative to the fixed effets and the raw data. difference ebtween fixed effects model and multilevelmodel, we see that there is a great distance in some districts while not in others. We see that in the fixed effects model that it has no pooling, while the multilevel model has pooling, hence districts learn from each other. Then we have the effect of number of observations for the given district. Since we have more priors in the multilevel model, we see that in district no. 3, there are only two data points, hence there is very litle information to update the posterior distribution for this district. Thus we see that multilvel models are able to adapt the regularization to the clusters (districts in this example), as it accounts for the amount of data that we are having. "],["ch-14-advuntetures-in-coviaraince.html", "14 Ch 14 - Advuntetures in Coviaraince 14.1 Varying slopes by construction 14.2 Advanced varying slopes 14.3 Exercises", " 14 Ch 14 - Advuntetures in Coviaraince Important note Jesper and Richard use brackets for vectors and paranthesis for matrices. in the previous chapter we saw how varying intercepts can help the model distinguish between different groups in the data. Now we are going to explore how varying slopes also can come in hand. Thus with varying intercepts we are basically able to have a model with different means for the given groups. And with the slopes we are able to build ‘massive interaction machines.’ Up until now we have only worked with unordered categorical variables, although in this chapter we will also take on variables of continous categories, that will be done with gaussian processes (whatever that is). We are also going to do inference on instrumental variables, that is inferring cause without closing the backdoor paths. Why multilevel models with varying slopes? We see that only focusing on averages and even varying intercepts we see that we are generalizing and hence loosing information. Thus including varying intercepts we may end up with a conclusion where we can conclude one thing for a part of the data and another for another part of the data. 14.1 Varying slopes by construction We need a 2-dimensional Gaussian distribution to represent the slope and the intercept. Then we also need a correlation matrix, as we need sigmas for each parameter, as they can practically have their own spread. The varying slopes model, we see the following model: We see that: mu = is the linear model with the varying intercepts and the varying slopes The matrix = the mutlivariate prior. one for each cafe and one for if it is in the afternoon. S = the covariance matrix, see the slides for an example of how that is found. The pc should find the result of this itself. The rest (alpha to sigma beta) = is just fixed (non-adaptive) priors. Alpha mean = 5, hence we expect to wait five minutes on average with an sd of 2. Beta (coefficient for afternoon) = -1 hence we expect to wait less than average in the afternoon. that is with an sd of 0.5. R = correlation matrix prior. We use LKJ prior. That is a nice family of priors. It is simply a distirbution and named after researcher names. This is the correlation between intecepts and slopes, hence we specify the relationship between slopes and intercepts. More on the LKJ prior (the onion prior). It looks the following: We can adjust eta. When: Eta = 1 : we have a uniform correlation matrix. Eta &lt; 1 : then it is sceptical according to correlation between slopes and intercepts. Eta &gt; 1 : it elevates extreme correlations. 14.1.1 Simulate the population We start with m14.1, where we have random intercepts and slopes. We start by simulating the cafés that the robot is going to visit in the next subsection. a &lt;- 3.5 # average morning wait time b &lt;- (-1) # average difference afternoon wait time sigma_a &lt;- 1 # std dev in intercepts sigma_b &lt;- 0.5 # std dev in slopes rho &lt;- (-0.7) # correlation between intercepts and slopes Now we create a vector of means for alpha and beta. Mean for alpha = general wait time in the morning and beta = wait time in the afternoon. Mu &lt;- c( a , b ) Mu ## [1] 3.5 -1.0 We see that in the afternoon the waits tend to be less. Now we can build the covariance matrix. We can build matrices in two ways: By using matrix: cov_ab &lt;- sigma_a*sigma_b*rho Sigma &lt;- matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) , ncol=2 ) Sigma ## [,1] [,2] ## [1,] 1.00 -0.35 ## [2,] -0.35 0.25 By do it more manually sigmas &lt;- c(sigma_a,sigma_b) # standard deviations Rho &lt;- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix # now matrix multiply to get covariance matrix Sigma &lt;- diag(sigmas) %*% Rho %*% diag(sigmas) We see that the result is the same. Now we can simulate this is just done by randomly sampling from the multivariate Gaussian distribution as we already have Mu and Sigma. Notice that we will get two vectors, the intecept and the slopes and the matrix we end up having represent the cafés, one for each café. library(MASS) N_cafes &lt;- 20 set.seed(5) # used to replicate example vary_effects &lt;- mvrnorm(n = N_cafes , Mu , Sigma ) message(&quot;Each row is a cafe, V1 = intercept, V2 = slope, it is plotted in a following chunk&quot;) ## Each row is a cafe, V1 = intercept, V2 = slope, it is plotted in a following chunk head(vary_effects) ## [,1] [,2] ## [1,] 4.223962 -1.6093565 ## [2,] 2.010498 -0.7517704 ## [3,] 4.565811 -1.9482646 ## [4,] 3.343635 -1.1926539 ## [5,] 1.700971 -0.5855618 ## [6,] 4.134373 -1.1444539 Now we can take the vectors and store them in a more meaningful object. a_cafe &lt;- vary_effects[,1] b_cafe &lt;- vary_effects[,2] Now we can plot the intercepts and slopes, we see that the simulated data has a negative correlation. We see that x = intercepts and y = the slopes. Notice that we use ellipses in the plot, these show the quantiles of the data, in a later plot the actual values are presented. But in general the closer you are to the centrum the more observations we are expected to see. Thus in general the intercepts tends towards 3 - 4 and a slope around -1. Recall that we we: set the mean of alpha = 3.5 and slope to -1 with, with standard deviations of respectively 1 and 0.5, hence the result is very much expected. We also explicitly set rho (correlation between slope and intercept) to be -0.7, hence the result is again very much as expected. If you count the points, you will see that there are 20, one for each café. plot( a_cafe , b_cafe , col=rangi2 , xlab=&quot;intercepts (a_cafe)&quot; , ylab=&quot;slopes (b_cafe)&quot; ) # overlay population distribution library(ellipse) for ( l in c(0.1,0.3,0.5,0.8,0.99) ) lines(ellipse(Sigma,centre=Mu,level=l),col=col.alpha(&quot;black&quot;,0.2)) Now we have simulated the data. It is now left for the robot to visit the cafés. That is done in the following subsection. 14.1.2 Simulate observations Here we will generate the observations that the robot will be making. library(MASS) set.seed(5) # used to replicate example N_cafes &lt;- 20 vary_effects &lt;- mvrnorm( N_cafes , Mu , Sigma ) set.seed(22) N_visits &lt;- 10 a_cafe &lt;- vary_effects[,1] b_cafe &lt;- vary_effects[,2] afternoon &lt;- rep(0:1,N_visits*N_cafes/2) #Every second is afternoon cafe_id &lt;- rep( 1:N_cafes , each=N_visits ) #Each café is visited 10 times mu &lt;- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon #Mean waiting times, every second is lower as it is afternoon. sigma &lt;- 0.5 # std dev within cafes wait &lt;- rnorm( N_visits*N_cafes , mu , sigma ) #The wait is defined by normal distribution based on mu and sigma #Collect information in a data frame d &lt;- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait ) head(d,15) cafe afternoon wait 1 0 3.967893 1 1 3.857198 1 0 4.727875 1 1 2.761013 1 0 4.119483 1 1 3.543652 1 0 4.190949 1 1 2.533224 1 0 4.124032 1 1 2.764887 2 0 1.628544 2 1 1.299709 2 0 2.382012 2 1 1.216717 2 0 1.614051 We can also plot them agains each other, here it is clear that each café is having very different mean waiting times # library(dplyr) # library(magrittr) # d1 &lt;- d %&gt;% # group_by(cafe,afternoon) %&gt;% # mutate(mu_group = mean(wait)) %&gt;% # select(-wait) %&gt;% # distinct() # # plot(d1,col = alpha(d1$afternoon + 1, 0.5), pch=16,main = &quot;Black = morning, red = afternoon&quot;) Notice that in the windoes with café each horizontal line is a cafe and wee see the different expected waiting times based on the time of the day. We see that overall the waiting time is the largest in the afternoon and also the cafés are differnt, some have a longer waiting time than the other. Thus it is interesting to have multiple levels on the slopes as well. 14.1.3 The varying slopes model This section elaborates on how we need to add the varying effects (the varying slopes). To do this, one must construct a matrix with the parameters for each group. We see in the following chunk that we have a matrix given \\(\\alpha_{cafe}\\) and \\(\\beta_{cafe}\\). Whenever you see the square brackets it basically means that it is a matrix. Notice that the varying effects is explained by a multivariate normal distribution, basically a distribution based on more than one distribution, try to google it for visualization. Then we have S here the standard deviations is multiplied by R times the stadnard deviations. R is typically given by LKJcorr distribution. LKJcorr is a distribution that is explained by eta. It takes the following shapes: eta = 1, then uniform correlation matrices eta &gt; 1, then correlation between intersect and slopes, but not extreme eta &lt; 1, elevates extreme correlations An example: In the following an example is also plotted. R &lt;- rlkjcorr( 1e4 , K=2 , eta=2 ) dens( R[,1,2] , xlab=&quot;correlation&quot; ) set.seed(867530) m14.1 &lt;- ulam( alist( wait ~ normal( mu , sigma ), mu &lt;- a_cafe[cafe] + b_cafe[cafe]*afternoon, c(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ), #&#39; a = alpha, b = beta, this create a matrix, hence each #&#39; row in the matrix = a cafe, that has a pair of params, a and b #&#39; We then write multi_normal, where provide the a and be params #&#39; Ulam knows how Rho matrix needs to be set up. a ~ normal(5,2), b ~ normal(-1,0.5), sigma_cafe ~ exponential(1), sigma ~ exponential(1), Rho ~ lkj_corr(2) #i.e., the onion prior. Eta = 2 ) , data=d , chains=4 , cores=4 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 post &lt;- extract.samples(m14.1) dens( post$Rho[,1,2] , xlim=c(-1,1) ) # posterior R &lt;- rlkjcorr( 1e4 , K=2 , eta=2 ) # prior dens( R[,1,2] , add=TRUE , lty=2 ) Notice that the dotted line is the prior and the solid line is the posterior. We see that almost all of the mass is between zero, hence a negative relationship. That is as expected as that is what we modeled it for. Now we can plot the differences between the raw data values (blue points) hence fixed effects estimates. The open circles are the varying effects estimates. Hence we see that the varying effects model is producing a bit different results. That is the product of being able to differentiate the model between different categories (cafés and waiting time) in the data. # compute unpooled estimates directly from data a1 &lt;- sapply( 1:N_cafes , function(i) mean(wait[cafe_id==i &amp; afternoon==0]) ) b1 &lt;- sapply( 1:N_cafes , function(i) mean(wait[cafe_id==i &amp; afternoon==1]) ) - a1 # extract posterior means of partially pooled estimates post &lt;- extract.samples(m14.1) a2 &lt;- apply( post$a_cafe , 2 , mean ) b2 &lt;- apply( post$b_cafe , 2 , mean ) # plot both and connect with lines plot( a1 , b1 , xlab=&quot;intercept&quot; , ylab=&quot;slope&quot; , pch=16 , col=rangi2 , ylim=c( min(b1)-0.1 , max(b1)+0.1 ) , xlim=c( min(a1)-0.1 , max(a1)+0.1 ) ) points( a2 , b2 , pch=1 ) for ( i in 1:N_cafes ) lines( c(a1[i],a2[i]) , c(b1[i],b2[i]) ) # compute posterior mean bivariate Gaussian 14.15 Mu_est &lt;- c( mean(post$a) , mean(post$b) ) rho_est &lt;- mean( post$Rho[,1,2] ) sa_est &lt;- mean( post$sigma_cafe[,1] ) sb_est &lt;- mean( post$sigma_cafe[,2] ) cov_ab &lt;- sa_est*sb_est*rho_est Sigma_est &lt;- matrix( c(sa_est^2,cov_ab,cov_ab,sb_est^2) , ncol=2 ) # draw contours library(ellipse) for ( l in c(0.1,0.3,0.5,0.8,0.99) ) lines(ellipse(Sigma_est,centre=Mu_est,level=l), col=col.alpha(&quot;black&quot;,0.2)) Plot interpretation: The contours of the inferred population, thus the multivariate distribution that is learned from the data. We see that each ellipse is the quantiles of the data. hence 50 = 50% of the data. We see that we have a couple of observations with a high slope (top of the graph). We see that the model is skeptical of that. Although when it regularize that, it also alters the intercept, that is because the model knows that slope and intercept is correlated. 14.2 Advanced varying slopes library(rethinking) data(chimpanzees) d &lt;- chimpanzees d$block_id &lt;- d$block d$treatment &lt;- 1L + d$prosoc_left + 2L*d$condition dat &lt;- list( L = d$pulled_left, tid = d$treatment, actor = d$actor, block_id = as.integer(d$block_id) ) set.seed(4387510) m14.2 &lt;- ulam( alist( L ~ dbinom(1,p), logit(p) &lt;- g[tid] + alpha[actor,tid] + beta[block_id,tid], #&#39; After alpha we have a matrix consisting of actor and treatment #&#39; alpha = an actor and tid = treatment, hence for each actor there #&#39; is information about the given actor and its treatment. #&#39; It goes the same for the blocks, where the matrix consists of: #&#39; row values = block id and each column = treamtnent id # adaptive priors vector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor), #&#39; We jave a matrix consisting of 4 treatments and the actors #&#39; vector[4] = we have four different treatments #&#39; It is distributed as a multivariate normal distribution #&#39; Rho actor = a matrix, the model already knows that #&#39; sigma_actor = a vector of length 4 vector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block), # fixed priors g[tid] ~ dnorm(0,1), sigma_actor ~ dexp(1), Rho_actor ~ dlkjcorr(4), #The onion prior, eta = 4 to regulaize sigma_block ~ dexp(1), Rho_block ~ dlkjcorr(4) #the onion prior, just for the blovks ) , data=dat , chains=4 , cores=4 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 We see that we are getting a lot of divergent transitions, that is what we have seen before, thus we should do the non centered model. This is similar varying intercepts, although it is a bit more complex when having varying slopes. To do this we are going to apply a Cholesky method (Cholesky Magic) this is done internally. There is an example of this in the slides, although it appears to be outside of the course scope. Now we are going to do the uncentered model (respecify the model) the non-centered model is simply just a standardized model, where all data is standardized (z-normal) set.seed(4387510) m14.3 &lt;- ulam( alist( L ~ binomial(1,p), logit(p) &lt;- g[tid] + alpha[actor,tid] + beta[block_id,tid], #&#39; We have matrices for alpha and beta, rows = actor / block and columns = treatment id # adaptive priors - non-centered transpars&gt; matrix[actor,4]:alpha &lt;- #rows = actors, columns = treatments compose_noncentered( sigma_actor , L_Rho_actor , z_actor ), #&#39; Transparse is just fitting the code into a stan framework #&#39; L_Rho_actor is some cholesky scores and z_actor is some z-scores for each actor transpars&gt; matrix[block_id,4]:beta &lt;- compose_noncentered( sigma_block , L_Rho_block , z_block ), matrix[4,actor]:z_actor ~ normal( 0 , 1 ), matrix[4,block_id]:z_block ~ normal( 0 , 1 ), # fixed priors g[tid] ~ normal(0,1), vector[4]:sigma_actor ~ dexp(1), cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2 ), #The correlation between actor and treatment #&#39; cholesky version of LKJ #&#39; LKJ distribution is still used vector[4]:sigma_block ~ dexp(1), cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ), #The correlation between block and treatment # compute ordinary correlation matrixes from Cholesky factors gq&gt; matrix[4,4]:Rho_actor &lt;&lt;- Chol_to_Corr(L_Rho_actor), gq&gt; matrix[4,4]:Rho_block &lt;&lt;- Chol_to_Corr(L_Rho_block) ) ,data=dat ,chains=4 ,cores=4 ,log_lik=TRUE ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 Now we will see that the model produce much more efficient results, although the result is the exact same, the model is simply just able to explore the probability regions more effectively. We see from the following plot that the effective samples fro the non-centered model is far better. # extract n_eff values for each model neff_nc &lt;- precis(m14.3,3,pars=c(&quot;alpha&quot;,&quot;beta&quot;))$n_eff neff_c &lt;- precis(m14.2,3,pars=c(&quot;alpha&quot;,&quot;beta&quot;))$n_eff plot( neff_c , neff_nc , xlab=&quot;centered (default)&quot; , ylab=&quot;non-centered (cholesky)&quot; , lwd=1.5 ) abline(a=0,b=1,lty=2) 14.2.0.1 Inference! Here we will see the effects, the first 4 being the actor effects (one for each treatment) and the same principle with the blocks, one for each treamt precis( m14.3 , depth=2 , pars=c(&quot;sigma_actor&quot;,&quot;sigma_block&quot;) ) x 1.3769325 0.8929684 1.8338578 1.5753516 0.4065954 0.4319259 0.2968035 0.4580187 x 0.4818628 0.4216382 0.5425387 0.6021124 0.3257347 0.3365054 0.2845219 0.3720914 x 0.7620038 0.3914742 1.1096785 0.8333287 0.0321586 0.0348733 0.0228155 0.0382124 x 2.2249059 1.6097088 2.7898991 2.6362733 0.9931536 1.0264000 0.7778716 1.1011438 x 947.8734 895.9099 1452.2190 1363.9190 1067.7924 948.9628 1209.0866 1152.5056 x 1.0027765 1.0044496 0.9999155 1.0026105 1.0029098 1.0022410 0.9998106 1.0002882 One entry for each treatment. We see that the mean of the blocks is more or less the same, that is because the model has learned that the blocks are basically the same. Although we see that the mean for sigma actor (variance for each chimp) is the lowest overall. Thus we see for the leftie chimp, the second treatment is also where there is the greatest probability of not pulling the left lever (i.e., lowest probability of pulling the left lever) Now we can intepret how the estimated data compared to the raw data. # compute mean for each actor in each treatment pl &lt;- by( d$pulled_left , list( d$actor , d$treatment ) , mean ) # generate posterior predictions using link datp &lt;- list( actor=rep(1:7,each=4) , tid=rep(1:4,times=7) , block_id=rep(5,times=4*7) ) p_post &lt;- link( m14.3 , data=datp ) p_mu &lt;- apply( p_post , 2 , mean ) p_ci &lt;- apply( p_post , 2 , PI ) # set up plot plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab=&quot;&quot; , ylab=&quot;proportion left lever&quot; , xaxt=&quot;n&quot; , yaxt=&quot;n&quot; ) axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) ) abline( h=0.5 , lty=2 ) for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 ) for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat(&quot;actor &quot;,j) , xpd=TRUE ) xo &lt;- 0.1 # offset distance to stagger raw data and predictions # raw data for ( j in (1:7)[-2] ) { lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 ) lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 ) } points( 1:28-xo , t(pl) , pch=16 , col=&quot;white&quot; , cex=1.7 ) points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 ) yoff &lt;- 0.175 text( 1-xo , pl[1,1]-yoff , &quot;R/N&quot; , pos=1 , cex=0.8 ) text( 2-xo , pl[1,2]+yoff , &quot;L/N&quot; , pos=3 , cex=0.8 ) text( 3-xo , pl[1,3]-yoff , &quot;R/P&quot; , pos=1 , cex=0.8 ) text( 4-xo , pl[1,4]+yoff , &quot;L/P&quot; , pos=3 , cex=0.8 ) # posterior predictions for ( j in (1:7)[-2] ) { lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 ) lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 ) } for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 ) points( 1:28+xo , p_mu , pch=16 , col=&quot;white&quot; , cex=1.3 ) points( 1:28+xo , p_mu , pch=c(1,1,16,16) ) Interpretation: blue points is the raw data and the black poinst are the posterior predictions We see the actor is simply just very left handed. If we then look at the meean for actor 2, we see that it is the lowest, that is because this is shrunk the most. 14.2.0.2 Conclusions - multilevel horoscopes Basically people tend to want defaults model given the data although there is no such thing. you should follow this flow: Think about the causal model first Begin with ‘empty’ model with varying intercept on relevant clusters Standardize predictors Use regularizing priors (simulate) Add in predictors and vary their slopes Can drop varying effects with tiny sigmas Consider two sorts of posterior prediction Same units: What happened in these data? New units: What might we expect for new units? Your knowledge of domain trumps all 14.3 Exercises 14.3.1 E1 Add to the following model varying slopes on the predictor x. notice that in the new edition we just use exponential instead of halfcauchy. \\[ \\begin{align*} \\text{First the probability of the data and the linear model} \\\\ \\\\ y_i \\sim Normial(\\mu_i,\\sigma) &amp;&amp; \\text{Likelihood} \\\\ \\mu_i = \\alpha_{GROUP[i]} + \\beta_{GROUP[i]} x_i &amp;&amp; \\text{Linear model} \\\\ \\\\ \\text{Then matrix of varying intercepts and slopes, with its covariance matrix} \\\\ \\\\ \\begin{bmatrix} \\alpha_{GROUP[i]} \\\\ \\beta_{GROUP[i]} \\\\ \\end{bmatrix} \\sim MVNormal( \\begin{bmatrix} \\alpha \\\\ \\beta \\\\ \\end{bmatrix} ,S) &amp;&amp; \\text{Population of varying effects} \\\\ S = \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\end{pmatrix} R \\begin{pmatrix} \\sigma_\\alpha &amp; 0 \\\\ 0 &amp; \\sigma_\\beta \\\\ \\end{pmatrix} &amp;&amp; \\text{construct covariance matrix} \\\\ \\\\ \\text{Then the hyper priors} \\\\ \\\\ \\alpha \\sim Normal(0,10) \\\\ \\beta \\sim Normal(0,1) \\\\ \\sigma \\sim Exponential(1) \\\\ \\sigma\\_\\alpha \\sim Exponential(1) \\\\ R \\sim LKJcorr(2) &amp;&amp; \\text{We assume intecept / slope correlations but not too strong} \\end{align*} \\] Notice that the MVNormal([]) means a matrix, that being the [] inside a matrix, that being the (). Notes on LKJCorr, notice that this distribution is the assumed relationship between slopes and intercepts, the LKJCorr is good distriubtion to make these assimptions, examples: We see that it goes from -1 to 1, thus we are able to use other distributions although this just suits very well. The higher the LKJCorr the closer to 0 are we certain that the correlation is. 14.3.2 E3 When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain We saw in the exercise and in the chapter, that the multilevel model is having an adaptive regularization, so if we see that there is little variance in the data we will observe that the multilevel model is able to regularize the model and thus recognize that it needs less ‘wiggle’ to fit the model. 14.3.3 H1 Let’s revisit the Bangladesh fertility data, data(bangladesh), from the practice problems for Chapter 13. Fit a model with both varying intercepts by district_id and varying slopes of urban by district_id. You are still predicting use contraception. Inspect the correlation between the intercepts and slopes. Can you interpret this correlation, in terms of what it tells you about the pattern of contraceptive use in the sample? It might help to plot the mean (or median) varying effect estimates for both the intercepts and slopes, by district. Then you can visualize the correlation and maybe more easily think through what it means to have a particular correlation. Plotting predicted proportion of women using contraception, with urban women on one axis and rural on the other, might also help. library(rethinking) data(bangladesh) d &lt;- bangladesh d$district_id &lt;- as.integer(as.factor(d$district)) dat_list &lt;- list( C = d$use.contraception, did = d$district_id ) 14.3.4 H2 "],["ch-14-adventures-in-covariance.html", "15 Ch 14 - Adventures in Covariance 15.1 Ch 14.3 - Instruments and causal designs 15.2 (Gaussian Processes) Continuous categories and the Gaussian process 15.3 Exercises", " 15 Ch 14 - Adventures in Covariance 15.1 Ch 14.3 - Instruments and causal designs Sometimes it may not be possible to close all backdoors in a DAG. There are different approaches to this, the most famous is first represented and then alternatives are presented. 15.1.1 Instrumental variables Althoguh in rare occations it may be possible to specify the model otherwise and include an instrumental variable to enable correct inferrential properties of a model. If the model is misspecified, then there is a possibility of the coefficients for the model parameters being wrong. Hence inference bbeing done on this model will be wrong. Thus empasizing the importance of evaluating the DAGs Let say we have the following DAG. We see that U is an observed variable and has a forking property, where it influence both W and E. Although it is unobserved hence we cannot include it in the model. Hence there is a backdoor! (confounding effect). We may be able to solve this with an instrumental variable. For an instrumental variable to be valid it needs to fulfill the following properties. He makes an example of regressing education on wage, where we also may expect that lazyness is a deterministic factor, although we cannot observe that, so by including age we also indicrectly include lazyness as high lazyness may mean less education but also on lower wages. Thus, we have opened a backdoor and sit with a confounding effect. The following section elaborates on how an instrument can be added. Must be independent of the unobserved U, hence Q independent of U Must not be dependent of the observed variable that we want to make inference on. Q (instrumental variable) cannot influence the target variable, as that would create a babckdoor. Code example, we are going to simulate some data so we know the relations. library(dagitty) library(rethinking) dag &lt;- dagitty( &quot;dag { U [unobserved] E [exposure] W [outcome] U -&gt; E U -&gt; W E -&gt; W }&quot; ) drawdag(dag) We want to measure E’s effect on W, although by including E we will also open the backdoor to U, which we cannot control for as it is unknown. Thus we are going to add the instrument variable Q. We simulate the effect so Education correltion on wages is 0. set.seed(73) N &lt;- 500 U_sim &lt;- rnorm( N ) Q_sim &lt;- sample( 1:4 , size=N , replace=TRUE ) E_sim &lt;- rnorm( N , U_sim + Q_sim ) #E = education W_sim &lt;- rnorm( N , U_sim + 0*E_sim ) #0 * E_sim = 500 zeros, W = wages dat_sim &lt;- list( W=standardize(W_sim) , E=standardize(E_sim) , Q=standardize(Q_sim) ) Adding Q in the dag. library(dagitty) library(rethinking) dag &lt;- dagitty( &quot;dag { U [unobserved] E [exposure] W [outcome] U -&gt; E U -&gt; W E -&gt; W Q -&gt; E }&quot; ) drawdag(dag) We see that Q and U are completely unrelated to other variables. E is based on U and Q and and W is based on U. The following presents three types of models, two wrong (misuses) and one good solution: 15.1.1.1 1. Only including E (we add confounding effect sideeffect of E) m14.4 &lt;- ulam( alist( W ~ dnorm( mu , sigma ), mu &lt;- aW + bEW*E, #By adding E also U is leaking unwanted information aW ~ dnorm( 0 , 0.2 ), bEW ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ) , data=dat_sim , chains=4 , cores=4) precis(m14.4) x 0.0012022 0.3972923 0.9188476 x 0.0403765 0.0397395 0.0311698 x -0.0634736 0.3354129 0.8699631 x 0.0660798 0.4583310 0.9709725 x 1706.487 1786.064 1846.831 x 1.0029059 1.0015440 0.9990899 We see that U is ruining the inference. Thus we might report that Education is increasing wages. Although it is in fact wrong. In this simulation we set the effect of E on W to be around 0. 15.1.1.2 2. Adding instrument Q (the wrong way) m14.5 &lt;- ulam( alist( W ~ dnorm( mu , sigma ), mu &lt;- aW + bEW*E + bQW*Q, #Add the effect of Q aW ~ dnorm( 0 , 0.2 ), bEW ~ dnorm( 0 , 0.5 ), bQW ~ dnorm( 0 , 0.5 ), sigma ~ dexp( 1 ) ) , data=dat_sim , chains=4 , cores=4 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 precis( m14.5 ) x -0.0007338 0.6352751 -0.4040064 0.8574049 x 0.0390265 0.0462069 0.0466945 0.0262698 x -0.0627500 0.5607676 -0.4800965 0.8164784 x 0.0621576 0.7100575 -0.3304081 0.9012749 x 2060.013 1426.895 1153.751 2309.213 x 1.0001492 1.0026464 1.0034973 0.9992567 We see that by simply adding the effect of Q, we are still having a back door and this makes the effect of E on W look even higher, THAT IS WRONG! 15.1.1.3 3. Correct use of the instrument variable The answer is to go multi level (create a generative model). This implies writing 4 functions (4 sub models) \\[ \\text{1. How wages are caused by education and U} \\\\ W_i \\sim Normal(\\mu_{w,i},\\sigma_w) \\\\ \\mu_{w,i} = \\alpha_w + \\beta_{EW}E_i + U_i \\\\ \\text{2. How education levels are caused by quarter of birth Q} \\\\ E_i \\sim Normal(\\mu_{E,i},\\sigma_E) \\\\ \\mu_{E,i} = \\alpha_E + \\beta_{QE}Q_i+U_i \\\\ \\text{3. The mode for Q (unif. dist.)} \\\\ Q_I \\sim Categorical([0.25,0.25,0.25,0.25]) \\\\ \\text{4. Specifying distribution for Q} \\\\ U_i \\sim Normal(0,1) \\] Notice that U could have had another distribution. Now we can define this as a statistical model, defining the joint distribution of wage and education. This basically means that each W and E get their own linear models (y = a + bx) as we have two mean values. \\[ \\begin{pmatrix} W_i \\\\ E_i \\end{pmatrix} \\sim MVNormal( \\begin{pmatrix} \\mu_{w,i} \\\\ \\mu_{E,i} \\end{pmatrix} , S ) \\\\ \\mu_{w,i} = \\alpha_W + \\beta_{EW}E_i \\\\ \\mu_{E,i} = \\alpha_E + \\beta_{QE}Q_i \\] Notice that we have: A linear model with education consists of some intercept and a slope for the education A linear model for the quarter of birth (instrument variable) Thus we estimate two linear models and we deal with the confound by doing the MVNormal, where S (the correlation parameter) gives information about how strong the confound is. Thus we just care about the correlation matrix so we can adjust the effect of E according to the effect of Q m14.6 &lt;- ulam( alist( c(W,E) ~ multi_normal( c(muW,muE) , Rho , Sigma ), muW &lt;- aW + bEW*E, #Mean wage muE &lt;- aE + bQE*Q, #Mean education c(aW,aE) ~ normal( 0 , 0.2 ), # c(bEW,bQE) ~ normal( 0 , 0.5 ), Rho ~ lkj_corr( 2 ), Sigma ~ exponential( 1 ) ), data=dat_sim , chains=4 , cores=4 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 precis( m14.6 , depth=3 ) x 0.0005356 0.0000560 0.5908850 -0.0446290 1.0000000 0.5390221 0.5390221 1.0000000 1.0215317 0.8079289 x 0.0362346 0.0457417 0.0352491 0.0755883 0.0000000 0.0523193 0.0523193 0.0000000 0.0453258 0.0257300 x -0.0572324 -0.0741125 0.5347942 -0.1647677 1.0000000 0.4517490 0.4517490 1.0000000 0.9517784 0.7686491 x 0.0585893 0.0724464 0.6475532 0.0786892 1.0000000 0.6207480 0.6207480 1.0000000 1.0952362 0.8513578 x 1544.451 1512.802 1334.902 1105.227 NaN 1010.346 1010.346 1311.056 1143.368 1625.887 x 1.0021929 0.9987730 1.0005552 0.9990276 NaN 0.9995543 0.9995543 0.9979980 0.9999386 1.0003926 Now we see that the slope for E is 0 (and the compatibility intercals on both sides of 0), while the interaction between E and Q has all the effect. Thus we have got rid of the confounding variable (unknown U). Now there are also a bunch of other parameters, but the slopes for E and QE are the most important. Key takeaway: in general it is not possibile to statistically prove if a variable is a good instrument. This is where domain knowledge is relevant!! Notice that it is almost impossible to know the ground truth. Thus emphasizing that we must construct a meaningful DAG, in which we must reason for and believe in!! 15.1.2 Other designs 15.1.2.1 Front-Door Criterion Lets suppose we have a scenario where we want to measure X effect on Y, although we have an unknown U, that is a confounding effect, as it affects both X and Y, hence exposure and outcome. library(dagitty) library(rethinking) dag &lt;- dagitty( &quot;dag { U [unobserved] X [exposure] Y [outcome] U -&gt; X U -&gt; Y X -&gt; Z Z -&gt; Y }&quot; ) Another way to solve this is find a Front-door criterion, Z in this example, which is independent of U, as when only Z is introduced the effect of U is not included in the model. WHY? Recall that we have a pipe from X to Y and when we condition on Z we close the relationship between X and Y and thus also U. 15.1.2.2 Regression discontinuity (RDD) The example of regressing academic reward on future success, where many unobserved variables occur. See the example on page 461. 15.2 (Gaussian Processes) Continuous categories and the Gaussian process This section is about including continuous variables, as up until now we have only been working with unordered categorical variables. Thus this also allows for continuous varying effects. This is used to measure similarity bbased on a continuous scale, where we up until now only have looked at similarity in unordered categories such as tadpoles. 15.2.1 Example: Spatial autocorrelation in Oceanic tools This example use the tool counts for a set of islands and also includes latitude and longitude data to regress population and tool counts with respect to their geological position. 15.2.1.1 Loading data + defining model # load the distance matrix library(rethinking) data(islandsDistMatrix) # display (measured in thousands of km) Dmat &lt;- islandsDistMatrix colnames(Dmat) &lt;- c(&quot;Ml&quot;,&quot;Ti&quot;,&quot;SC&quot;,&quot;Ya&quot;,&quot;Fi&quot;,&quot;Tr&quot;,&quot;Ch&quot;,&quot;Mn&quot;,&quot;To&quot;,&quot;Ha&quot;) round(Dmat,1) ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Malekula 0.0 0.5 0.6 4.4 1.2 2.0 3.2 2.8 1.9 5.7 ## Tikopia 0.5 0.0 0.3 4.2 1.2 2.0 2.9 2.7 2.0 5.3 ## Santa Cruz 0.6 0.3 0.0 3.9 1.6 1.7 2.6 2.4 2.3 5.4 ## Yap 4.4 4.2 3.9 0.0 5.4 2.5 1.6 1.6 6.1 7.2 ## Lau Fiji 1.2 1.2 1.6 5.4 0.0 3.2 4.0 3.9 0.8 4.9 ## Trobriand 2.0 2.0 1.7 2.5 3.2 0.0 1.8 0.8 3.9 6.7 ## Chuuk 3.2 2.9 2.6 1.6 4.0 1.8 0.0 1.2 4.8 5.8 ## Manus 2.8 2.7 2.4 1.6 3.9 0.8 1.2 0.0 4.6 6.7 ## Tonga 1.9 2.0 2.3 6.1 0.8 3.9 4.8 4.6 0.0 5.0 ## Hawaii 5.7 5.3 5.4 7.2 4.9 6.7 5.8 6.7 5.0 0.0 We have a distance matrix and we see the flight distance between each island. Notice that we want to use distance as a similarity in technology exposure. We can define the model for number of tools with: \\[ T_I \\sim Poisson(\\lambda_i)\\\\ \\lambda_i = \\alpha P_i^\\beta/\\gamma \\] Now we can adjust lambda with a varying intercept for each society. We also want the value to always be positive, hence we add exponential for each society. We add the index for each society, this will yield a multiple intercept model. Thus the modified formula: \\[ T_I \\sim Poisson(\\lambda_i)\\\\ \\lambda_i = exp(k_{societry[i]})\\alpha P_i^\\beta/\\gamma \\] Recall that exp(0) = 1 Thus we multply the original contents with the exponential category k. Thus we see that \\(k_{societry[i]\\) is the varting intercept. We need to define a prior for the varying intercepts, that is done by: \\[ \\text{Prior for the varying intercepts:} \\\\ \\begin{pmatrix} k_1 \\\\ k_2 \\\\ k_3 \\\\ ... \\\\ k_10 \\end{pmatrix} \\sim MVNormal \\begin{pmatrix} \\begin{pmatrix} 0 \\\\ 0 \\\\ 0 \\\\ ... \\\\ 0 \\end{pmatrix} ,K \\end{pmatrix} \\\\ \\text{Covariance matrix K:} \\\\ K_{ij} = \\eta^2 exp(-\\rho^2D^2_{ij})+\\delta_{ij}\\sigma^2 \\] We see that: First expression is a 10-dimensional gaussian prior for the intercepts. There are 10 societies and thus one intercept for each society. Notice that \\(K_{ij}\\) is a matrix, as we have two variables. Thus the \\(K_{ij}\\) is able to identify a correlation between two variables (islands). Ultimately it creates a matrix including numbers for the equation \\(K_{ij}\\), elaborated a bit in the following. the K parameter is defined by three parameters - eta, rho and sigma. And the purpose of K is to set the covariance between societies as the distances between them change. The K parameter can be separated into three parts: \\(exp(-\\rho^2D^2_{ij})\\) Shape of K. \\(D^2_{ij}\\) - It is the squared distance between i and j. \\(\\eta^2\\) - The maximum covariance between any two societies i and j. \\(\\delta_{ij}\\sigma^2\\) - The additional variance, this can be turned on and off. where sigma squared is simply just the variance where delta is able to turn it on and off. (for some reason this we do not have to care too much abbout) More on the first part: Shape of K The shape of K is \\(exp(-\\rho^2D^2_{ij})\\), where \\(D^2_{ij}\\) is the distance between the i’th and j’th object (between two societies). We simply see that the larger the distance the lower is the correlation among the two islands. Two toy examples where rho is fixed: options(scipen = 999) rho = 0.1 D = 10 #D = distance exp(-rho^2*D^2) ## [1] 0.3678794 rho = 0.1 D = 1 #two islands 10 times closer than the previous example exp(-rho^2*D^2) ## [1] 0.9900498 We simply see that the closer the islands the larger the expected similarity. Parameter \\(\\rho\\): The rate of decline. When it is large, the covariance declines rapidly with squared distance. rho = 0.1 D = 1 #two islands 10 times closer than the previous example exp(-rho^2*D^2) rho = 1 D = 1 #two islands 10 times closer than the previous example exp(-rho^2*D^2) ## [1] 0.9900498 ## [1] 0.3678794 Now we see that the smaller rho (declining similarity) that larger is the similarity given that distance is fixed. Why square the distance???: you don’t have to, they just do that. We can plot two examples where one is squared and one is just the distance: # linear curve( exp(-1*x) , from=0 , to=4 , lty=2 , xlab=&quot;distance&quot; , ylab=&quot;correlation&quot; ) # squared curve( exp(-1*x^2) , add=TRUE ) mtext(&quot;Solid = squared distances, striped = linear distances.&quot;) Now we see that correlation decrease as distance increase between islands. Thus naturally we see that the islands correlation is 1 to itself and then declines from there. Notice that this i kind of a half normal distribution. Notice that the linear has some kind of linear decrease, where the squared example takes a while before decreasing, hence we do account for close islands to be collaborating, while the linear model will say that you quickly or almost immediately is different even from neighboring islands! 15.2.1.2 The full model \\[ T_i \\sim Poisson(\\lambda_i) \\\\ \\lambda_i = exp(k_{societry[i]})\\alpha P_i^\\beta/\\gamma \\\\ k \\sim MVNormal(0,...,0),K) \\\\ K_{ij} = \\eta^2 exp(-\\rho^2D^2_{ij})+\\delta_{ij}\\sigma^2\\\\ \\text{The priors:} \\\\ \\alpha \\sim Exponential(1) \\\\ \\beta \\sim Exponential(1) \\\\ \\eta^2 \\sim Exponential(2) \\\\ \\rho^2 \\sim Exponential(0.5) \\] Eta and rho must be positive for this example, hence we use the exponential distribution. k is a matrix based on the multivariate normal distribution, where, Each K element is defined by a certain distribution. Eta and rho squared does not have to be squared, but they are Notice that the code includes GPL2, that is simply Gaussian Process Prior for squared distance data(Kline2) # load the ordinary data, now with coordinates d &lt;- Kline2 d$society &lt;- 1:10 # index observations dat_list &lt;- list( T = d$total_tools, P = d$population, society = d$society, Dmat=islandsDistMatrix ) m14.8 &lt;- ulam( alist( T ~ dpois(lambda), lambda &lt;- (a*P^b/g)*exp(k[society]), vector[10]:k ~ multi_normal( 0 , SIGMA ), matrix[10,10]:SIGMA &lt;- cov_GPL2( Dmat , etasq , rhosq , 0.01 ), c(a,b,g) ~ dexp( 1 ), etasq ~ dexp( 2 ), #for eta squared rhosq ~ dexp( 0.5 ) #for rho squared ), data=dat_list , chains=4 , cores=4 , iter=2000 ) ## Running /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c ## clang -I&quot;/Library/Frameworks/R.framework/Resources/include&quot; -DNDEBUG -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/Rcpp/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/unsupported&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/BH/include&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/src/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppParallel/include/&quot; -I&quot;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/rstan/include&quot; -DEIGEN_NO_DEBUG -DBOOST_DISABLE_ASSERTS -DBOOST_PENDING_INTEGER_LOG2_HPP -DSTAN_THREADS -DBOOST_NO_AUTO_PTR -include &#39;/Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp&#39; -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1 -I/usr/local/include -fPIC -Wall -g -O2 -c foo.c -o foo.o ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:88: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:1: error: unknown type name &#39;namespace&#39; ## namespace Eigen { ## ^ ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:628:16: error: expected &#39;;&#39; after top level declarator ## namespace Eigen { ## ^ ## ; ## In file included from &lt;built-in&gt;:1: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/StanHeaders/include/stan/math/prim/mat/fun/Eigen.hpp:13: ## In file included from /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Dense:1: ## /Library/Frameworks/R.framework/Versions/4.0/Resources/library/RcppEigen/include/Eigen/Core:96:10: fatal error: &#39;complex&#39; file not found ## #include &lt;complex&gt; ## ^~~~~~~~~ ## 3 errors generated. ## make: *** [foo.o] Error 1 precis(m14.8,depth = 3) x -0.1597404 -0.0081989 -0.0574348 0.3693217 0.0910797 -0.3768972 0.1590307 -0.2009949 0.2821525 -0.1460112 0.6033752 0.2759142 1.4188937 0.1828687 1.3984071 x 0.2896103 0.2793731 0.2680856 0.2504507 0.2481793 0.2558091 0.2422138 0.2486070 0.2340778 0.3296204 0.5987057 0.0824483 1.1000008 0.1782422 1.6649221 x -0.6120088 -0.4358372 -0.4739379 0.0022704 -0.2841544 -0.7944999 -0.2140414 -0.5821251 -0.0611343 -0.6788603 0.0687307 0.1514212 0.2289676 0.0305899 0.0932570 x 0.3005815 0.4359577 0.3699686 0.7744077 0.4903225 0.0050948 0.5415994 0.1763924 0.6573243 0.3503978 1.6841055 0.4166303 3.4330089 0.4992089 4.6126826 x 1152.765 1091.713 1065.910 1172.896 1104.527 1333.654 1076.464 1145.790 1036.562 1048.215 1887.105 1392.449 2124.661 1364.828 2134.975 x 1.000283 1.001192 1.000922 1.000571 1.001655 1.000964 1.000455 1.000703 1.000547 1.002470 1.000475 1.002043 1.000081 1.000869 1.000430 precis(m14.8,depth = 1) x 0.6033752 0.2759142 1.4188937 0.1828687 1.3984071 x 0.5987057 0.0824483 1.1000008 0.1782422 1.6649221 x 0.0687307 0.1514212 0.2289676 0.0305899 0.0932570 x 1.6841055 0.4166303 3.4330089 0.4992089 4.6126826 x 1887.105 1392.449 2124.661 1364.828 2134.975 x 1.000475 1.002043 1.000081 1.000869 1.000430 15.2.1.3 Gaussian Process Posterior post &lt;- extract.samples(m14.8) # plot the posterior median covariance function plot( NULL , xlab=&quot;distance (thousand km)&quot; , ylab=&quot;covariance&quot; , xlim=c(0,10) , ylim=c(0,2),main = &quot;Posterior covariance function&quot;) # compute posterior mean covariance x_seq &lt;- seq( from=0 , to=10 , length.out=100 ) pmcov &lt;- sapply( x_seq , function(x) post$etasq*exp(-post$rhosq*x^2) ) pmcov_mu &lt;- apply( pmcov , 2 , mean ) lines( x_seq , pmcov_mu , lwd=2 ) # plot 60 functions sampled from posterior for ( i in 1:50 ) curve( post$etasq[i]*exp(-post$rhosq[i]*x^2) , add=TRUE , col=col.alpha(&quot;black&quot;,0.3) ) This is a covariance prior simulation. We see that the lower the distance the larger the covariance. That is expected as that is what we defined in the K parameter. The covariances are on the log scale. JUST AS ANYTHING ELSE IN A POISSON GLM!!!. # compute posterior median covariance among societies K &lt;- matrix(0,nrow=10,ncol=10) for ( i in 1:10 ) for ( j in 1:10 ) K[i,j] &lt;- median(post$etasq) * exp( -median(post$rhosq) * islandsDistMatrix[i,j]^2 ) diag(K) &lt;- median(post$etasq) + 0.01 #Convert K to a correlation matrix # convert to correlation matrix Rho &lt;- round( cov2cor(K) , 2 ) # add row/col names for convenience colnames(Rho) &lt;- c(&quot;Ml&quot;,&quot;Ti&quot;,&quot;SC&quot;,&quot;Ya&quot;,&quot;Fi&quot;,&quot;Tr&quot;,&quot;Ch&quot;,&quot;Mn&quot;,&quot;To&quot;,&quot;Ha&quot;) rownames(Rho) &lt;- colnames(Rho) Rho ## Ml Ti SC Ya Fi Tr Ch Mn To Ha ## Ml 1.00 0.77 0.67 0.00 0.27 0.03 0.00 0.00 0.06 0 ## Ti 0.77 1.00 0.86 0.00 0.27 0.04 0.00 0.00 0.04 0 ## SC 0.67 0.86 1.00 0.00 0.13 0.09 0.00 0.01 0.01 0 ## Ya 0.00 0.00 0.00 1.00 0.00 0.01 0.13 0.11 0.00 0 ## Fi 0.27 0.27 0.13 0.00 1.00 0.00 0.00 0.00 0.58 0 ## Tr 0.03 0.04 0.09 0.01 0.00 1.00 0.07 0.52 0.00 0 ## Ch 0.00 0.00 0.00 0.13 0.00 0.07 1.00 0.28 0.00 0 ## Mn 0.00 0.00 0.01 0.11 0.00 0.52 0.28 1.00 0.00 0 ## To 0.06 0.04 0.01 0.00 0.58 0.00 0.00 0.00 1.00 0 ## Ha 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1 Malekula (Ml), Tikopia (Ti), and Santa Cruz (SC)—are highly correlated, all above 0.8 with one another. We see that these islands are close and have similar tool totals. Correlations with Hawaii is 0 with all islands. All other societies have some relationship to the other societies in some way. This can be plotted: # scale point size to logpop psize &lt;- d$logpop / max(d$logpop) psize &lt;- exp(psize*1.5)-2 # plot raw data and labels plot( d$lon2 , d$lat , xlab=&quot;longitude&quot; , ylab=&quot;latitude&quot; , col=rangi2 , cex=psize , pch=16 , xlim=c(-50,30) ) labels &lt;- as.character(d$culture) text( d$lon2 , d$lat , labels=labels , cex=0.7 , pos=c(2,4,3,3,4,1,3,2,4,2) ) # overlay lines shaded by Rho for( i in 1:10 ) for ( j in 1:10 ) if ( i &lt; j ) lines( c( d$lon2[i],d$lon2[j] ) , c( d$lat[i],d$lat[j] ) , lwd=2 , col=col.alpha(&quot;black&quot;,Rho[i,j]^2) ) We see that the triad that had high correlations are closely arranged together. Darker lines = greater correlation No line = no correlation Now we can plot the islands with respect to the log population and the total no. of tools, while encoding as the population size. We are also suppesed to plot the median difference and the 80% percentage intervals. # compute posterior median relationship, ignoring distance logpop.seq &lt;- seq( from=6 , to=14 , length.out=30 ) #### FOR SOME REASON THIS IS MISCALCULATED!!! lambda &lt;- sapply( logpop.seq , function(lp) exp( post$a + post$b*lp ) ) lambda.median &lt;- apply( lambda , 2 , median ) lambda.PI80 &lt;- apply( lambda , 2 , PI , prob=0.80 ) # plot raw data and labels plot( d$logpop , d$total_tools , col=rangi2 , cex=psize , pch=16 ,xlab=&quot;log population&quot; , ylab=&quot;total tools&quot; ) text( d$logpop , d$total_tools , labels=labels , cex=0.7 ,pos=c(4,3,4,2,2,1,4,4,4,2) ) # display posterior predictions lines( logpop.seq , lambda.median , lty=2 ) # lines( logpop.seq , lambda.PI80[1,] , lty=2 ) # lines( logpop.seq , lambda.PI80[2,] , lty=2 ) # overlay correlations for( i in 1:10 ) for ( j in 1:10 ) if ( i &lt; j ) lines( c( d$logpop[i],d$logpop[j] ) , c( d$total_tools[i],d$total_tools[j] ) , lwd=2 , col=col.alpha(&quot;black&quot;,Rho[i,j]^2) ) In the book it looks the following: We see that the center line is the average prediction while the out bounds are the 80% compatibility intervals. We clearly see that there is a correlation and we see that some islands are above and others below the expected number of tools given their population size. 15.2.2 Example: Phylogenetic distance (not in the curriculum) This is yet another example where we measure distance, not in geographical way but we apply the distance of different monkey species (namely phylogenetic distance) and see if there is a correlation between the distance and the species. The end result is that there is very little covariance (when looking at the posterior distribution), thus implying that covariance is very little at any phylogenetic distance. The overall approach to get these results are similar and we do create a parameter K, including eta squared and exponential distribution, taking inputs from rho squared and the distance matrix. The section introduce some processes and methods, of which I don’t know if they have any relevance. Such as: Brownian motion Pagel’s Lambda Ornstain-Uhlenbeck Process 15.3 Exercises 15.3.1 M3 re-estiamte the varying slopes model for the UCBBadmit data, now using a non-centered parameterization. Compare the efficiency of the forms of the model, using n_eff. Which is better? Which chain sampled faster? #Loading the data library(rethinking) data(UCBadmit) d &lt;- UCBadmit dat_list &lt;- list( admit = d$admit, applications = d$applications, gid = ifelse( d$applicant.gender==&quot;male&quot; , 1 , 2 ), dep = rep(1:6,each=2) ) Previously we had the following model: The non centered model we will add adaptive priors. #From chapter 11 m1 &lt;- ulam( alist( admit ~ dbinom(applications, p), logit(p) &lt;- a[gid], a[gid] ~ dnorm(0,1) ) ,data = dat_list,chains = 1,cores = 1 ) ## ## SAMPLING FOR MODEL &#39;ff9c585b744194e1c435a1847ea630ad&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 1.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.19 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 1000 [ 0%] (Warmup) ## Chain 1: Iteration: 100 / 1000 [ 10%] (Warmup) ## Chain 1: Iteration: 200 / 1000 [ 20%] (Warmup) ## Chain 1: Iteration: 300 / 1000 [ 30%] (Warmup) ## Chain 1: Iteration: 400 / 1000 [ 40%] (Warmup) ## Chain 1: Iteration: 500 / 1000 [ 50%] (Warmup) ## Chain 1: Iteration: 501 / 1000 [ 50%] (Sampling) ## Chain 1: Iteration: 600 / 1000 [ 60%] (Sampling) ## Chain 1: Iteration: 700 / 1000 [ 70%] (Sampling) ## Chain 1: Iteration: 800 / 1000 [ 80%] (Sampling) ## Chain 1: Iteration: 900 / 1000 [ 90%] (Sampling) ## Chain 1: Iteration: 1000 / 1000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.011038 seconds (Warm-up) ## Chain 1: 0.008412 seconds (Sampling) ## Chain 1: 0.01945 seconds (Total) ## Chain 1: set.seed(13) m2 &lt;- ulam( alist( admit ~ dbinom( applications , p ) , logit(p) &lt;- delta[dept_id] + bm[dept_id]*male, c(delta,bbm) ) ,data=dat_list ,chains=1 ,cores=1 ,log_lik=TRUE ) We see that the sampler is not super efficient, although it is okay. Rhat should approach 1, that is looking okay. 15.3.2 M4 The result of this exercise will compare three different models and expecrience that the more complex model is superior to the two other models. library(rethinking) data(Kline2) d &lt;- Kline2 d$society &lt;- 1:10 #Index observations d$contact_id &lt;- ifelse(d$contact == &#39;high&#39;,2,1) #If the contact was high head(d) culture population contact total_tools mean_TU lat lon lon2 logpop society contact_id Malekula 1100 low 13 3.2 -16.3 167.5 -12.5 7.003065 1 1 Tikopia 1500 low 22 4.7 -12.3 168.8 -11.2 7.313220 2 1 Santa Cruz 3600 low 24 4.0 -10.7 166.0 -14.0 8.188689 3 1 Yap 4791 high 43 5.0 9.5 138.1 -41.9 8.474494 4 2 Lau Fiji 7400 high 33 5.0 -17.7 178.1 -1.9 8.909235 5 2 Trobriand 8000 high 19 4.0 -8.7 150.9 -29.1 8.987197 6 2 Now we see the different islands, their population, the contact level, the total tools. Their position, log op population, and the two variables society and contact id that we created We are also going to load in the islands dastiance matrix data(&quot;islandsDistMatrix&quot;) head(islandsDistMatrix) ## Malekula Tikopia Santa Cruz Yap Lau Fiji Trobriand Chuuk Manus ## Malekula 0.000 0.475 0.631 4.363 1.234 2.036 3.178 2.794 ## Tikopia 0.475 0.000 0.315 4.173 1.236 2.007 2.877 2.670 ## Santa Cruz 0.631 0.315 0.000 3.859 1.550 1.708 2.588 2.356 ## Yap 4.363 4.173 3.859 0.000 5.391 2.462 1.555 1.616 ## Lau Fiji 1.234 1.236 1.550 5.391 0.000 3.219 4.027 3.906 ## Trobriand 2.036 2.007 1.708 2.462 3.219 0.000 1.801 0.850 ## Tonga Hawaii ## Malekula 1.860 5.678 ## Tikopia 1.965 5.283 ## Santa Cruz 2.279 5.401 ## Yap 6.136 7.178 ## Lau Fiji 0.763 4.884 ## Trobriand 3.893 6.653 Naturally we see that it is 0 on the diagonal, as the distance from an island to itself is 0. Now we can construct the dataset. dat_list &lt;- list( T = d$total_tools, P = d$population, society = d$society, Dmat = islandsDistMatrix ) dat2 &lt;- list( T = d$total_tools, P = d$population, cid = d$contact_id ) head(dat_list) ## $T ## [1] 13 22 24 43 33 19 40 28 55 71 ## ## $P ## [1] 1100 1500 3600 4791 7400 8000 9200 13000 17500 275000 ## ## $society ## [1] 1 2 3 4 5 6 7 8 9 10 ## ## $Dmat ## Malekula Tikopia Santa Cruz Yap Lau Fiji Trobriand Chuuk Manus ## Malekula 0.000 0.475 0.631 4.363 1.234 2.036 3.178 2.794 ## Tikopia 0.475 0.000 0.315 4.173 1.236 2.007 2.877 2.670 ## Santa Cruz 0.631 0.315 0.000 3.859 1.550 1.708 2.588 2.356 ## Yap 4.363 4.173 3.859 0.000 5.391 2.462 1.555 1.616 ## Lau Fiji 1.234 1.236 1.550 5.391 0.000 3.219 4.027 3.906 ## Trobriand 2.036 2.007 1.708 2.462 3.219 0.000 1.801 0.850 ## Chuuk 3.178 2.877 2.588 1.555 4.027 1.801 0.000 1.213 ## Manus 2.794 2.670 2.356 1.616 3.906 0.850 1.213 0.000 ## Tonga 1.860 1.965 2.279 6.136 0.763 3.893 4.789 4.622 ## Hawaii 5.678 5.283 5.401 7.178 4.884 6.653 5.787 6.722 ## Tonga Hawaii ## Malekula 1.860 5.678 ## Tikopia 1.965 5.283 ## Santa Cruz 2.279 5.401 ## Yap 6.136 7.178 ## Lau Fiji 0.763 4.884 ## Trobriand 3.893 6.653 ## Chuuk 4.789 5.787 ## Manus 4.622 6.722 ## Tonga 0.000 5.037 ## Hawaii 5.037 0.000 m11.11 &lt;- ulam( alist( T ~ dpois( lambda ), #T = total number of tools lambda &lt;- exp(a[cid])*P^b[cid]/g, #P = population size a[cid] ~ dnorm(1,1), b[cid] ~ dexp(1), g ~ dexp(1) ), data=dat2 , chains=1 , iter=2000 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;58422f20040c774e9740e486280fe76b&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.4e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.24 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.332709 seconds (Warm-up) ## Chain 1: 0.286882 seconds (Sampling) ## Chain 1: 0.619591 seconds (Total) ## Chain 1: #In this model we simply have an exponential function for alpha as well m11.12 &lt;- ulam( alist( T ~ dpois( lambda ), lambda &lt;- a[cid]*P^b[cid]/g, a[cid] ~ dexp(1), b[cid] ~ dexp(1), g ~ dexp(1) ), data=dat2 , chains=1 , iter=2000 , log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;2df9af959b40e14e7671c593bf1229d1&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 2.2e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.22 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 0.308024 seconds (Warm-up) ## Chain 1: 0.302042 seconds (Sampling) ## Chain 1: 0.610066 seconds (Total) ## Chain 1: #In this model we will include the distance matrix to measure similarity m14.8 &lt;- ulam( alist( T ~ dpois(lambda), lambda &lt;- (a*P^b/g)*exp(k[society]), vector[10]:k ~ multi_normal( 0 , SIGMA ), #Multivariate normal distribution for the distance matrix matrix[10,10]:SIGMA &lt;- cov_GPL2( Dmat , etasq , rhosq , 0.01 ), c(a,b,g) ~ dexp( 1 ), #g = the Gaussian process varying intercepts for each society etasq ~ dexp( 2 ), rhosq ~ dexp( 0.5 ) ), data=dat_list , chains=1 , iter=2000, log_lik=TRUE ) ## ## SAMPLING FOR MODEL &#39;88f2ad7a384c819633238280368dab9f&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 7.9e-05 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.79 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 2000 [ 0%] (Warmup) ## Chain 1: Iteration: 200 / 2000 [ 10%] (Warmup) ## Chain 1: Iteration: 400 / 2000 [ 20%] (Warmup) ## Chain 1: Iteration: 600 / 2000 [ 30%] (Warmup) ## Chain 1: Iteration: 800 / 2000 [ 40%] (Warmup) ## Chain 1: Iteration: 1000 / 2000 [ 50%] (Warmup) ## Chain 1: Iteration: 1001 / 2000 [ 50%] (Sampling) ## Chain 1: Iteration: 1200 / 2000 [ 60%] (Sampling) ## Chain 1: Iteration: 1400 / 2000 [ 70%] (Sampling) ## Chain 1: Iteration: 1600 / 2000 [ 80%] (Sampling) ## Chain 1: Iteration: 1800 / 2000 [ 90%] (Sampling) ## Chain 1: Iteration: 2000 / 2000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 2.02199 seconds (Warm-up) ## Chain 1: 1.94882 seconds (Sampling) ## Chain 1: 3.9708 seconds (Total) ## Chain 1: compare(m11.11,m11.12,m14.8) x 68.53786 80.10445 81.23926 x 2.707832 11.267383 11.533889 x 0.00000 11.56659 12.70140 x NA 11.22271 11.55810 x 4.527584 4.856916 5.468303 x 0.9951991 0.0030638 0.0017371 We see that model 14.8 is the best model looking at the out of sample estimate. Also it has the fewest effective parameters, despite being the model with most absolute parameters. That is because we have a varying effects model, and due to the pooling effects, we are able to have less effective parameters. As by observing one society we also get to know a bit for another society. 15.3.3 H2 We are going to draw the following DAG. library(dagitty) library(rethinking) dag_6M1 &lt;- dagitty(&quot;dag{ A -&gt; C A -&gt; K -&gt; C }&quot;) coordinates(dag_6M1) &lt;- list( x=c(A=0,K=2,C=1), y=c(A=0,K=0, C=1) ) drawdag(dag_6M1) We want to predict the number of kids (C). By including A we will also include some infromation about K as there is causality from age to K, that e.g., if you are old you dont want more kids, thus you start using contraception. Thus we need two models: One for the effect of A on C One for the effect of A through K. library(rethinking) data(bangladesh) d &lt;- bangladesh dat_list &lt;- list( C = d$use.contraception, did = as.integer( as.factor(d$district) ), urban = d$urban ) dat_list$children &lt;- standardize( d$living.children ) dat_list$age &lt;- standardize( d$age.centered ) m14H2.1 &lt;- ulam( alist( C ~ bernoulli( p ), logit(p) &lt;- a[did] + b[did]*urban + bA*age, c(a,b)[did] ~ multi_normal( c(abar,bbar) , Rho , Sigma ), abar ~ normal(0,1), c(bbar,bA) ~ normal(0,0.5), Rho ~ lkj_corr(2), Sigma ~ exponential(1) ) , data=dat_list , chains=4 , cores=4 , iter=1000 ) 15.3.4 H4 Varying intercepts Varying slopes Predict height using age clustered by subject (Person). We are going to do a varying intercepts and varying slopes model data(Oxboys) d &lt;- Oxboys dat_list &lt;- list( subject = d$Subject, age = d$age, height = d$height ) head(d) Subject age height Occasion 1 -1.0000 140.5 1 1 -0.7479 143.4 2 1 -0.4630 144.8 3 1 -0.1643 147.1 4 1 -0.0027 147.7 5 1 0.2466 150.2 6 library(rethinking) data(Oxboys) d &lt;- Oxboys d$A &lt;- standardize( d$age ) d$id &lt;- coerce_index( d$Subject ) m14H4.1 &lt;- ulam( alist( height ~ dnorm( mu , sigma ), mu &lt;- a_bar + a[id] + (b_bar + b[id])*A, a_bar ~ dnorm(150,10), #mean height b_bar ~ dnorm(0,10), #mean slope for all persons c(a,b)[id] ~ multi_normal( 0 , Rho_id , sigma_id ), #Correlation matrix sigma_id ~ dexp(1), #standard deciation for each person Rho_id ~ dlkjcorr(2), #the onion distribution for slope / intercepts correlation sigma ~ dexp(1) #standard deviation ), data=d , chains=1 , cores=1 , iter=4000 ) ## ## SAMPLING FOR MODEL &#39;fe0fd9bb3f480cc772124ed1119cf20e&#39; NOW (CHAIN 1). ## Chain 1: ## Chain 1: Gradient evaluation took 0.000199 seconds ## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 1.99 seconds. ## Chain 1: Adjust your expectations accordingly! ## Chain 1: ## Chain 1: ## Chain 1: Iteration: 1 / 4000 [ 0%] (Warmup) ## Chain 1: Iteration: 400 / 4000 [ 10%] (Warmup) ## Chain 1: Iteration: 800 / 4000 [ 20%] (Warmup) ## Chain 1: Iteration: 1200 / 4000 [ 30%] (Warmup) ## Chain 1: Iteration: 1600 / 4000 [ 40%] (Warmup) ## Chain 1: Iteration: 2000 / 4000 [ 50%] (Warmup) ## Chain 1: Iteration: 2001 / 4000 [ 50%] (Sampling) ## Chain 1: Iteration: 2400 / 4000 [ 60%] (Sampling) ## Chain 1: Iteration: 2800 / 4000 [ 70%] (Sampling) ## Chain 1: Iteration: 3200 / 4000 [ 80%] (Sampling) ## Chain 1: Iteration: 3600 / 4000 [ 90%] (Sampling) ## Chain 1: Iteration: 4000 / 4000 [100%] (Sampling) ## Chain 1: ## Chain 1: Elapsed Time: 15.3363 seconds (Warm-up) ## Chain 1: 15.7947 seconds (Sampling) ## Chain 1: 31.131 seconds (Total) ## Chain 1: precis( m14H4.1 , depth=2 , pars=c(&quot;a_bar&quot;,&quot;b_bar&quot;,&quot;sigma_id&quot;) ) x 149.589744 4.224061 7.269658 1.055018 x 1.2786225 0.1943985 0.8412816 0.1464204 x 147.3818161 3.9100682 6.0363687 0.8474578 x 151.552963 4.549907 8.723843 1.313094 x 83.66149 125.44741 1464.79695 1130.66298 x 1.0012601 1.0084685 0.9995069 0.9995083 What do we see from the model?: Notice that the data is on the standardized scale, hence one unit of x is one standard deviation. We have a noncentered model as the mean is taken out of the intercepts, that being a_bar Mu consists of mean height (a_bar), the intercept for each person and then b_bar is a mean change of height given standard deviation in age (thus a slope for height given the change in age), this is accompanied with a slope for each person. All persons are on average 149 cm, that is with a standard deviation of 1.4 On average one standard deviation of age increase leads to a person being 4.21 cm taller Notice very inefficient sampling in a_bar and b_bar Now we can plot the heights given the standardized x (demeaned and divided by one standard deviation). plot( height ~ age , type=&quot;n&quot; , data=d ) for ( i in 1:26 ) { h &lt;- d$height[ d$Subject==i ] a &lt;- d$age[ d$Subject==i ] lines( a , h , col=col.alpha(&quot;slateblue&quot;,0.5) , lwd=2 ) } Now what we can do instead is to predict the heights for the persons given their age. Thus we use the link function. Notice as we have a linear model we will have linear correlations, although we have several predictions, one for each person, thus we are able to distinguish between two persons and say given his current height, some may end up being 140 tall while other higher than 170 when growing one standard deviation in age. m14H4.1_link &lt;- link(m14H4.1) h_pred &lt;- apply(m14H4.1_link, 2, mean) # 2 for columns d &lt;- cbind(d, h_pred) #Add the predictions to the the dataframe plot( height ~ age , type=&quot;n&quot; , data=d ) for ( i in 1:26 ) { h &lt;- d$h_pred[ d$Subject==i ] a &lt;- d$age[ d$Subject==i ] lines( a , h , col=col.alpha(&quot;slateblue&quot;,0.5) , lwd=2 ) } Notice that there are varying intercepts and varying slopes that is visible in the way that some slopes are steeper than others and they all have different intercepts. "],["references.html", "References", " References McElreath, Richard. 2020. Statistical Rethinking. Chapman; Hall/CRC. https://doi.org/10.1201/9780429029608. "]]
