---
output: html_document
editor_options: 
  chunk_output_type: console
---

# Chapter 10 - Big Entropy and the Generalized Linear Model

## Maximum Entropy

It can be beneficiary to find a distribution that can as many of the possible outcomes as possible. We call such distributions maximum entropy, as this is the distribution that can be found the most ways.

Thus we want to pick the flattest distribution within the constraints that we are having. A constraint on the mean or the variance. Here are some examples of the outcome distribution (maximum entropy distribubtion) given the constraint:

![](images/paste-1215E581.png){width="231"}

That also implies, that the maximum entropy distribution is the most conservative.

**What is our goal?**

-   Connect linear model to outcome variable.

-   Our model is still geocentric

-   Strategy:

    1.  Pick an outcome distriubtion. Notice that you cannot look at the data before specifying the model. As we want to use our knowledge to build the model, hence we dont want to be perceived by the data. Always go for maximum entropy. The following are some good guidelines (in in the section \@ref(meet-the-family)):

        -   Distances and durations: exponential, gamma (survival or event history)

        -   Counts: Poisson, binomial, multinomial, geometric.

        -   Monsters: Ranks and ordered categories.

        -   Mixtures: Bbeta/binomial, gamma-Poisson, zero-inflated process, occupancy models.

    2.  Model its parameters using links to linear models

        -   In a linear model, we see that the outcome is the same scale as the input, e.g., predicting height will still be height.

        -   Although if you want to connect a linear model to a probability of success (p), then it is unitless. Thus you may have an input of a count, but an output as a probability.

        -   ***Notice that in this case we substitute \$\\mu\$ with a \$p\$. As the outcome of a model will not be on the same scale, we need a function for getting the probability. This is where the link function comes in play. Hence we will have \$f(p_i) = \\alpha + \\beta x_i\$.***

    3.  Compute the posterior

        -   With a GLM it is harder to search for the parameters, OLS can be used, but for some reason it is not optimal. We will just use MCMC where we also can use priors. (quap works sometimes but not always, hence we will just rely on MCMC.)

-   We can model multivariate relationships and non/linear responses

Notice that this is the building blocks of a multilevel model.

### Binomial

Here are some examples in a binomial scenario:

```{r 10.5}
# build list of the candidate distributions
p <- list()
p[[1]] <- c(1/4,1/4,1/4,1/4)
p[[2]] <- c(2/6,1/6,1/6,2/6)
p[[3]] <- c(1/6,2/6,2/6,1/6)
p[[4]] <- c(1/8,4/8,2/8,1/8)

par(mfrow = c(2,2))
for (i in 1:4) plot(p[[i]],type = 'b',ylim = c(0,1),pch = 20) %>% grid()
```

We see that we have four distributions, one with even outcomes possibilities, and three with different outcome possibilities.

We can now calculate the entropy of each distritution.

```{r 10.6}
sapply(p,function(p) -sum(p*log(p)))
```

```{r}
par(mfrow = c(1,1))
sapply(p,function(p) -sum(p*log(p))) %>% plot(type = 'l')
```

We see that the entropy is decreasing as the distribution gets less uniform

#### Example from the lecture

We see that the outcome scale will now impose interactions between the parameters no matter what you specify, as the outcome of a model is now condition. E.g.,

![](images/paste-F782CF95.png)

We see that the lizard can only die once and live once, the floor and ceiling effects will put the model output on a fixed scale, hence it does not matter how much we feed the lizard, if it is simply just too cold.

## Generalized linear models

When we apply the principle of maximum entropy, we see that we have dealt with unreal scenarios, while attempted to create a model, most outcomes.

This results in generalized linear model. this looks the following:

$$y_i ~ Binomial(n,p_i)$$

$$f(p_i) = \alpha + \beta(x_i-\bar{x})$$

hence we see that $\mu$ is exchanged with an $f$. This represents a **link function**.

### Meet the family

This section introduce some of the ddistributions from the exponential family, these are often widely used as they are all maximum entropy distributions given certain constraints.

We see that these are also often used in traditional statistics, although we often arrive at these in different ways.

![](images/paste-4C691A71.png)

Explanation:

-   We see that the exponential distribution is the core of the family.

-   Binomial distribution: is when we count events underlying the exp distribution. e.g., coin flips. We need n trials and p probability of a given event. Hence the expected outcome of a sequence of trials is then $n * p$.

-   Poisson: it is basically the binomial where the probability of a given event is very low.

-   gamma: eg., lets say that you have a binomial event, that a washing machine is breaking. The probability of the machine breaking is in general low. But if we sum the probability of breaking over time, we will see that we get a gamma distribution. Hence the distribution is in this case reflecting the waiting time. If the mean is large, then it is a gaussian distribution.

-   Gaussian: the gamma but with a large mean.

conclusion: we see that max entropy leads to a distribution that will explain the problem at hand given the constraints, although this family of distributions are related, which is the key takeaway.

### Linking linear models to distributions

First, lets clarify the purpose of the link function. It is effectively to map a linear function onto a non linear space of a given parameter ($\theta$).

In general, we are having two link functions:

1.  logit link (log-odds). It looks the following:

    $$
    y_i \sim Binomial(n,p_i) \\
    logit(p_i) = \alpha + \beta x_i
    $$

    We see that the value is now linear to the logit (log-odds) of p. Hence we see that we take the probability p and map it onto the log-odds scale. Hence the model may output the log-odds, but the logit function, can map the log-odds onto the probability scale.

    One can then take the inverse-link of the function, that is called the logistic.

    Summary:

    -   log-odds of the log of the odds.

    -   We can get back to probabilities, by using the inverse link on the log-odds.

2.  log link.

Log-odds scale vs. the probability scale:

![](images/paste-F1A0C887.png){width="319"}

We see that log-odds = 0, is 50% probability, while log-odds = 3 is 95% probability. Notice it is symmetric, log-odds of -3 is 5% of the time. **This is important for defining priors.**

The log-odds scale goes from negative to positive infinity.

**The logit link:**

We see that the logit link is able to transform the log-odds of the model into probability, meaning we go from a linear model to map this on a y range between 0 and 1, this is typically also called a logistic function. It can look as the following:

![](images/paste-30A9A55F.png)

> what are the odds of an event? It is merely the probability of an event happening divided by the probability of the event not happening. Thus this can be written with:
>
> $$
> log\frac{p_i}{1-p_i} = \alpha + \beta x_i
> $$
>
> , where \$p_i\$ = $p_i=\frac{exp(\alpha + \beta x_i)}{1 + exp(\alpha + \beta x_i)}$, this is also called the *logistic* or *the inverse-logit*.

**The log link:**

Basically this is a link function that is applied whenever an exponential relationship is being inferred / predicted. It takes the following shape:

![](images/paste-1D1AE7DE.png)

------------------------------------------------------------------------

Keep in mind that the link functions are assumptions, if they do not work well, then try other methods.

## Maximum entropy priors

This is explained in the earlier sections of the chapter, the following image summarize it.

![](images/paste-1215E581.png){width="231"}

It is basically about selecting a prior that captures the distribution that can be constructed the most possible ways given the constraints.

# Chapter 11 - God Spiked the Integers

The following sections makes examples using a binomial outcome and exponential outcome (poisson). Also the last section is about a multinomial problem, where there is more than two outcomes.

In general it is very good practice not to create proportions before running the model, as the counts indicate the magnitude of the model, while the proportions will not reflect such.

## Binomial Regression

Binomial regression is when you have an outcome variable with two outcomes, e.g., yes/no, reject/accept etc. We see that it takes the following formula:

$$
y \sim Binomial(n,p)
$$

Where y is a count. P = a probability of a certain trial success and n is the number of trials.

#### Logistic regression: Prosocial chimpanzees:

In the following an example with chimpanzees is presented. In real life they had a table with a chimpanzee in one end, and then some food on a table. This food could be delivered in each ends of the table, depending on what level a chimpanzee pulls. The trick is then for every second chimpanzee there will be another chimpanzee in the other end of the table. Hence they want to investigate if a chimpanzee is more likely to get food for both ends of the table, if there is another chimpanzee present.

Prosocial = is about caring, hence if there is a partnering chimp. A proscial option is when there is food on the other side of the table, that can be send to the other chimpanzee. (opposite of prosocail = asocial)

Notice that one should account for the handedness of a chimpanzee (left or right), as that may be a backdoor if we were to draw the DAG.

![](images/paste-F90E83ED.png)

We will create a model with a binomial distribution explaining the outcome variable (if one or the other lever is pulled).

Then we will have a logit function (logistic) explaining the probably outcome (likelihood) and we will include two parameters, an intercept ($\alpha$) and a coefficient ($\beta$). There will be one ($\alpha$) for each unique combination of outcomes.

*Notice that we do not use dummy variables, but instead an index to model the interaction. For explanation of this, it is referred to CH5.*

```{r}
#Loading data 11.1
library(rethinking)
data(chimpanzees)
d <- chimpanzees

#Create condition, treatment 1, 2, 3 and 4. see p. 326
d$treatment <- 1 + d$prosoc_left + 2*d$condition

xtabs( ~ treatment + prosoc_left + condition , d ) #if condition = 1, then partner present
```

Now we see the different outcomes given the condition (if there is a partner present).

*Notice that the* $\alpha$ prior is based on log-odds. We see that the log odds larger or smaller than 4 and -4 means great certainty on 0 or 1, see the picture of this in a earlier section. Hence in the following example, we see that the log-odds spans all the way up to \>15, hence it is simply just var too extreme, hence it says that pulling the left lever is either always or never happens. Hence when we transform to the probability scale, it will be either 0 or 1 (more or less). This is a horrible priors.

The following is just an example prior, which is almost flat, although the outcome leads to very non flat results, as we break the usual scale of the log-odds.

```{r}
m11.1 <- quap(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a ,
    a ~ dnorm( 0 , 10 ) #Mean = 0, as log-odds is centered at 0. sd of 10 is very wide.
  ) , data=d )

#Sampling from the prior
set.seed(1999)
prior <- extract.prior(m11.1,n = 1e4)

#Using inverse-link function to convert the parameter to the outcome scale
p <- inv_logit(prior$a)
dens(p,adj = 0.1)
```

Se see that this prior will lead to great certainty in pulling the left or the right lever.

```{r}
#Prior plot, plotting prior
x <- seq(-5, 5, length=100)
hx <- dnorm(x,mean = 0,sd = 10)
plot(x, hx, type="l", lty=2, xlab="x value",
  ylab="Density", main="Comparison of prior Distributions")
```

We see that the prior is very wide and despite a bad prior, we get very certain outcomes of 0 or 1.

We will go for an sd of 1.5. We will also set the beta prior for dnorm of mean of 0 and standard deviation of 0.5. In the book they also examplifies with beta = dnorm(0,10), this is equally bad, hence it is regularized.

```{r}
m11.3 <- quap( 
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a + b[treatment] ,
    a ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 ) #the treatment paramtere
  )
  ,data=d )

set.seed(1999)
prior <- extract.prior( m11.3 , n=1e4 )

#Get 4 vectors of samples based on the priors
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )

#Compute the difference between two vectors
mean( abs( p[,1] - p[,2] ) )
```

Now we see that on average, there is 10% difference.

> From this we can tell that the samples deviate, but not very much. We want to limit the model from modeling unnatural outcomes, while actually being able to adapt. Hence we want the model to be skeptical of large differences.

Now we can estimate the mode using **Hamiltonian Monte Carlo**

```{r 11.10}
#Estimating the posterior distribution using Hamiltonian Monte Carlo

# prior trimmed data list
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment) )

# Constructing the model 11.11
m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + b[treatment] , #actor = chimp, treatment = the experiment setup
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
  ) 
  ,data=dat_list , chains=4 , log_lik=TRUE )

# OUTPUT = mean = log-odds of outcomes for each parameter.  

precis( m11.4 , depth=2 )
```

Thus, we see a bunch of different alpha values and the four different beta values (one for each treatment). Interpretation:

-   Alpha = for each chimp.

-   Alpha mean = the log-odds of pulling the left lever (success = left lever pull)

    -   When alpha \> 0 we know that the probability is \>50%, we have three chimps with positive numbers. where it is already clear that chimp 2 almost always pulled the left lever (i think it was a lefty chimp).

    -   In general we see that there is a tendency for right handedness, as most chimps have a preference of the right lever (alpha mean (log odds) less than 50%)

-   beta = for each type of treatment.

-   beta mean = the log-odds of pulling the left lever given the treatment. We see that it typically spans in the negative and positive region, hence an indication of the treatment not having a great effect. more about this in the following.

Let us take a look of the tendency of pulling the LEFT lever. In general what we see is the preference for left and right handedness for each chimp. This is the reason for controlling for each actor (chimp)

```{r 11.12}
post <- extract.samples(m11.4)
p_left <- inv_logit( post$a )
plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )
```

We see that that chimpanzee 1, 3, 4, 5 have a preference to pull the right lever, that is because the probability of pulling the left lever is below 50%. Chimpanzee no. 6 is a bit on both sides, while chimpanzee 2 and 7 tends to pull the left lever.

Let us inspect the actual cases for chimpanzee no. 2.

```{r}
d[d$actor == 2,"pulled_left"] %>% table()
```

We see that this guy in fact only pulled the left lever, hence the great certainty.

Now we want to measure the treatment effect. We can derive the effect from the $\beta$ coefficient.

```{r 11.13}
labs <- c("R/N","L/N","R/P","L/P") #R = right, L = Left, P = Partner, N = No partner
{plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs)
mtext("R = Right, L = Left, N = no partner, P = Partner")}
```

It already seems as if they are overlapping. But what is really interesting is the difference between the scenarios. This we inspect in the following:

```{r 11.14}
diffs <- list(
  db13 = post$b[,1] - post$b[,3], #No partner / partner treatment
  db24 = post$b[,2] - post$b[,4] )
plot( precis(diffs) )
```

We see some tendencies, although as both ranges on both sides of 0, there is no strong evidance of a pattern.

In the book they also compare the posterior predictions compared with the osberved events. This will not be shown.

Ultimately it also compares the model with a non interaction model (index for oth chimpanzee and treatment). So a modell looks the following:

```{r}
#Creating data for condition and pulled lever
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

#Specify model
dat_list2 <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  side = d$side,
  cond = d$cond
  )

m11.5 <- ulam(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + bs[side] + bc[cond] ,
    a[actor] ~ dnorm( 0 , 1.5 ),
    bs[side] ~ dnorm( 0 , 0.5 ),
    bc[cond] ~ dnorm( 0 , 0.5 )
  ) , data=dat_list2 , chains=4
  , log_lik=TRUE #Keep log_lik for comparabillity
  )

#compare the model with the previous model:
compare( m11.5 , m11.4 , func=PSIS )
```

We see that the simpler model is working just as fine. That was expected as we did not discover any effects in the original model, hence there should not be one either in the simpler model.

#### Relative shark and absolute deer

Notice that logistic regression should be seen as measuring *relative effects*, this is the relative change in odds of an outcome.

We can calculate the *proportional odds* by saying: case we switch from a scenario with no partner (2) and to a scenario where we have a partner (4).

```{r 11.23}
post <- extract.samples(m11.4)
mean(exp(post$b[,4] - post$b[,2]))
  # 2 = two food items and no partner
  # 4 = two food items and a partner
```

0.93 = 7% reduction of odds of the monkey pulling the left lever (0.93 % relative chance of success (pulling left lever)).

Thus notice that the realtive scale is perceptive, as change from one category to another, e.g., from the chimpanzee example from one treatment to another, may seem large on a relative scale, although in absolute number, you may be working on a very small magnitude.

Example: lung cancer is rare and we see that smoking increases your odds of getting lung cancer by three. lung cancer is still rare, although a third of lung cancer patients just smoked.

Another example from the lecture:

![](images/paste-68634DED.png){width="222"}

#### Aggregated binomial: Chimpanzees again, condensed.

This is the same example just as the model above, here the data is basically just aggregated, so we have one row for each chimpanzee and not one for each experiment.

Now the model is just specified with:

$$
leftpuls \sim Binomial(18,p)
$$

As $Binomial(N,p)$ and N = number of osbervations, as we have 18 experiments per chimpanzee, we will use N = 18. Notice that N can only be fixed, if there is an equal number of observations pr. row in the underlying data for the aggregated data (the following section deals with this).

```{r,eval=FALSE}
m11.6 <- ulam(
  alist(
    left_pulls ~ dbinom( 18 , p ) , #18 instead of 1
    logit(p) <- a[actor] + b[treatment] ,
    a[actor] ~ dnorm( 0 , 1.5 ) ,
    b[treatment] ~ dnorm( 0 , 0.5 )
  ) , data=dat , chains=4 , log_lik=TRUE )
```

For the rest of the example, I refer to the book. the only difference we see, is that the PSIS is a bit different (due to a bit different parametization), although the model performs equally well.

#### Aggregated binomial: Graduate school admissions

In this example we see that there is not an equal number of observations per aggregation category. The following example show two different models:

1.  One without index variable for department
2.  One with index variable for department, to adjust for the mediating effect the department has on the DAG

![G = Gender, D = Department, A = Acceptance](images/paste-85D16C60.png)

```{r 11.28}
#Loading the data
library(rethinking)
data(UCBadmit)
d <- UCBadmit
d
```

We see that there is different number of applications for each department for both males and females. We deal with this, merely by inserting a variable in N's place in the equation.

```{r 11.29}
dat_list <- list(
  admit = d$admit,
  applications = d$applications,
  gid = ifelse( d$applicant.gender=="male" , 1 , 2 )
  )

m11.7 <- ulam(
  alist(
    admit ~ dbinom( applications , p ) ,
    logit(p) <- a[gid] , #index for gender
    a[gid] ~ dnorm( 0 , 1.5 )
    ) 
  ,data=dat_list , chains=4 )

precis( m11.7 , depth=2 ) #Output mean ) log odds
```

We see the log odds, recall that 0 = \<50% probability, females look to be more certainly not being admitted.

We see from the first example, that females (2) appear to be accepted less than males. Although how much higher? We can show the contrast on the aboslute and relative scale.

```{r 11.30}
#Compute the difference in probability measures.
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]

#Going from log odds to probability
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

On a probability scale there is somewhere between 0.12 and 0.16 probability larger difference for males. Hence either there is huge discrimination going on or there is something lurking in the data.

We see that based on the difference of a, we see that there is certainly a somewhat difference. This we can also interpret visually.

```{r 11.31,fig.cap="Blue = obbserved proportions in raw data, and the black points are the expected difference. We are very much off here."}
postcheck( m11.7 )
# draw lines connecting points from same dept
for ( i in 1:6 ) {
x <- 1 + 2*(i-1)
y1 <- d$admit[x]/d$applications[x]
y2 <- d$admit[x+1]/d$applications[x+1]
lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}
```

Notice that the first dot on the line is male, and the second dot is female. We see in the first case, there is a larger posterior prediction for males compared to females, although it is observed that in real life, the actual prob of admission is greater for females. We do in fact see that females are admitted more in all but 2 departments. Thus there is something totally off here. **So what is happening? Lets look at the DAG.**

![On the right we have the suggested adjusted model, to include the departments, hence the effect within each department instead of across all departments.](images/paste-10080A62.png)

We see that there is a backdoor to admission, hence we must include department in the model.

> But did the model do anything wrong? no, we just told it that it can calculate log-odds across departments, but now it is clear that we must direct the model elsewise.

Now we see that we are very much off. Hence we can try to include the department into the model.

**Lets add an index variable for department**

```{r 11.32}
dat_list$dept_id <- rep(1:6,each=2)
m11.8 <- ulam(
  alist(
    admit ~ dbinom( applications , p ) ,
    logit(p) <- a[gid] + delta[dept_id] , #Added index for department
    a[gid] ~ dnorm( 0 , 1.5 ) ,
    delta[dept_id] ~ dnorm( 0 , 1.5 )
  ) , data=dat_list , chains=4 , iter=4000 )

# Precis where we include department
precis( m11.8 , depth=2 )
```

First of all we see that the means are very similar now (to each other, male is lower), where the model also accounts for each department. And we also see that each department is very different in the effect that it has on acceptance rate.

We also see that some department (most) tend to admit more females than males, by looking within each department.

Now we can estimate the differences as before:

```{r 11.33}
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2] #on the relative scale
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2]) #on the absolute scale (probability scale)
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

We see that on average the difference is -0.1 and the intervals are mostly negative. On the probability scale, we see that there is a negative difference, hence a small disadvantage for males. Although we see that the probability difference is very close to zero, it is likely that the difference between males and females is very small.

We see that the difference is much smaller now where we account for the department which is a mediator effect that we see in the DAG in the beginning for the section.

## Poisson Regression

Sometimes there will be an exponential effect in the data. This we will not be able to model using a binomial approach, hence we need the poisson distribution. It looks the following:

$$
y_i \sim Poisson(\lambda)
$$

Where lambda is the expected value of outomce y, i.e., expected variance of the counts y.

When going for poisson regression one will typically also go for the **log link** function, which is a new link function, it looks the following

$$
y_i \sim Poisson(\lambda_i)
$$

$$
log(\lambda_i)= \alpha + \beta(x_i-\bar{x})
$$

The log link ensures that $\lambda_i$ is always positive.

#### Negative binomial (gamma-poisson) models.

It is common practice for some reason to swap the Poisson distribution with a negative binomial distribution, also sometimes called a gamma-poisson distribution. It is basically a mixture of different poisson distributions.

## Multinomial Regression

In a binomial setting you will have two different outcomes, although that is not always the case, hence you must apply models that can take on multiple categories.

Such models are called multinomial regression. The output is a multinomial logit, i.e., the softmax.

They present some different approaches also in stan. I will not replicate the stan code.

The example is with admission rates for UC Berkely.

```{r}
# binomial model of overall admission probability
m_binom <- quap(
  alist(
    admit ~ dbinom(applications,p),
    logit(p) <- a,
    a ~ dnorm( 0 , 1.5 )
  )
  ,data=d )

# Poisson model of overall admission rate and rejection rate
# 'reject' is a reserved word in Stan, cannot use as variable name
dat <- list( admit=d$admit , rej=d$reject )
m_pois <- ulam(
alist(
    admit ~ dpois(lambda1),
    rej ~ dpois(lambda2),
    log(lambda1) <- a1,
    log(lambda2) <- a2,
    c(a1,a2) ~ dnorm(0,1.5)
  ), data=dat , chains=3 , cores=3 )
```

binomial probability of admission across the entire data set.

```{r 11.62}
#Binomial model inference
inv_logit(coef(m_binom))
```

We see the probability of a female being accepted is 38%.

```{r}
#log odds for binomial model
precis(m_binom)
```

We can see the same for the poisson model:

```{r 11.63}
k <- coef(m_pois)
a1 <- k['a1']
a2 <- k['a2']
exp(a1)/(exp(a1)+exp(a2))
```

We see that the result is very similar.

```{r}
#log-rate for the poisson
precis(m_pois)
```

### Example from the lecture

Relationship between tools, population and contact with the world.

```{r 11.36}
library(rethinking)
data(Kline)
d <- Kline
d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )
```


```{r}
dat <- list(
  T1 = d$total_tools , #I named it T1, as T is reserved for TRUE
  P = d$P ,
  cid = d$contact_id 
  )

# intercept only
m11.9 <- ulam(
  alist(
    T1 ~ dpois( lambda ),
    log(lambda) <- a,
    a ~ dnorm(3,0.5)
  )
  , data=dat , chains=4 , log_lik=TRUE ) #Log like to be able to compare

# interaction model
m11.10 <- ulam(
  alist(
    T1 ~ dpois( lambda ),
    log(lambda) <- a[cid] + b[cid]*P,
    a[cid] ~ dnorm( 3 , 0.5 ),
    b[cid] ~ dnorm( 0 , 0.2 )
  ), data=dat , chains=4 , log_lik=TRUE ) #Log like to be able to compare

#Lets compare these, pLOO = effective number of parameters
#compare( m11.9 , m11.10 , func=PSIS )
compare( m11.9 , m11.10 , func=LOO ) #Should return the pLOO
```

We see from this model, that the effective number of parameters (pLOO), hence the intuitively more complicated model is less prone to overfitting, due to the lower effective number of parameters.

The following plots represent the posterior predictions

```{r 11.47_48,results='hold',fig.cap="Dashed = low contact and solid line = high contact"}
par(mfrow = c(1,2))
k <- PSIS( m11.10 , pointwise=TRUE )$k

plot( dat$P , dat$T , xlab="log population (std)" , ylab="total tools" ,
      col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
      ylim=c(0,75) , cex=1+normalize(k)
      )

# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-1.4 , to=3 , length.out=ns )

# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )


# code chunk 11.48
plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
      col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
      ylim=c(0,75) , cex=1+normalize(k))

ns <- 100

P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)

# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )
```

## Exercies

### E1

If an event has probability of 0.35, what are the log-odds of this event?

```{r}
#find log odds (logit) based on the probability
logit(x = 0.35) # - 0.61
```

We know that when odds = 0, then we have 50/50 probability, and odds higher / lower than 3/-3 equals very certain outcomes (5% / 95%).

This can also be calculated with:

```{r}
p = 0.35 #recal that p = exp(alpha + beta * xi)/(exp(alpha + beta * xi)), where xi is one observation
log(p/(1-p))
```


### E2

If an event has log-odds 3.2, what is the probability of this event?

```{r}
#transform log-odds to a probability
logistic(x = 3.2)
```

log-odds = logits

we can find the odds, by taking the exponential of the log-odds.

```{r}
exp(3.2) # = 23.5
```


### E3

Suppose that a coefficient in a logistic regression has value 1.7. What does that imply about the proportional change in odds of the outcome?

That means that by one unit change in x, the log-odds will increase by 1.7, thus if we are working with standardized x values, then one standard deviation of change in x, leads to 1.7 higher log-odds,

```{r}
logistic(1.7)
```

We see that 1.7 log odds is corresponding with:

```{r}
exp(1.7) #5.5
```

That implies that there is a 5.5 greater chance of something happening, given a one unit change in x.


### M7

Use quap to construct a quadratic approximate posterior distribution for the chimpanzee model that includes a unique intercept for each actor, m11.4 (p. 330). Compare the quadratic approximation to the posterior distribution produced instead from MCMC. Can you explain both the differences and the similarities between the quadratic approximate and the MCMC distributions?

Relax the prior on the actor intercepts to normal(0,10). Re-estimate the posterior using both ulam and quap. Do the differences increase or decrease?

```{r,message=FALSE}
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

# prior trimmed data list
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment) )

# Constructing the model
model <- alist(
    pulled_left ~ dbinom( 1 , p ) , # = Logistic regression
    logit(p) <- a[actor] + b[treatment] , #actor = chimp, treatment = the experiment setup
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
  )

m11.4 <- ulam( #Estimating the posterior distribution using Hamiltonian Monte Carlo
  flist = model
  ,data=dat_list , chains=4 , log_lik=TRUE)

m11.4_quap <- quap(
  flist = model
  ,data = dat_list
)
```

Now we can compare the outputs of these models, both in a table, but also the in a scatterplot.

```{r}
prec_ulam <- precis(m11.4,depth = 2)[,1:4] #Remove n_eff and rhat
prec_quap <- precis(m11.4_quap,depth = 2)
round(cbind(prec_ulam,prec_quap),digits = 2)
```

```{r}
colors <- c("#7fc97f","#fdc086")
plot(prec_ulam$mean,pch = 20,col = colors[1],xlab = "Parameters",ylab = "Parameter Coefficient"
     ,main = "Mean comparisons",sub = "Comparison of mean values for the model parameters"
     ,ylim = c(min(prec_quap$`5.5%`),max(prec_quap$`94.5%`))
     ,panel.first = list(grid()))
# lines(x = c(prec_ulam$`5.5%`[1],prec_ulam$`94.5%`[1]))
for (i in 1:length(prec_ulam$mean)) lines(x = c(i,i),y = c(prec_ulam$`5.5%`[i],prec_ulam$`94.5%`[i])
                                          ,col = alpha(colors[1],alpha = 0.7),lwd = 1.2)
for (i in 1:length(prec_quap$mean)) lines(x = c(i,i),y = c(prec_quap$`5.5%`[i],prec_quap$`94.5%`[i])
                                          ,col = alpha(colors[2],alpha = 0.7),lwd = 1.2)
points(prec_quap$mean,pch = 20,col = colors[2])
```



Notice that we apply the inverse-link (inv_logit) to convert the parameter to the outcome scale, thus we will see the x-axis going from 0 to 1.

```{r,results='hold'}
#Extract samples for posterior distribution inference
post_ulam <- extract.samples(m11.4)
post_quap <- extract.samples(m11.4_quap)
head(post_ulam$a)
```

Notice that we get results for each chimp. We want to just compare one against another chimp. We select chimp no. 2.

```{r,results='hold'}
chimpNo <- 2

par(mfrow = c(2,1))
dens(inv_logit(post_ulam$a[,chimpNo]))
dens(inv_logit(post_quap$a[,chimpNo]),add = T,col = "darkred")
mtext("With inv_logit to transform x to the outcome scale (probability / logistic)")
legend("topleft",legend = c("ulam","quap"),lty = 1,col = c("black","darkred"))

dens(post_ulam$a[,chimpNo])
dens(post_quap$a[,chimpNo],add = T,col = "darkred")
mtext("Raw alpha outputs = log-odds scale")
legend("topright",legend = c("ulam","quap"),lty = 1,col = c("black","darkred"))
```

We see that on an overall level, the distributions appear to be similar. And naturally they both confirm, that for chimp 2 (was that the leftie?) we see a very high expected probability or odds for selecting the left lever.

We can convert the log-odds to odds:

```{r}
exp(mean(post_ulam$a[,chimpNo])) #= 48.87329
```

We see that on average, we expect to see that chimp selecting the left lever more than 48 times compared to the right lever. Thus if the chimp are to pull the right lever twice (two times), we expect him to also have pulled the left lever around 97 times (48.87329 * 2).

Thus the conclusion is that it almost always pulls the left lever.

Notice that on the log-odds scale we also see VERY high values, that is as the practical difference between log-odds of 5, 6 and 7 is practically the same, they all mean always (or almost always).

```{r}
mean(inv_logit(post_ulam$a[,chimpNo])) #= 0.9742542
```


```{r}
dev.off()
rm(list = ls())
```


### H1

Use WAIC or PSIS to compare the chimoanzee model that includes a unqie intercept for each actor, m11.4 (p. 330), to the simpler model fit in the same section. Interpret the results.

I choose to compare all four models:

1. Merely an intercept and a wide alpha prior
2. Adding a slope for treatment group, with a vaguely informative prior.
3. Regularizing the alpha prior, otherwise the same as model 2.
4. MCMC fitted model.

```{r message=FALSE}
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

# prior trimmed data list
dat_list <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  treatment = as.integer(d$treatment) )

## Fitting different models with different levels of complexity

m11.1 <- quap(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a ,
    a ~ dnorm( 0 , 10 ) #Mean = 0, as log-odds is centered at 0. sd of 10 is very wide.
  ) , data=d,)

m11.2 <- quap(
    alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a + b[treatment],
    a ~ dnorm( 0 , 10 ), #Mean = 0, as log-odds is centered at 0. sd of 10 is very wide.
    b[treatment] ~ dnorm(0,0.5)
  ) , data=d)

m11.3 <- quap( 
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a + b[treatment] ,
    a ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 ) #the treatment paramtere
  )
  ,data=d )

m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + b[treatment] , #actor = chimp, treatment = the experiment setup
    a[actor] ~ dnorm( 0 , 1.5 ),
    b[treatment] ~ dnorm( 0 , 0.5 )
  ) 
  ,data=dat_list,chains=4,log_lik=TRUE,messages = F)
```

```{r}
compare(m11.1,m11.2,m11.3,m11.4)
```

Notice that we get an warning, as different fitting techniques are used, that being quap and ulam.

We see that the multiple slopes and multiple intercepts model is the better model, at least on a WAIC level. Notice that we have more than twice as many effective parameters as in the simpler models.

When we look at the simpler models, it does appear as if the only intercept model (the average) is suffice.

```{r}
plot(compare(m11.1,m11.2,m11.3,m11.4))
```

lets just take a quick look at the precis tables for the most complex and simple models.

```{r}
precis(m11.4,depth = 2)
precis(m11.1)
```

Notice that the multiple intercepts model is able to capture the left handed chimp, that the single intercept model is simply not able to do.

```{r,include = FALSE,eval=TRUE}
rm(list = ls())
```


### H2

The data contained in library(mass);data(eagles) are records of salmon pirating attempts by Bald Eagles in Washing State. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the victim and the thief the "pirate". Use the available data to build a binomial GLM of successful pirating attempts.

a. Consider the following model

$$
y_i \sim Binomial(n_i,p_i) \\
logit(p_i) = \alpha + \beta_PP_i + \beta_VV_i + \beta_AA_i \\
\alpha \sim Normal(0,1.5) \\
\beta_P,\beta_V,\beta_A \sim Normal(0,0.5)
$$

Where:

+ y is the number of successful attempts, 
+ n is the total number of attempts. 
+ P is a dummy variable indicating whether or not the pirate had large body size, 
+ V is a dummy variable indicating whether or not the victim had large body size, and finally 
+ A is a dummy variable indicating whether or not the pirate was an adult. 

Fit the model above to the eagels data, using both quap and ulam. Is the quadratic approximation okay?

b. Now interpret the estimates. If the quadratic approximation turned out okay, then its okay to use the quap estimates. Otherwise stick to ulam estimates. Then plot the posterior predictions.

Compute and display both:

1. The predicted probability of success and its 89% interval for each row (i) on the data, as well as,

2. The predicted success count and its 89% interval. What different information does each type fo posterior prediction provide?

c. Now try to improve the model. Consider an interaction between the pirates size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret.

#### A:

```{r}
library(MASS)
data(eagles)
d <- eagles
d$P <- ifelse( d$P=="L" , 1 , 0 ) #P = Pirate, L = large, S = Small
d$V <- ifelse( d$V=="L" , 1 , 0 ) #V = Victim, L = large, S = Small
d$A <- ifelse( d$A=="A" , 1 , 0 ) #A = Adult, 1 = yes, 0 = no.
head(d)
```

The following error is probably due to some missspecified names, hence some of the parameters cannot be mapped to data.

_Error in quap(alist(y ~ dbinom(n, p), logit(p) <- a + bP * P + bV * V +  :_
  *initial value in 'vmmin' is not finite*
*The start values for the parameters were invalid. This could be caused by missing values (NA) in the data or by start values outside the parameter constraints. If there are no NA values in the data, try using explicit start values.*

```{r message=FALSE}
#The model
model <- alist(
    y ~ dbinom(n,p),
    logit(p) <- a + bP*P + bV*V + bA*A, #returns log odds
    a ~ dnorm(0,1.5),
    c(bP,bV,bA) ~ dnorm(0,0.5)
  )

m_q <- quap(flist = model,data = d) #The quadratic approximation
m_u <- ulam(flist = model,data = d,chains = 4,cores = 4,log_lik = T,messages = F)
```

Now we can call `precis` to inspect how similar the estimates are.

```{r,results='hold'}
precis(m_q)
precis(m_u)
```

We see that most estimates are similar although the coefficient for adult is very different. We know that quap is not always an effective fitting technique. We see that the effective sample looks good in the MCMC model, and we also see that trank plot shows healthy chains. Thus we go forward with the MCMC model.

```{r}
trankplot(m_u)
```

#### B:

```{r}
#lets extract the coefficients first:
prec <- precis(m_u)
a <- prec["a","mean"]
bA <- prec["bA","mean"]
bV <- prec["bV","mean"]
bP <- prec["bP","mean"]

pe <- c() #for point estimate
for (i in 1:nrow(d)){ 
  pe[i] <- a + bP*d$P[i] + bV*d$V[i] + bA*d$A[i]
}
d$PE_logodds <- pe
d$PE_p <- logistic(x = pe) #transform from log odds to logistic
```

Already from the point estimates we are able to see that the greatest probability of success is when the pirate is large and an adult and while the victim is not large.

On the other hand we see that the smallest chance of success is when we have a small and non adult pirate and while victim is an adult.

To do some inference on the model and the compatibility intervals, lets extract some samples.

```{r}
post <- extract.samples(m_u)
quantile(logistic(post$a),probs = c(0.055,0.5,0.945)) #This is for the null model, where all variables = 0
```

Then one could do the same for each combination

```{r,results='hold'}
quantile(logistic(post$a + post$bA),probs = c(0.055,0.5,0.945)) #a small adult pirate and small victim
quantile(logistic(post$a + post$bV),probs = c(0.055,0.5,0.945)) #a small non adult pirate and large victim
quantile(logistic(post$a + post$bP),probs = c(0.055,0.5,0.945)) #a large non adult pirate and small victim
quantile(logistic(post$a + post$bA + post$bV),probs = c(0.055,0.5,0.945)) #an adult small pirate and large victim
quantile(logistic(post$a + post$bA + post$bV + post$bP),probs = c(0.055,0.5,0.945))
#This could be done for combinations
```


```{r,results='hold'}
d$psuccess <- d$y / d$n

p <- link(fit = m_u) #simulate posterior probabilities (for the probability model)
y <- sim(fit = m_u) #simulate posterior observations (for the absolute counts model)

p.mean <- apply( p , 2 , mean )
p.PI <- apply( p , 2 , PI )
y.mean <- apply( y , 2 , mean )
y.PI <- apply( y , 2 , PI )

par(mfrow = c(2,1),oma = c(0,0,0,0),mar = c(4,4,2,0.2))

## Predicted relative successes

# plot raw proportions success for each case
plot( d$psuccess , col=rangi2 ,
    ylab="successful proportion" , xlab="case" , xaxt="n" ,
    xlim=c(0.75,8.25) , pch=16,main = "Predicted relative successes"
    ,panel.first = list(grid()))

# label cases on horizontal axis
axis( 1 , at=1:8 ,
    labels=c( "LAL","LAS","LIL","LIS","SAL","SAS","SIL","SIS" ) )

# display posterior predicted proportions successful
points( 1:8 , p.mean )
for ( i in 1:8 ) lines( c(i,i) , p.PI[,i] )

## Predicted absolute successes

# plot raw counts success for each case
plot( d$y , col=rangi2 ,
    ylab="successful attempts" , xlab="case" , xaxt="n" ,
    xlim=c(0.75,8.25) , pch=16,main = "Predicted absolute successes"
    ,panel.first = list(grid()))

# label cases on horizontal axis
axis( 1 , at=1:8 ,
    labels=c( "LAL","LAS","LIL","LIS","SAL","SAS","SIL","SIS" ) )

# display posterior predicted successes
points( 1:8 , y.mean )
for ( i in 1:8 ) lines( c(i,i) , y.PI[,i] )
```


#### C:

Now try to improve the model. Consider an interaction between the pirates size and age (immature or adult). Compare this model to the previous one, using WAIC. Interpret.

```{r}
#The model
model_int <- alist(
    y ~ dbinom(n,p),
    logit(p) <- a + bP*P + bV*V + bA*A + bPA*P*A, #returns log odds
    a ~ dnorm(0,1.5),
    c(bP,bV,bA,bPA) ~ dnorm(0,0.5)
  )

m2_u <- ulam(flist = model_int,data = d,chains = 4,cores = 4,log_lik = T,messages = F)
precis(m2_u)
```

Now we see that we have a parameter for the interaction, although we should notice that the 89% CI goes on both sides of 0. Hence indicating that it does not add a lot of information to the model. Lets compare the two

```{r}
compare(m_u,m2_u)
```

Now we see that the more complex model has less predictive power than the interaction model.

### H3

The data contained in data(salamanders) are counts of salamanders from 47 different 49-m2 plots in northern california. THe column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. You will model SALAMAN as a Poisson variable.

a. Model the relationship between density and percent over, using a log-link (same as the examepl in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing quap to ulam. Then plot the expected counts and their 89% interval against percent cover. In which ways dows the model do a good hjob? bad job?

b. Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction?

The answer follows the guiding solutions:

```{r}
data(salamanders)
d <- salamanders
d$C <- standardize(d$PCTCOVER)
d$A <- standardize(d$FORESTAGE)
```

#### Prior predictive simulation

```{r}
dev.off()
N <- 50 # 50 samples from prior
a <- rnorm( N , 0 , 1 )
bC <- rnorm( N , 0 , 1 )
C_seq <- seq( from=-2 , to=2 , length.out=30 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,20) ,
    xlab="cover (standardized)" , ylab="salamanders" )
for ( i in 1:N )
    lines( C_seq , exp( a[i] + bC[i]*C_seq ) , col=grau() , lwd=1.5 )
```

Now lets try a more regularized prior:

```{r}
bC <- rnorm( N , 0 , 0.5 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,20) ,
    xlab="cover (standardized)" , ylab="salamanders" )
for ( i in 1:N )
    lines( C_seq , exp( a[i] + bC[i]*C_seq ) , col=grau() , lwd=1.5 )
```

we see that this can still model extreme examples, but we assume less salamanders in general per plot.

#### Fitting the model

##### Using poisson

```{r}
f <- alist(
    SALAMAN ~ dpois( lambda ), #Notice that we use poisson
    log(lambda) <- a + bC*C,
    a ~ dnorm(0,1),
    bC ~ dnorm(0,0.5) )
m1 <- ulam( f , data=d , chains=4 )
```

```{r}
precis(m1)
```

We see some correlation between the count of salamanders and the percent of ground cover. Lets plot this relationship

```{r,results='hold'}
plot( d$C , d$SALAMAN , col=rangi2 , lwd=2 ,
    xlab="cover (standardized)" , ylab="salamanders observed" )
C_seq <- seq( from=-2 , to=2 , length.out=30 )
l <- link( m1 , data=list(C=C_seq) ) #Make predictions
lines( C_seq , colMeans( l ) )
shade( apply( l , 2 , PI ) , C_seq ) #add the CI
```

We see that the higher the ground cover is, the larger is the predicted number of salamanders.

##### Using gamma poisson

 For comparison, we can try out a gamma-Poisson (aka negative binomial) model. You can read about the model in chapter 12.1.2 in the book. The gamma-Poisson introduces a new parameter, $\phi$, which controls the dispersion (scale) of the rates across cases. $\phi$ must be positive and controls the variance. That variance of the gamma-Poisson is $\lambda + \lambda / \phi$ϕ. This means that the larger the $\phi$ values, the more similar the distribution will be to a pure Poisson process. Let's try one out for size and compare:
 
```{r,results='hold',message=FALSE}
f <- alist(
  SALAMAN ~ dgampois( lambda, phi ),
  log(lambda) <- a + bC*C,
  a ~ dnorm(0,1),
  bC ~ dnorm(0,0.5),
  phi ~ dexp(1))
m2 <- ulam( f , data=d , chains=4,messages = F)

#The plot
plot( d$C , d$SALAMAN , col=rangi2 , lwd=2 ,
      xlab="cover (standardized)" , ylab="salamanders observed" )
C_seq <- seq( from=-2 , to=2 , length.out=30 )
l <- link( m1 , data=list(C=C_seq) )
lines( C_seq , colMeans( l ) )

shade( apply( l , 2 , PI ) , C_seq )

l2 <- link( m2 , data=list(C=C_seq) )
lines( C_seq , colMeans( l2 ), col = "red" )

shade( apply( l2 , 2 , PI ) , C_seq, col = col.alpha("red", 0.15) )
```
 
In general we see that this is not doing anything much better than just the poisson.

We will see in the following chapters how multilevel models are better at capturing trends in different clusters.

