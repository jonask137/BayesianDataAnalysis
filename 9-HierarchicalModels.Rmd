# Ch 13 - Models with Memory

The multilevels is able to contain more knowledge as we not only having a category or index for each category/cluster in the data, but we will have a model, in which parameters can be defined.

This is often beneficiary if you have a model, where you need to interpret the outcome for different levels, i.e., clusters in the data. In the chapter he use the chimpanzee data again, where we will interpret the preference for the left or right lever depending on the unique actor (chimpanzee) and the treatment, thus we will again obtain a model with parameters for each chimp and this is based on priors that are specific for the different chimpanzees.

The purpose of the chapter is to introduce the multi level models, here we modify the mathematical models, where the distribution of $\alpha$ now can be defined by other paramereters, namely *hyperparameters*, the distribution for the *hyperparameters* is called *hyperpriors*.

It is said that multilevel models has memory, that originates in the fitting procedure where we normally will update the whole model as new data / observations are introduced, hence it will simply just disregard previous information. In the multilevel models, we are able to tweak the model seperately, as we have a specific prior for each cluster, hence when we observe one cluster, the prior for this is updated and when we look at another, we (the model) will tweak another prior. Hence by separating this, *recall the café visiting robot that measures waiting times. The traditional robot would always update the one waiting time prior for each café, but the smarter robot will have a prior for each café and then monitor waiting times in one parameter per café (cluster)*.

More on clusters: these are individuals or observations from the same population that share the same characteristics, but you want to observe where these individuals differ.

Three drawbacks (i.e. costs):

1.  Define some distributions from where clusters form

2.  There are some challneges with estimation

3.  THey are hard to understand as they make prediction on different levels of the data.

## Example: Multilevel tadpoles

The outcome of this example is that the multilevel model is able to extract more information from the data. Although it is still a bit abstract.

We had parameters for the parameters (namely hyperparameters) and we now got multiple intercepts.

In this example we have different tanks with different number of tadpoles in each tank (density).

We want to measure how many tadpoles that survives in each tank. Thus we can fit two different models:

1.  Dummy variable for each tank. Using index for each tank. Thus each alpha is assigned the same prior, although recall that from the precis, we will still have an intercept for each tank, it is just not fitted as they are different tanks.
2.  Multilevel model with varying intercepts by tank, thus we will have a prior for each tank, hence can account for survival defaults for each particular tank.

Continouing on the multilevel model: We will see that there is added priors inside the priors (which makes it multilevel)

![](images/paste-E6E11CFD.png){width="315"}

We are now able to learn the data from the inside of the data.

We see that we have have two levels in the model:

1.  Priors for the binomial distribution
2.  Priors for the parameter alpha.

We see that `ulam` will all be updated simultaniously during the markov chain sampling.

Comparing the performance of the fixed model and the multilevel model we see that the latter is the best and it even has less effective parameters (pWAIC), despite initially more paramters.

![](images/paste-A0F5A9F3.png){width="363"}

![](images/paste-80645198.png){width="282"}

Thus the model is less flexible and fits better. The reason is that we are now having better priors.

## Varying effects and the underfitting / overfitting trade-off

### The model

In the examples that we have seen we introduce $\bar{\alpha}$ and $\sigma$ . Sigma is in these cases reflecting the difference that we assume the cluster will have. Hence if sigma = 0, then we say that all clusters are effectively the same (complete pooling) or if we on the other hand set sigma = positive infinity, we will say that the cluster are entirely different, hence we will not pool information (share information). We want to set a prior that is between 0 and positive infinity (which is why exp(1) is neat), as this will enable *partial pooling* and hence we acknowledge cluster being similar but different.

This leads us back to $\bar{\alpha}$, being the mean across all groups and ultimately the mean of the specific clusters (groups) can be defined by the combination of the overall mean and the standard deviation we define under $\sigma$.

Thus, having sigma 0 will lead to total underfit, and a very high sigma will lead to a total overfit.

> we are dancing in the middle of total underfitting and total overfitting.

Ultimately this is about twisting the last information out of the data.

### Assign values to the parameters

### Simulate survivors

Logit link function, thus the probability is defined by the logistic function.

## More than one type of cluster

This is an example with the chimpanzee prosocial example (if they select the option benefiting other chimps in the experiment). Here we will see a cluster for each actor and block.

Notice that actor = chimpanzee, and block = a sequence of experiments happening the same day.

Hence we add a level for each actor and each block as we see in the following model.

![](images/paste-32144185.png){width="457"}

We see that gamma is relating to each block, the mean of gamma is just 0. It seems as the model would just add alpha bar with gamma bar, if we were to add this.

Notice that you can select clusters yourself, although the problem is mostly how complicated model can the user actually cope with.

## Divergent transitions and non-centered priors

Divergent transitions is when you are sampling and the energy is not the same in the beginning as it is in the end, hence the energy is diverging. This typically happens whenever the posterior distribution is very steep in some places but not in others. i.e., that is when there are steep changes in probability.

`ulam()` (HMC) will warn about diverging transitions and this typically implies that there are probability regions that are difficult to explore. E.g., this can happen if there is a valley or trench in a given probability region, like a marian trench.

**This can be overcome with the following:**

1.  Tune the sampler to not overshoot these. This implies a longer warmup phase, with a higher acceptance rate (so it can go farther). You must use `adapt_delta`.

2.  Rewrite the model, i.e., *reparameterization*. Many identitical model can be written mathematically the same, but numerically different. With reparameterization we distinguish between centered and non-centered model:

    ![](images/paste-5BEE5E03.png){width="379"}

A centered model is when the you include a parameter to scale onto a normal distribution with a mean of 0 and standard deviation of 1.

The following are two sections are examples.

#### The Devil's Funnel: (example)

Lets take an example with two parameters, one depending on the other, hence a typical multilevel model.

$$
\nu \sim Normal(0,3)\\
x \sim Normal(0,exp(\nu))
$$

This model may seem simple, but apparantly it is horrible to sample from.

```{r}
m13.7 <- ulam(
  alist(
    v ~ normal(0,3),
    x ~ normal(0,exp(v))
  ), data = list(N=1), chains = 4
)
```

We see that there are many divergent transition, we see that this results in an ineffective sampling (we look at the traceplot after), where the effective sample size is low, hence the model means may be reliable.

From the precis below, we see how few effective samples we are having.

```{r}
precis(m13.7)
```

Lets at the traceplot.

```{r}
traceplot(m13.7)
```

We see that `v` appear is somewhat OK, although it might not be entirely stationary. But we know that `x` depends on `v` and the exploration of `x` does look horrible, we are stuck in several places for many iterations. Althougnh we do get fewer effectie samples from the `v` parameter, even though it might look more hairy than `x` (this might be due to the scaling issues). Thus we can look at the trank plot.

We can also take look at the `trankplot()` and it becomes clear why we get fewer effective samples from `v`, as it is showing much randomness. Hence due to that the few effective number of samples, we deem this an unhelathy chain.

```{r}
trankplot(m13.7)
```

**Why is this model not working:**

We see that as $\nu$ changes the distriubtion of $x$ is changing.

```{r}
x = -10:10
y = dnorm(x = x,mean = 0,sd = 3)
plot(x,y,type = 'l',main = "v prior")
```

Hence we see that v can output a lot of different numbers, thus the distribution of x can take many shapes! Hence the dsitribubtino of x is conditional on another parameter, this is called **centered parametrization**.

This can be addressed with **non-centered parametrization**, where one parameter is is not conditional on one or more other parameters.

We can then specify the model with the following:

$$
\nu \sim Normal(0,3)\\
z \sim Normal(0,1)\\
x = z * exp(\nu)
$$

Now we see that the distribution of $z$ is fixed, and then $x$ is defined by z and $\nu$. Essentially what z is doing, is that it is standardizing the values, hence onto a scale, with a mean of 0 and a standard deviation of 1.

Now we can fit the model:

```{r}
m13.7 <- ulam(
  alist(
    v ~ normal(0,3),
    z ~ normal(0,1),
    gq> real[1]:x <<- z*exp(v)
  ),data = list(N=1),chains = 4
)
```

Now we do not get any warnings, lets look at the precis.

```{r}
precis(m13.7)
```

We see that we have imensely mroe effective samples.

```{r,results='hold'}
traceplot(m13.7)
trankplot(m13.7)
```

Now we see the hairy cattarpillar, as it is stationary and hairy. Thus, it is controlled randomness where much more of the probability space is being explored.

#### Non-cnetered chimpanzees (example)

This is another example. Here we will aim for increasing the acceptance rate (`adapt_delta`). THe default is 0.95, we will fix it to 0.00.

We will initially use m13.4 and then update it with the adept_delta.

First the initial model is specified, and run, we will see that we get some divergent transitions, hence ineffective sampling.

```{r m13.4}
library(rethinking)
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition

dat_list <- list(
  pulled_left = d$pulled_left
  ,actor = d$actor
  ,block_id = d$block
  ,treatment = as.integer(d$treatment)
)

set.seed(13)
m13.4 <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a[actor] + g[block_id] + b[treatment] ,
    b[treatment] ~ dnorm( 0 , 0.5 ),
    
    ## adaptive priors
    a[actor] ~ dnorm( a_bar , sigma_a ),
    g[block_id] ~ dnorm( 0 , sigma_g ),
    
    ## hyper-priors
    a_bar ~ dnorm( 0 , 1.5 ),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1)
  ) 
  ,data=dat_list 
  ,chains=4 
  ,cores=4 
  ,log_lik=TRUE 
  )
```

Now we can inspect the model parameters, we have a lot, hence we can also plot these.

```{r}
precis(m13.4,depth = 2)
```

```{r}
plot(precis(m13.4,depth = 2))
```

##### Solution 1: Centered parameterization (Increasing acceptance rate)

Let us now run the same mode, with a higher acceptance rate:

```{r 13.28}
set.seed(13)
m13.4b <- ulam(m13.4,chains = 4,cores = 4,control = list(adapt_delta = 0.99))
#divergent(m13.4b) #cannot see what this does
```

Now we see that we do not get any divergent transitions, although the sampling is still not perfect. Lets inspect the `precis`.

```{r,results='hold'}
precis(m13.4b,depth = 2)
plot(precis(m13.4b,depth = 2))
```

Notice that the effective number of samples is not much different.

```{r,results='hold'}
prec1 = precis(m13.4,depth = 2)
prec2 = precis(m13.4b,depth = 2)
plot(prec1$n_eff,type = 'l',col = 'blue',ylab = 'n_eff',xlab = 'Parameters',ylim = c(0,1200),xaxt = 'n')
#xtick <- row.names(prec1)
xtick<-seq(1, nrow(prec1), by=1)
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick,y = par("usr")[3], 
     labels = row.names(prec1), srt = 45, pos = 1, xpd = TRUE)
grid()
lines(prec2$n_eff,col = 'red',lty = 2)
legend("topleft",legend = c("Initial model w. div. trans.","Iterated model"),lty = 1:2,col = c("blue","red"),cex = 0.8)
```

We see that the effective number of samples tends to be higher, although not always. We had four chains with 500 samples from each, hence 2000 samples, so not a perfect exploration.

**Improving through reparametirization**

We see that we can improve the effective number of samples, by doing the same trick as in the previous example. Thus we will add a parameter z and x to standardize the results, the model then looks the following:

![](images/paste-5AD51626.png)

Thus, we see that *z* gives the standardized intercept for each actor, and the evctor *x* gives the standardiced intercept for each block.

We still have a linear model that is mapped on to the logistic function, hence the logit(p), where all of the initial parameters still appear.

Notice that the intercept for each actor (chimp) is:

$$
\alpha_j = \bar{\alpha} + z_j * \sigma_\alpha
$$

Thus we have mean alpha plus the parameter for each chimp times sampling from the exponential distribution, where values tends towards 0.

##### Solution 2: Non-centered version (reparameterize)

Thus we have **reparametized** and the model can now be specified with the following and also sampled from.

```{r}
set.seed(13)
m13.4nc <- ulam(
  alist(
    pulled_left ~ dbinom( 1 , p ) ,
    logit(p) <- a_bar + z[actor]*sigma_a + # actor intercepts
    x[block_id]*sigma_g + # block intercepts
    b[treatment] ,
    b[treatment] ~ dnorm( 0 , 0.5 ),
    z[actor] ~ dnorm( 0 , 1 ),
    x[block_id] ~ dnorm( 0 , 1 ),
    a_bar ~ dnorm( 0 , 1.5 ),
    sigma_a ~ dexp(1),
    sigma_g ~ dexp(1),
    gq> vector[actor]:a <<- a_bar + z*sigma_a,
    gq> vector[block_id]:g <<- x*sigma_g
  ) 
  ,data=dat_list 
  ,chains=4 
  ,cores=4
  )
```

Notice that we do not get any warnings

```{r}
precis(m13.4nc,depth = 2)
```

```{r}
prec1 = precis(m13.4,depth = 2)
prec2 = precis(m13.4b,depth = 2)
prec3 = precis(m13.4nc,depth = 2)
plot(prec1$n_eff,type = 'l',col = 'blue',ylim = c(0,2000),xaxt = 'n'
     ,main = 'Comparison on model variates',ylab = 'n_eff',xlab = 'Parameters')
#xtick <- row.names(prec1)
xtick<-seq(1, nrow(prec1), by=1)
axis(side=1, at=xtick, labels = FALSE)
text(x=xtick,y = par("usr")[3], 
     labels = row.names(prec1), srt = 45, pos = 1, xpd = TRUE)
grid()
lines(prec2$n_eff,col = 'red',lty = 2)
lines(prec3$n_eff,col = 'purple',lty = 2)
legend("topleft",legend = c("Initial model w. div. trans.","Delta_adapt = 0.99","Reparameterized"),lty = c(1,2,2),col = c("blue","red","purple"),cex = 0.8)
```

Now we see that the sampling is far far better.

## Multilevel posterior predictions

This section elaborates on how one can make predictions from a multilevel model. He distinguish two types of predictions:

1.  Posterior prediction for same clusters
2.  Posterior prediction for new clusters

*What is a cluster? In the chimpanzee example, we have 7 unique chimps, these are the clusters, hence we have one cluster for each actor.*

Thus we distinguish between predicting a known cluster and an unknown cluster.

Notice that to evaluate model predictions we can use information criteria, such as AIC or WAIC.

### Posterior prediction for same clusters

In this example we predict outcomes for a given actor, hence for a cluster that we already know.

```{r 13.31}
chimp <- 2
d_pred <- list(
  actor = rep(chimp,4),
  treatment = 1:4,
  block_id = rep(1,4)
)
p <- link(m13.4,data = d_pred)
p_mu <- apply(p,2,mean)
p_ci <- apply(p,2,PI)
```

now we can extract priors from the model.

```{r 13.32}
post <- extract.samples(m13.4)
str(post)
```

We see that the actors are on the columns, hence we want to select the given column for the chimp. In this case, we select chimp intersects form the alpha object. We can plot the posterior distribution for the intersect for chimp 5. Hence:

```{r}
dens(post$a[,5]) #Actor 5
```

Now we build our own link finction to create predictions.

```{r 13.34}
p_link <- function( treatment , actor=1 , block_id=1 ) {
  logodds <- with( post ,
  a[,actor] + g[,block_id] + b[,treatment] ) #We select the columns with the input in the func.
  return( inv_logit(logodds) )
  }
```

```{r 13.35}
p_raw <- sapply(X = 1:4 , function(i) p_link(treatment = i , actor=2 , block_id=1 ) ) #We loop over 1 to 4, where treatment  = i (1 to 4)
p_mu <- apply( p_raw , 2 , mean )
p_ci <- apply( p_raw , 2 , PI )
```

```{r}
plot(p_mu,type = 'l',ylim = c(0.9,1),xaxt = 'n',xlab = 'Treatment')
axis(1,at = 1:4,labels = c(1,2,3,4),lwd.ticks = 1,lwd = 0)
shade(p_ci,1:4)
```

We see that for all treatments, the chimp will favour pulling the left lever.

### Posterior prediction for new clusters

Now we can do the same, but for a chimp that is not in the experiment that we have, imagine having to estimate the same experiment for a whole new population of chimps.

For this we will construct an average chimp.

```{r}
#13.36
p_link_abar <- function( treatment ) {
  logodds <- with( post , a_bar + b[,treatment] )
  return( inv_logit(logodds) )
  }

#13.37 - extract samples and get the mean and compability intervals
post <- extract.samples(m13.4)
p_raw <- sapply( 1:4 , function(i) p_link_abar( i ) )
p_mu <- apply( p_raw , 2 , mean )
p_ci <- apply( p_raw , 2 , PI ) #Recall default = 0.89

#Plotting the outcome
par(mfrow = c(1,2))
plot( NULL , xlab="treatment" , ylab="proportion pulled left" ,
      ylim=c(0,1) , xaxt="n" , xlim=c(1,4),main = 'Average actor')
axis( 1 , at=1:4 , labels=c("R/N","L/N","R/P","L/P") )
lines( 1:4 , p_mu )
shade( p_ci , 1:4 )

#13.38 - simulate chimps
a_sim <- with( post 
               #use rnorm to sample random chimps
               ,rnorm(n = length(post$a_bar) #We want as many samples extracted samples
                       ,mean = a_bar #mean = the average chimp
                       ,sd = sigma_a #The standard deviation
                      ) 
               )

p_link_asim <- function( treatment ) {
  logodds <- with( post , a_sim + b[,treatment] )
  return( inv_logit(logodds) )
  }

p_raw_asim <- sapply(1:4 #Simulate over treatment 1 to 4
                     ,function(i) p_link_asim(treatment = i) #apply function with input i using function p_link_asim() with treatm. = i
                     )

#13.39 - plotting the simulated actors
plot( NULL , xlab="treatment" , ylab="proportion pulled left" ,
      ylim=c(0,1) , xaxt="n" , xlim=c(1,4),main = 'Simulated actors')

axis( 1 , at=1:4 , labels=c("R/N","L/N","R/P","L/P") )

for ( i in 1:100 ) lines( 1:4 , p_raw_asim[i,] , col=grau(0.25) , lwd=2 )
```

Recall that the treatments were:

1.  Two food items on right and no partner
2.  Two food items on left and no partner
3.  Two food items on right and partner present
4.  Two food items on left and partner present

We see that whenever the foods is on the left, there is a tendency to to pull the left lever, no matter if a partner is present or not.

Hence we can use this information to simulate examples.

## Exercises

### E1

*Which of the following priors will produce more shrinkage in the estimates? (a) αtank ∼ Normal(0, 1); (b) αtank ∼ Normal(0, 2).*

The first will have the most shrinkage as the standard deviation of the normal distribution is the smallest.

### E2

*Make the following model into a multilevel model.*

$$
y_i \sim Binomial(1,p_i)\\
logit(p_i) = \alpha_{GROUP[i]} + \beta x_i\\
\alpha_{GROUP} \sim Normal(0,10)\\
\beta \sim Normal(0,1)
$$

This can be transformed into:

$$
\begin{align*}
y_i \sim Binomial(1,p_i)\\
logit(p_i) = \alpha_{GROUP[i]} + \beta x_i \\
\alpha_{GROUP} \sim Normal(\bar{\alpha},\sigma_\alpha) && \text{Insert hyperparameter in the prior} \\
\beta \sim Normal(0,1)\\
\bar{\alpha} \sim Normal(0,10) && \text{Define distributions for new hyperpriors} \\
\sigma_\alpha \sim Normal(0,1) && \text{Define distributions for new hyperpriors} \\
\end{align*}
$$

### E3

*Make the following model into a multilevel model.*

<!-- && \text{} -->

$$
\begin{align*}
y_i \sim Binomial(\mu_i,\sigma) && \text{Notice that we already have a sigma}\\
\mu_i = \alpha_{GROUP[i]} + \beta x_i\\
\alpha_{GROUP} \sim Normal(0,10)\\
\beta \sim Normal(0,1) \\
\sigma \sim HalfCaucy(0,2)
\end{align*}
$$

Now we already have a sigma, we will just define a new sigma for each alpha (cluster), that will have their own intersect.

$$
\begin{align*}
y_i \sim Binomial(\mu_i,\sigma) && \text{Notice that we already have a sigma}\\
\mu_i = \alpha_{GROUP[i]} + \beta x_i\\
\alpha_{GROUP} \sim Normal(\bar{\alpha},\sigma_\alpha) && \text{We add the hyperparameters} \\
\beta \sim Normal(0,1) \\
\sigma \sim HalfCaucy(0,2) \\
\bar{\alpha} \sim Normal(0,10) && \text{The new hyperpriors} \\
\sigma_\alpha \sim Normal(0,1) && \text{The new hyperpriors}
\end{align*}
$$

### E4

*Write an example mathematical model formula for a Poisson regression with varying intercepts.*

Notice that the poisson distribution relates to an exponential scenario, thus the logit cannot be used, we will then use the log function.

$$
\begin{align*}
y_i \sim Poisson(\lambda_i)\\
log(\lambda_i) = \alpha_{GROUP[i]} + \beta x_i\\
\alpha_{GROUP} \sim Normal(\bar{\alpha},\sigma_\alpha) && \text{We add the hyperparameters} \\
\beta \sim Normal(0,1) \\
\bar{\alpha} \sim Normal(0,10) && \text{The hyperprior} \\
\sigma_\alpha \sim Normal(0,1) && \text{The hyperprior}
\end{align*}
$$

### M1

Revisit the Reed frog survival data, `data(reedfrogs)`, and add the predation and size treatment variables to the varying intercepts model. Consider models with either main effect alone, both main effects, as well as a model including both and their interaction. Instead of focusing on inferences about these two predictor variables, focus on the inferred variation across tanks. Explain why it changes as it does across models.

```{r}
library(rethinking) 
data(reedfrogs)
d <- reedfrogs

dat <- list(
  S = d$surv, #Survival - absolute numbers
  n = d$density, #Density in tadpole
  tank = 1:nrow(d), #Tank number
  pred = ifelse( d$pred=="no" , 0L , 1L ), #pred = predator, if pred then predator
  size_ = ifelse( d$size=="small" , 1L , 2L ) #Size of the tadpoles
)
```

Now we can define the varying intercepts model. This is basically just as in the book.

```{r}
m1.1 <- ulam(
  alist(
    S ~ binomial( n , p ),
    logit(p) <- a[tank], #We have simply just an intercept for each tank (mean)
    a[tank] ~ normal( a_bar , sigma ),
    a_bar ~ normal( 0 , 1.5 ), #Mean of 0 with sd of 1
    sigma ~ exponential( 1 )
  ), data=dat , chains=1 , cores=1 , log_lik=TRUE )
precis(m1.1,depth = 1)
```

In general we are not having super high n_eff but there are no warnings, hence the model should be fine, but lets look at the trace plots to make sure. Notice that this is the equivalent of saying `depth = 2` in the precis.

```{r}
traceplot(m1.1)
```

We see that the scales are wide, although it does look stationary and a bit hairy, one may inspect each plot individually. Although in general we are having a lot of effective observations.

Now we can define a model with a slope parameter for the presence of predators in the tadpoles, this is named bp.

```{r}
# pred 
m1.2 <- ulam(
  alist(
    S ~ binomial( n , p ),
    logit(p) <- a[tank] + bp*pred, #Adding parameter for predator
    a[tank] ~ normal( a_bar , sigma ), #abar is the mean, and then there is a deviatino for each tadpole
    bp ~ normal( -0.5 , 1 ), #We say on average there are none, but the right tail does allow preds
    a_bar ~ normal( 0 , 1.5 ),
    sigma ~ exponential( 1 )
  ), data=dat , chains=1 , cores=1 , log_lik=TRUE )
precis(m1.2)
```

Here we see that this model is much more difficult to sample from, as we have way less effective observations. We may plot the trace or trank plot, but we already have a good idea on the conclusion.

Now lets define a model where we include tank size instead. Notice there are two different tank sizes

```{r}
# size
m1.3 <- ulam(
  alist(
    S ~ binomial( n , p ),
    logit(p) <- a[tank] + s[size_], #tank size is included.
    a[tank] ~ normal( a_bar , sigma ),
    s[size_] ~ normal( 0 , 0.5 ),
    a_bar ~ normal( 0 , 1.5 ),
    sigma ~ exponential( 1 )
  ), data=dat , chains=1 , cores=1 , log_lik=TRUE )
precis(m1.3)
```

Again we see bad sampling

Now we can add both size and predators.

```{r}
# pred + size
m1.4 <- ulam(
  alist(
    S ~ binomial( n , p ),
    logit(p) <- a[tank] + bp*pred + s[size_], #We add bo and size parameter
    a[tank] ~ normal( a_bar , sigma ),
    bp ~ normal( -0.5 , 1 ),
    s[size_] ~ normal( 0 , 0.5 ),
    a_bar ~ normal( 0 , 1.5 ),
    sigma ~ exponential( 1 )
  ), data=dat , chains=1 , cores=1 , log_lik=TRUE )
precis(m1.4,depth = 2)
```

Again bad sampling. Notice that there is a parameter for each size and one slope and then one intercept for each tank, we have 48 tanks.

Lastly we can add an interaction between size and predators. That is done in the following:

```{r}
# pred + size + interaction
m1.5 <- ulam(
  alist(
    S ~ binomial( n , p ),
    logit(p) <- a_bar + z[tank]*sigma + bp[size_]*pred + s[size_],
    z[tank] ~ normal( 0 , 1 ),
    bp[size_] ~ normal( -0.5 , 1 ),
    s[size_] ~ normal( 0 , 0.5 ),
    a_bar ~ normal( 0 , 1.5 ),
    sigma ~ exponential( 1 )
  ), data=dat , chains=1 , cores=1 , log_lik=TRUE )
precis(m1.5,depth = 2)
```

Now we have one overall mean, then a z score for each tank that is to the magnitude of the standard deviation. Then we have a slope for the predators (beta p) and a coefficient for each size of tadpoles, notice we have two - small and big.

Lastly we can look at the posterior predictions for sigma as we want to see which model that has the largest variance.

```{r}
plot( coeftab( m1.1 , m1.2 , m1.3 , m1.4 , m1.5 ), pars="sigma")
```

We see that `m1.1` and `m1.3` both have a larger variance, these differ as they exclude predators. Thus adding predator effects in the model makes the variance lower, i.e., we include meaningful information.


### H1

```{r}
library(rethinking)
data(bangladesh)
d <- bangladesh
d$district_id <- as.integer(as.factor(d$district))
dat_list <- list(
  C = d$use.contraception,
  did = d$district_id )
```

#### Fixed effects model:

```{r}
m13H1.1 <- ulam( 
  alist(
    C ~ bernoulli( p ), #bernoulli distribution
    logit(p) <- a[did],
    a[did] ~ normal( 0 , 1.5 )
  ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )
```



#### Varying intercepts model:

```{r}
m13H1.2 <- ulam( 
  alist(
    C ~ bernoulli( p ),
    logit(p) <- a[did],
    a[did] ~ normal( a_bar , sigma ),
    a_bar ~ normal( 0 , 1.5 ),
    sigma ~ exponential( 1 )
  ) , data=dat_list , chains=4 , cores=4 , log_lik=TRUE )
```

#### Extract samples

Now we can extract from each model to see how they estimate the data.

```{r,results='hold',fig.cap="Blue = Fixed effects, Black = open circels, Plus = raw data"}
post1 <- extract.samples( m13H1.1 )
post2 <- extract.samples( m13H1.2 )
p1 <- apply( inv_logit(post1$a) , 2 , mean )
p2 <- apply( inv_logit(post2$a) , 2 , mean )
# compute raw estimate from data in each district
t3 <- table( d$use.contraception , d$district_id )
n_per_district <- colSums( t3 )
p_raw <- as.numeric( t3[2,]/n_per_district )
nd <- max(dat_list$did)
{
  plot( NULL , xlim=c(1,nd) , ylim=c(0,1) , ylab="prob use contraception" ,
      xlab="district" )
  points( 1:nd , p1 , pch=16 , col=rangi2 )
  points( 1:nd , p2 )
  points( 1:nd , p_raw , pch=3 )
  abline( h=mean(inv_logit(post2$a_bar)) , lty=2 )
  #identify( 1:nd , p_raw , labels=n_per_district ) #include in rmd to interact with the plot
}
```

In general we see that the varying effects is shring towards the mean relative to the fixed effets and the raw data.


# Ch 14 - Advuntetures in Coviaraince

in the previous chapter we saw how varying intercepts can help the model distinguish between different groups in the data. Now we are going to explore how varying slopes also can come in hand.

Thus with varying intercepts we are basically able to have a model with different means for the given groups. And with the slopes we are able to build 'massive interaction machines'.

Up until now we have only worked with unordered categorical variables, although in this chapter we will also take on variables of continous categories, that will be done with gaussian processes (whatever that is).

We are also going to do inference on instrumental variables, that is inferring cause without closing the backdoor paths.

Why multilevel models with varying slopes? We see that only focusing on averages and even varying intercepts we see that we are generalizing and hence loosing information. Thus including varying intercepts we may end up with a conclusion where we can conclude one thing for a part of the data and another for another part of the data.

## Varying slopes by construction

We need a 2-dimensional Gaussian distribution to represent the slope and the intercept.

Then we also need a correlation matrix, as we need sigmas for each parameter, as they can practically have their own spread.

![](images/paste-DCA25A95.png)

The varying slopes model, we see the following model:

![](images/paste-DBBCDD03.png){width="264"}

We see that:

-   mu = is the linear model with the varying intercepts and the varying slopes

-   The matrix = the mutlivariate prior. one for each cafe and one for if it is in the afternoon.

-   S = the covariance matrix, see the slides for an example of how that is found. The pc should find the result of this itself.

-   The rest (alpha to sigma beta) = is just fixed (non-adaptive) priors.

    -   Alpha mean = 5, hence we expect to wait five minutes on average with an sd of 2.

    -   Beta (coefficient for afternoon) = -1 hence we expect to wait less than average in the afternoon. that is with an sd of 0.5.

-   R = correlation matrix prior. We use LKJ prior. That is a nice family of priors. It is simply a distirbution and named after researcher names.

More on the LKJ prior (the onion prior). It looks the following:

![](images/paste-0C16324B.png)

We can adjust eta. When:

-   Eta = 1 : we have a uniform correlation matrix.

-   Eta \< 1 : then it is sceptical according to correlation between slopes and intercepts.

-   Eta \> 1 : it elevates extreme correlations.

### Simulate the population

We start with m14.1, where we have random intercepts and slopes.

We start by simulating the cafés that the robot is going to visit in the next subsection.

```{r 14.1}
a <- 3.5 # average morning wait time
b <- (-1) # average difference afternoon wait time
sigma_a <- 1 # std dev in intercepts
sigma_b <- 0.5 # std dev in slopes
rho <- (-0.7) # correlation between intercepts and slopes
```

Now we create a vector of means for alpha and beta. Mean for alpha = general wait time in the morning and beta = wait time in the afternoon.

```{r 14.2}
Mu <- c( a , b )
Mu
```

We see that in the afternoon the waits tend to be less.

Now we can build the covariance matrix.

We can build matrices in two ways:

1.  By using matrix:

```{r 14.3}
cov_ab <- sigma_a*sigma_b*rho
Sigma <- matrix( c(sigma_a^2,cov_ab,cov_ab,sigma_b^2) , ncol=2 )
Sigma
```

2.  By do it more manually

```{r 14.5}
sigmas <- c(sigma_a,sigma_b) # standard deviations
Rho <- matrix( c(1,rho,rho,1) , nrow=2 ) # correlation matrix
# now matrix multiply to get covariance matrix
Sigma <- diag(sigmas) %*% Rho %*% diag(sigmas)
```

We see that the result is the same.

**Now we can simulate** this is just done by randomly sampling from the multivariate Gaussian distribution as we already have Mu and Sigma.

Notice that we will get two vectors, the intecept and the slopes and the matrix we end up having represent the cafés, one for each café.

```{r 14.7, message=TRUE}
library(MASS)
N_cafes <- 20
set.seed(5) # used to replicate example
vary_effects <- mvrnorm(n = N_cafes , Mu , Sigma )
message("Each row is a cafe, V1 = intercept, V2 = slope, it is plotted in a following chunk")
head(vary_effects)
```

Now we can take the vectors and store them in a more meaningful object.

```{r 14.8}
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]
```

Now we can plot the intercepts and slopes, we see that the simulated data has a negative correlation. We see that x = intercepts and y = the slopes.

Notice that we use ellipses in the plot, these show the quantiles of the data, in a later plot the actual values are presented. But in general the closer you are to the centrum the more observations we are expected to see. Thus in general the intercepts tends towards 3 - 4 and a slope around -1. Recall that we we:

-   set the mean of alpha = 3.5 and slope to -1 with,
-   with standard deviations of respectively 1 and 0.5, hence the result is very much expected.
-   We also explicitly set rho (correlation between slope and intercept) to be -0.7, hence the result is again very much as expected.
-   If you count the points, you will see that there are 20, one for each café.

```{r 14.9}
plot( a_cafe , b_cafe , col=rangi2 ,
xlab="intercepts (a_cafe)" , ylab="slopes (b_cafe)" )
# overlay population distribution
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
lines(ellipse(Sigma,centre=Mu,level=l),col=col.alpha("black",0.2))
```

Now we have simulated the data. It is now left for the robot to visit the cafés. That is done in the following subsection.

### Simulate observations

Here we will generate the observations that the robot will be making.

```{r 14.10}
library(MASS)
set.seed(5) # used to replicate example
N_cafes <- 20
vary_effects <- mvrnorm( N_cafes , Mu , Sigma )

set.seed(22)
N_visits <- 10
a_cafe <- vary_effects[,1]
b_cafe <- vary_effects[,2]

afternoon <- rep(0:1,N_visits*N_cafes/2) #Every second is afternoon
cafe_id <- rep( 1:N_cafes , each=N_visits ) #Each café is visited 10 times
mu <- a_cafe[cafe_id] + b_cafe[cafe_id]*afternoon #Mean waiting times, every second is lower as it is afternoon.
sigma <- 0.5 # std dev within cafes
wait <- rnorm( N_visits*N_cafes , mu , sigma ) #The wait is defined by normal distribution based on mu and sigma

#Collect information in a data frame
d <- data.frame( cafe=cafe_id , afternoon=afternoon , wait=wait )
head(d,15)
```

We can also plot them agains each other, here it is clear that each café is having very different mean waiting times

```{r}
# library(dplyr)
# library(magrittr)
# d1 <- d %>% 
#   group_by(cafe,afternoon) %>% 
#   mutate(mu_group = mean(wait)) %>% 
#   select(-wait) %>% 
#   distinct()
# 
# plot(d1,col = alpha(d1$afternoon + 1, 0.5), pch=16,main = "Black = morning, red = afternoon")
```

![](images/paste-18F9298B.png)

Notice that in the windoes with café each horizontal line is a cafe and wee see the different expected waiting times based on the time of the day. We see that overall the waiting time is the largest in the afternoon and also the cafés are differnt, some have a longer waiting time than the other.

Thus it is interesting to have multiple levels on the slopes as well.

### The varying slopes model

This section elaborates on how we need to add the varying effects (the varying slopes). To do this, one must construct a matrix with the parameters for each group. We see in the following chunk that we have a matrix given $\alpha_{cafe}$ and $\beta_{cafe}$. Whenever you see the square brackets it basically means that it is a matrix. Notice that the varying effects is explained by a multivariate normal distribution, basically a distribution based on more than one distribution, try to google it for visualization.

Then we have S here the standard deviations is multiplied by R times the stadnard deviations. R is typically given by LKJcorr distribution.

LKJcorr is a distribution that is explained by eta. It takes the following shapes:

-   eta = 1, then uniform correlation matrices
-   eta \> 1, then correlation between intersect and slopes, but not extreme
-   eta \< 1, elevates extreme correlations

An example:

![](images/paste-B415E975.png)

In the following an example is also plotted.

```{r 14.11}
R <- rlkjcorr( 1e4 , K=2 , eta=2 )
dens( R[,1,2] , xlab="correlation" )
```

```{r 14.12}
set.seed(867530)
m14.1 <- ulam(
  alist(
    wait ~ normal( mu , sigma ),
    mu <- a_cafe[cafe] + b_cafe[cafe]*afternoon,
    c(a_cafe,b_cafe)[cafe] ~ multi_normal( c(a,b) , Rho , sigma_cafe ),
      #' a = alpha, b = beta, this create a matrix, hence each
      #' row in the matrix = a cafe, that has a pair of params, a and b
      #' We then write multi_normal, where provide the a and be params
      #' Ulam knows how Rho matrix needs to be set up.
  
    a ~ normal(5,2),
    b ~ normal(-1,0.5),
    sigma_cafe ~ exponential(1),
    sigma ~ exponential(1),
    Rho ~ lkj_corr(2) #i.e., the onion prior. Eta = 2
  ) , data=d , chains=4 , cores=4 )
```

```{r 14.13}
post <- extract.samples(m14.1)
dens( post$Rho[,1,2] , xlim=c(-1,1) ) # posterior
R <- rlkjcorr( 1e4 , K=2 , eta=2 ) # prior
dens( R[,1,2] , add=TRUE , lty=2 )
```

*Notice that the dotted line is the prior and the solid line is the posterior.*

We see that almost all of the mass is between zero, hence a negative relationship. That is as expected as that is what we modeled it for.

Now we can plot the differences between the raw data values (blue points) hence fixed effects estimates. The open circles are the varying effects estimates. Hence we see that the varying effects model is producing a bit different results. That is the product of being able to differentiate the model between different categories (cafés and waiting time) in the data.

```{r 14.14, message=FALSE}
# compute unpooled estimates directly from data
a1 <- sapply( 1:N_cafes ,
  function(i) mean(wait[cafe_id==i & afternoon==0]) )
b1 <- sapply( 1:N_cafes ,
  function(i) mean(wait[cafe_id==i & afternoon==1]) ) - a1

# extract posterior means of partially pooled estimates
post <- extract.samples(m14.1)
a2 <- apply( post$a_cafe , 2 , mean )
b2 <- apply( post$b_cafe , 2 , mean )

# plot both and connect with lines
plot( a1 , b1 , xlab="intercept" , ylab="slope" ,
  pch=16 , col=rangi2 , ylim=c( min(b1)-0.1 , max(b1)+0.1 ) ,
  xlim=c( min(a1)-0.1 , max(a1)+0.1 ) )

points( a2 , b2 , pch=1 )

for ( i in 1:N_cafes ) lines( c(a1[i],a2[i]) , c(b1[i],b2[i]) )

# compute posterior mean bivariate Gaussian 14.15
Mu_est <- c( mean(post$a) , mean(post$b) )
rho_est <- mean( post$Rho[,1,2] )
sa_est <- mean( post$sigma_cafe[,1] )
sb_est <- mean( post$sigma_cafe[,2] )
cov_ab <- sa_est*sb_est*rho_est
Sigma_est <- matrix( c(sa_est^2,cov_ab,cov_ab,sb_est^2) , ncol=2 )
# draw contours
library(ellipse)
for ( l in c(0.1,0.3,0.5,0.8,0.99) )
lines(ellipse(Sigma_est,centre=Mu_est,level=l),
      col=col.alpha("black",0.2))
```

Plot interpretation:

-   The contours of the inferred population, thus the multivariate distribution that is learned from the data. We see that each ellipse is the quantiles of the data. hence 50 = 50% of the data.

-   We see that we have a couple of observations with a high slope (top of the graph). We see that the model is skeptical of that. Although when it regularize that, it also alters the intercept, that is because the model knows that slope and intercept is correlated.

## Advanced varying slopes

```{r 14.18}
library(rethinking)

data(chimpanzees)
d <- chimpanzees
d$block_id <- d$block
d$treatment <- 1L + d$prosoc_left + 2L*d$condition

dat <- list(
  L = d$pulled_left,
  tid = d$treatment,
  actor = d$actor,
  block_id = as.integer(d$block_id) )

set.seed(4387510)
m14.2 <- ulam(
  alist(
    
    L ~ dbinom(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
      #' After alpha we have a matrix consisting of actor and treatment
      #' alpha = an actor and tid = treatment, hence for each actor there 
      #' is information about the given actor and its treatment.
      
      #' It goes the same for the blocks, where the matrix consists of:
        #' row values = block id and each column = treamtnent id
    
    # adaptive priors
    vector[4]:alpha[actor] ~ multi_normal(0,Rho_actor,sigma_actor),
      #' We jave a matrix consisting of 4 treatments and the actors
      #' vector[4] = we have four different treatments
      #' It is distributed as a multivariate normal distribution
      #' Rho actor = a matrix, the model already knows that
      #' sigma_actor = a vector of length 4
    vector[4]:beta[block_id] ~ multi_normal(0,Rho_block,sigma_block),
    
    # fixed priors
    g[tid] ~ dnorm(0,1),
    sigma_actor ~ dexp(1), 
    Rho_actor ~ dlkjcorr(4), #The onion prior, eta = 4 to regulaize
    sigma_block ~ dexp(1),
    Rho_block ~ dlkjcorr(4) #the onion prior, just for the blovks
  ) , data=dat , chains=4 , cores=4 )
```

We see that we are getting a lot of divergent transitions, that is what we have seen before, thus we **should do the non centered model**.

This is similar varying intercepts, although it is a bit more complex when having varying slopes.

To do this we are going to apply a Cholesky method (Cholesky Magic) this is done internally. There is an example of this in the slides, although it appears to be outside of the course scope.

**Now we are going to do the uncentered model (respecify the model)**

```{r 14.19}
set.seed(4387510)
m14.3 <- ulam(
  alist(

    L ~ binomial(1,p),
    logit(p) <- g[tid] + alpha[actor,tid] + beta[block_id,tid],
      #' We have matrices for alpha and beta, rows = actor / block and columns = treatment id
    
    # adaptive priors - non-centered
    transpars> matrix[actor,4]:alpha <- #rows = actors, columns = treatments
      compose_noncentered( sigma_actor , L_Rho_actor , z_actor ),
    
      #' Transparse is just fitting the code into a stan framework
      #' L_Rho_actor is some cholesky scores and z_actor is some z-scores for each actor
    
    transpars> matrix[block_id,4]:beta <-
      compose_noncentered( sigma_block , L_Rho_block , z_block ),
      matrix[4,actor]:z_actor ~ normal( 0 , 1 ),
      matrix[4,block_id]:z_block ~ normal( 0 , 1 ),
    
    # fixed priors
    g[tid] ~ normal(0,1),
    vector[4]:sigma_actor ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_actor ~ lkj_corr_cholesky( 2 ), 
      #' cholesky version of LKJ
      #' LKJ distribution is still used
    vector[4]:sigma_block ~ dexp(1),
    cholesky_factor_corr[4]:L_Rho_block ~ lkj_corr_cholesky( 2 ),
    
    # compute ordinary correlation matrixes from Cholesky factors
    gq> matrix[4,4]:Rho_actor <<- Chol_to_Corr(L_Rho_actor),
    gq> matrix[4,4]:Rho_block <<- Chol_to_Corr(L_Rho_block)
  ) 
  ,data=dat 
  ,chains=4 
  ,cores=4 
  ,log_lik=TRUE 
  )
```

Now we will see that the model produce much more efficient results, although the result is the exact same, the model is simply just able to explore the probability regions more effectively.

We see from the following plot that the effective samples fro the non-centered model is far better.

```{r}
# extract n_eff values for each model
neff_nc <- precis(m14.3,3,pars=c("alpha","beta"))$n_eff
neff_c <- precis(m14.2,3,pars=c("alpha","beta"))$n_eff
plot( neff_c , neff_nc , xlab="centered (default)" ,
ylab="non-centered (cholesky)" , lwd=1.5 )
abline(a=0,b=1,lty=2)
```

#### Inference!

Here we will see the effects, the first 4 being the actor effects (one for each treatment) and the same principle with the blocks, one for each treamt

```{r}
precis( m14.3 , depth=2 , pars=c("sigma_actor","sigma_block") )
```

We see that the mean of the blocks is more or less the same, that is because the model has learned that the blocks are basically the same.

Now we can intepret how the estimated data compared to the raw data.

```{r 14.22}
# compute mean for each actor in each treatment 
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )

# generate posterior predictions using link
datp <- list(
actor=rep(1:7,each=4) ,
tid=rep(1:4,times=7) ,
block_id=rep(5,times=4*7) )
p_post <- link( m14.3 , data=datp )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )

# set up plot
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
  ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )

for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )

xo <- 0.1 # offset distance to stagger raw data and predictions

# raw data
for ( j in (1:7)[-2] ) {
  lines( (j-1)*4+c(1,3)-xo , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
  lines( (j-1)*4+c(2,4)-xo , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}

points( 1:28-xo , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28-xo , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )

yoff <- 0.175

text( 1-xo , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2-xo , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3-xo , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4-xo , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )

# posterior predictions
for ( j in (1:7)[-2] ) {
  lines( (j-1)*4+c(1,3)+xo , p_mu[(j-1)*4+c(1,3)] , lwd=2 )
  lines( (j-1)*4+c(2,4)+xo , p_mu[(j-1)*4+c(2,4)] , lwd=2 )
}

for ( i in 1:28 ) lines( c(i,i)+xo , p_ci[,i] , lwd=1 )
points( 1:28+xo , p_mu , pch=16 , col="white" , cex=1.3 )
points( 1:28+xo , p_mu , pch=c(1,1,16,16) )
```

Interpretation:

-   blue points is the raw data and the black poinst are the posterior predictions
-   We see the actor is simply just very left handed. If we then look at the meean for actor 2, we see that it is the lowest, that is because this is shrunk the most.

#### Conclusions - multilevel horoscopes

Basically people tend to want defaults model given the data although there is no such thing. you should follow this flow:

1.  Think about the causal model first

2.  Begin with 'empty' model with varying intercept on relevant clusters

3.  Standardize predictors

4.  Use regularizing priors (simulate)

5.  Add in predictors and vary their slopes

6.  Can drop varying effects with tiny sigmas

7.  Consider two sorts of posterior prediction

    a.  Same units: What happened in these data?
    b.  New units: What might we expect for new units?

8.  Your knowledge of domain trumps all

## Exercises

### E1

*Add to the following model varying slopes on the predictor x.*

![](images/paste-292D958D.png)

$$
\begin{align*}
\text{First the probability of the data and the linear model} \\ \\
y_i \sim Normial(\mu_i,\sigma) && \text{Likelihood} \\ 
\mu_i = \alpha_{GROUP[i]} + \beta x_i && \text{Linear model} \\
\\ \text{Then matrix of varying intercepts and slopes, with its covariance matrix} \\ \\
\begin{bmatrix} \alpha_{GROUP[i]} \\ \beta_{GROUP[i]} \\ \end{bmatrix} \sim MVNormal( \begin{bmatrix} \alpha \\ \beta \\ \end{bmatrix} ,S) && \text{Population of varying effects} \\
S = \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta  \end{pmatrix} R \begin{pmatrix} \sigma_\alpha & 0 \\ 0 & \sigma_\beta \\  \end{pmatrix} && \text{construct covariance matrix} \\ 
\\ \text{Then the hyper priors} \\ \\
\alpha \sim Normal(0,10) \\
\beta \sim Normal(0,1) \\
\sigma \sim HalfCauchy(0,2) \\
\sigma\_\alpha \sim HalfCauchy(0,2) \\
R \sim LKJcorr(2) && \text{We assume intecept / slope correlations but not too strong}
\end{align*}
$$

### E3

*When is it possible for a varying slopes model to have fewer effective parameters (as estimated by WAIC or DIC) than the corresponding model with fixed (unpooled) slopes? Explain*

Recall that the more effective parameters that we are having the flexible is the model. Although the model has memory and if it is able to identify low variance we will see that the effective number of parameters will also be shrunken. This must be due to the bayesian updating.
